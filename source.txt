=== BEGIN FILE: ./mindscript/builtin_misc.go ===
package mindscript

import (
	"fmt"
	"math"
	"math/rand"
	"os"
	"runtime/debug"
	"strconv"
	"sync"
	"time"
)

// --- Random Utilities ----------------------------------------------------

func registerRandomBuiltins(ip *Interpreter, target *Env) {
	// Instance-local RNG and mutex; closures capture these.
	var (
		rng   = rand.New(rand.NewSource(time.Now().UnixNano()))
		rngMu sync.Mutex
	)

	ip.RegisterRuntimeBuiltin(
		target,
		"seedRand",
		[]ParamSpec{{Name: "n", Type: S{"id", "Int"}}},
		S{"id", "Null"},
		func(_ *Interpreter, ctx CallCtx) Value {
			n := ctx.Arg("n")
			rngMu.Lock()
			rng.Seed(n.Data.(int64))
			rngMu.Unlock()
			return Null
		},
	)
	setBuiltinDoc(target, "seedRand", `Seed the pseudo-random number generator.

Use a fixed seed for reproducible sequences.

Params:
	n: Int — seed value

Returns:
	Null`)

	ip.RegisterRuntimeBuiltin(
		target,
		"randInt",
		[]ParamSpec{{Name: "n", Type: S{"id", "Int"}}},
		S{"id", "Int"},
		func(_ *Interpreter, ctx CallCtx) Value {
			n := ctx.Arg("n").Data.(int64)
			// Contractual: n must be > 0 (hard error)
			if n <= 0 {
				fail("randInt: n must be > 0")
			}
			// Guard against overflow when converting to platform int.
			intMax := int64(int(^uint(0) >> 1))
			if n > intMax {
				fail("randInt: n too large on this platform")
			}
			rngMu.Lock()
			res := rng.Intn(int(n))
			rngMu.Unlock()
			return Int(int64(res))
		},
	)
	setBuiltinDoc(target, "randInt", `Uniform random integer in [0, n).

Params:
	n: Int — upper bound (must be > 0)

Returns:
	Int`)

	ip.RegisterRuntimeBuiltin(
		target,
		"randFloat",
		[]ParamSpec{{Name: "_", Type: S{"id", "Null"}}},
		S{"id", "Num"},
		func(_ *Interpreter, ctx CallCtx) Value {
			rngMu.Lock()
			f := rng.Float64()
			rngMu.Unlock()
			return Num(f)
		},
	)
	setBuiltinDoc(target, "randFloat", `Uniform random number in [0.0, 1.0).

Params:
	_: Null

Returns:
	Num`)
}

// --- Casting Utilities ----------------------------------------------------

func registerCastBuiltins(ip *Interpreter, target *Env) {
	// pretty(src: Str) -> Str?   (caret-formatted parse errors as soft null)
	ip.RegisterRuntimeBuiltin(
		target,
		"formatCode",
		[]ParamSpec{{Name: "src", Type: S{"id", "Str"}}},
		S{"unop", "?", S{"id", "Str"}},
		func(_ *Interpreter, ctx CallCtx) (v Value) {
			s := ctx.Arg("src").Data.(string)

			// Catch panics from Pretty / printer and turn them into a soft error.
			defer func() {
				if r := recover(); r != nil {
					// Print Go-side details to stderr for debugging.
					fmt.Fprintf(
						os.Stderr, // or os.Stderr, depending on your wiring
						"formatCode panic: %v\n%s\n",
						r,
						debug.Stack(),
					)
					v = annotNull(fmt.Sprintf("formatCode panic: %v", r))
				}
			}()

			out, err := Pretty(s)
			if err != nil {
				return annotNull(err.Error())
			}
			return Str(out)
		},
	)
	setBuiltinDoc(target, "formatCode", `Format source code.

Parses the input and pretty-prints it with normalized whitespace and minimal parentheses. Supports PRE/POST annotations (# ... lines above; trailing # ... forces newline). On parse failure, returns null with a caret-formatted error.

Params:
	src: Str

Returns:
	Str?`)

	// formatValue(x: Any) -> Str   (renders with annotations)
	ip.RegisterRuntimeBuiltin(
		target,
		"formatValue",
		[]ParamSpec{{Name: "x", Type: S{"id", "Any"}}},
		S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value {
			return Str(FormatValue(ctx.Arg("x")))
		},
	)
	setBuiltinDoc(target, "formatValue", `Render a runtime value (with annotations).

Produces a stable, readable string: scalars are literal; arrays/maps inline when short, otherwise multi-line (maps sort keys). PRE annotations print as header lines; POST as trailing comments. Functions show as <fun: ...>, types as <type: ...>, modules as <module: ...>.

Params:
	x: Any

Returns:
	Str`)

	// str(x: Any) -> Str?   (ignores annotations; soft-error on unsupported)
	ip.RegisterRuntimeBuiltin(
		target,
		"str",
		[]ParamSpec{{Name: "x", Type: S{"id", "Any"}}},
		S{"unop", "?", S{"id", "Str"}},
		func(_ *Interpreter, ctx CallCtx) Value {
			// Normalize modules to maps so they print as their map view.
			in := AsMapValue(ctx.Arg("x"))
			// Functions, types, and handles are intentionally not printable.
			switch in.Tag {
			case VTFun, VTType, VTHandle:
				return annotNull("unsupported type, cannot convert to Str")
			}
			// Strip annotations recursively before printing.
			clean := stripAnnDeep(in)

			// Identity for plain strings (no quotes).
			if clean.Tag == VTStr {
				return clean
			}

			// Everything else uses the pretty renderer.
			return Str(FormatValue(clean))
		},
	)
	setBuiltinDoc(target, "str", `Convert to string if possible; otherwise err.

Converts values of type Null, Bool, Int, Num, Str, [...], and {...}.

Params:
	x: Any

Returns:
	Str?`)

	ip.RegisterRuntimeBuiltin(
		target,
		"int",
		[]ParamSpec{{Name: "x", Type: S{"id", "Any"}}},
		S{"unop", "?", S{"id", "Int"}},
		func(_ *Interpreter, ctx CallCtx) Value {
			v := ctx.Arg("x")
			switch v.Tag {
			case VTInt:
				return v
			case VTNum:
				return Int(int64(v.Data.(float64)))
			case VTBool:
				if v.Data.(bool) {
					return Int(1)
				}
				return Int(0)
			case VTStr:
				if n, err := strconv.ParseInt(v.Data.(string), 10, 64); err == nil {
					return Int(n)
				}
				return annotNull("unsupported type, cannot convert to Int")
			default:
				return annotNull("unsupported type, cannot convert to Int")
			}
		},
	)
	setBuiltinDoc(target, "int", `Convert to Int when possible; otherwise errs.

Rules:
	• Int → Int
	• Num → truncated toward zero
	• Bool → 1 or 0
	• Str → parsed base-10 integer, or null on failure
	• Others → null

Params:
	x: Any

Returns:
	Int?`)

	ip.RegisterRuntimeBuiltin(
		target,
		"num",
		[]ParamSpec{{Name: "x", Type: S{"id", "Any"}}},
		S{"unop", "?", S{"id", "Num"}},
		func(_ *Interpreter, ctx CallCtx) Value {
			v := ctx.Arg("x")
			switch v.Tag {
			case VTNum:
				return v
			case VTInt:
				return Num(float64(v.Data.(int64)))
			case VTBool:
				if v.Data.(bool) {
					return Num(1)
				}
				return Num(0)
			case VTStr:
				if f, err := strconv.ParseFloat(v.Data.(string), 64); err == nil {
					return Num(f)
				}
				return annotNull("unsupported type, cannot convert to Num")
			default:
				return annotNull("unsupported type, cannot convert to Num")
			}
		},
	)
	setBuiltinDoc(target, "num", `Convert to Num when possible; otherwise errs.

Rules:
	• Num → Num
	• Int → floating-point value
	• Bool → 1.0 or 0.0
	• Str → parsed as float64, or null on failure
	• Others → null

Params:
	x: Any

Returns:
	Num?`)

	ip.RegisterRuntimeBuiltin(
		target,
		"bool",
		[]ParamSpec{{Name: "x", Type: S{"id", "Any"}}},
		S{"unop", "?", S{"id", "Bool"}},
		func(_ *Interpreter, ctx CallCtx) Value {
			v := ctx.Arg("x")
			switch v.Tag {
			case VTBool:
				return v
			case VTNull:
				return Bool(false)
			case VTInt:
				return Bool(v.Data.(int64) != 0)
			case VTNum:
				return Bool(v.Data.(float64) != 0)
			case VTStr:
				return Bool(v.Data.(string) != "")
			case VTArray:
				return Bool(len(v.Data.(*ArrayObject).Elems) > 0)
			case VTMap:
				return Bool(len(v.Data.(*MapObject).Entries) > 0)
			default:
				return annotNull("unsupported type, cannot convert to Bool")
			}
		},
	)
	setBuiltinDoc(target, "bool", `Convert to Bool using common "truthiness" rules; otherwise errs.

Falsey:
	• null
	• 0, 0.0
	• "" (empty string)
	• [] (empty array)
	• {} (empty map)

Truthy:
	• Any other value for Int, Num, [...], and {...}.

Params:
	x: Any

Returns:
	Bool?`)

	ip.RegisterRuntimeBuiltin(
		target,
		"len",
		[]ParamSpec{{Name: "x", Type: S{"id", "Any"}}},
		S{"unop", "?", S{"id", "Int"}},
		func(_ *Interpreter, ctx CallCtx) Value {
			x := AsMapValue(ctx.Arg("x"))
			switch x.Tag {
			case VTArray:
				return Int(int64(len(x.Data.(*ArrayObject).Elems)))
			case VTMap:
				mo := x.Data.(*MapObject)
				// Use ordered keys length to reflect object “length”
				return Int(int64(len(mo.Keys)))
			case VTStr:
				// Str is bytes: length is byte count.
				return Int(int64(len(x.Data.(string))))
			default:
				return Null
			}
		},
	)
	setBuiltinDoc(target, "len", `Length of a value.

Rules:
	• [a, b, c] yields 3
	• {k: v, ...} yields number of keys
	• "..." yields byte count
	• Others yield null

Params:
	x: Any

Returns:
	Int?`)
}

// --- Math Utilities ----------------------------------------------------

func registerMathBuiltins(ip *Interpreter, target *Env) {
	// Constants
	target.Define("PI", Num(math.Pi))
	target.Define("E", Num(math.E))
	setBuiltinDoc(target, "PI", `Mathematical constant π.

Returns:
	Num`)
	setBuiltinDoc(target, "E", `Euler's number e.

Returns:
	Num`)

	// Unary math helpers
	un1 := func(name string, f func(float64) float64, doc string) {
		ip.RegisterRuntimeBuiltin(
			target,
			name,
			[]ParamSpec{{Name: "x", Type: S{"id", "Num"}}},
			S{"id", "Num"},
			func(_ *Interpreter, ctx CallCtx) Value {
				return Num(f(ctx.NumArg("x").Data.(float64)))
			},
		)
		setBuiltinDoc(target, name, doc)
	}
	un1("sin", math.Sin, `Sine of an angle in radians.

Params:
	x: Num — radians

Returns:
	Num`)
	un1("cos", math.Cos, `Cosine of an angle in radians.

Params:
	x: Num — radians

Returns:
	Num`)
	un1("tan", math.Tan, `Tangent of an angle in radians.

Params:
	x: Num — radians

Returns:
	Num`)
	un1("sqrt", math.Sqrt, `Square root.

Params:
	x: Num — non-negative

Returns:
	Num`)
	un1("log", math.Log, `Natural logarithm (base e).

Params:
	x: Num — positive

Returns:
	Num`)
	un1("exp", math.Exp, `Exponential function e^x.

Params:
	x: Num

Returns:
	Num`)

	ip.RegisterRuntimeBuiltin(
		target,
		"pow",
		[]ParamSpec{
			{Name: "base", Type: S{"id", "Num"}},
			{Name: "exp", Type: S{"id", "Num"}},
		},
		S{"id", "Num"},
		func(_ *Interpreter, ctx CallCtx) Value {
			return Num(math.Pow(ctx.NumArg("base").Data.(float64), ctx.NumArg("exp").Data.(float64)))
		},
	)
	setBuiltinDoc(target, "pow", `Power: base^exp.

Params:
	base: Num
	exp:  Num

Returns:
	Num`)
}

// --- Process Utilities ----------------------------------------------------

func registerProcessBuiltins(ip *Interpreter, target *Env) {
	// exit(code:Int?) -> Null (terminates the host process)
	ip.RegisterRuntimeBuiltin(
		target,
		"exit",
		[]ParamSpec{{Name: "code", Type: S{"unop", "?", S{"id", "Int"}}}},
		S{"id", "Null"},
		func(_ *Interpreter, ctx CallCtx) Value {
			codeV := ctx.Arg("code")
			code := 0
			if codeV.Tag == VTInt {
				code = int(codeV.Data.(int64))
			}
			os.Exit(code)
			return Null // unreachable
		},
	)
	setBuiltinDoc(target, "exit", `Terminate the current process with an optional status code.

By convention, 0 indicates success; non-zero indicates an error.

Params:
	code: Int? — exit status (default 0)

Returns:
	Null (never returns; process exits)`)
}

// stripAnnDeep removes annotations from a value recursively.
//   - Clears Value.Annot on all nodes.
//   - For arrays, deep-copies elements with annotations stripped.
//   - For maps, deep-copies entries (annotations stripped), preserves key order (Keys),
//     and clears KeyAnn entirely.
func stripAnnDeep(v Value) Value {
	// Always drop the annotation on this node.
	v.Annot = ""
	switch v.Tag {
	case VTArray:
		ao := v.Data.(*ArrayObject)
		elems := make([]Value, len(ao.Elems))
		for i := range ao.Elems {
			elems[i] = stripAnnDeep(ao.Elems[i])
		}
		return Arr(elems) // Arr() builds a fresh ArrayObject
	case VTMap:
		mo := v.Data.(*MapObject)
		cpE := make(map[string]Value, len(mo.Entries))
		for k, vv := range mo.Entries {
			cpE[k] = stripAnnDeep(vv)
		}
		cpK := make([]string, len(mo.Keys))
		copy(cpK, mo.Keys)
		return Value{
			Tag: VTMap,
			Data: &MapObject{
				Entries: cpE,
				Keys:    cpK,
			},
		}
	default:
		// Primitives, modules already normalized via AsMapValue, etc.
		return v
	}
}

func registerOracleBuiltins(ip *Interpreter, target *Env) {
	// oracleExamples(fn: Any) -> Any
	// Returns the oracle's stored examples (VTArray) or null.
	ip.RegisterRuntimeBuiltin(
		target,
		"oracleGetExamples",
		[]ParamSpec{{Name: "fn", Type: S{"id", "Any"}}},
		S{"id", "Any"},
		func(_ *Interpreter, ctx CallCtx) Value {
			v := ctx.Arg("fn")
			if v.Tag != VTFun || v.Data == nil {
				fail("oracleExamples: expected an oracle function")
			}
			f := v.Data.(*Fun)
			if f == nil || !f.IsOracle {
				fail("oracleExamples: expected an oracle function")
			}
			// Stored in canonical form: [ [arg1..argN, ret], ... ] or null.
			return f.Examples
		},
	)
	setBuiltinDoc(target, "oracleGetExamples", `Get examples attached to an oracle.

Params:
	fn: Any — must be an oracle function

Returns:
	Any — the examples array in canonical form, or null`)

	// oracleSetExamples(fn: Any, examples: Any) -> Null
	// Uses the engine validator; hard-fails (panic) on mismatch.
	ip.RegisterRuntimeBuiltin(
		target,
		"oracleSetExamples",
		[]ParamSpec{
			{Name: "fn", Type: S{"id", "Any"}},
			{Name: "examples", Type: S{"id", "Any"}},
		},
		S{"id", "Bool"},
		func(ip *Interpreter, ctx CallCtx) Value {
			fn := ctx.Arg("fn")
			ex := ctx.Arg("examples")
			if err := ip.OracleSetExamples(fn, ex); err != nil {
				fail("oracleSetExamples: " + err.Error())
			}
			return Bool(true)
		},
	)
	setBuiltinDoc(target, "oracleSetExamples", `Set examples on an oracle (hard-fails on type mismatch).

Canonical example format for an oracle with N parameters:
	[arg1, arg2, ..., argN, returnValue]

Pass null to clear examples.

Params:
	fn: Any — must be an oracle function
	examples: Any — array of examples or null

Returns:
	true if it succeeds.`)
}

=== END FILE: ./mindscript/builtin_misc.go ===

=== BEGIN FILE: ./mindscript/builtin_strings.go ===
// === FILE: std_sys.go ===
package mindscript

import (
	"regexp"
	"strings"
	"unicode"
)

func registerStringBuiltins(ip *Interpreter, target *Env) {
	// substr(s, i, j)
	ip.RegisterRuntimeBuiltin(
		target,
		"substr",
		[]ParamSpec{{"s", S{"id", "Str"}}, {"i", S{"id", "Int"}}, {"j", S{"id", "Int"}}},
		S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value {
			s := ctx.Arg("s").Data.(string)
			i := int(ctx.Arg("i").Data.(int64))
			j := int(ctx.Arg("j").Data.(int64))
			if i < 0 {
				i = 0
			}
			if j < i {
				j = i
			}
			if i > len(s) {
				i = len(s)
			}
			if j > len(s) {
				j = len(s)
			}
			return Str(string(s[i:j]))
		},
	)
	setBuiltinDoc(target, "substr", `Substring by byte index.

Takes the half-open slice [i, j). Indices are clamped to bounds and negative
values are treated as 0.

Params:
  s: Str — source string
  i: Int — start index (inclusive)
  j: Int — end index (exclusive)

Returns:
  Str`)

	ip.RegisterRuntimeBuiltin(
		target,
		"toLower",
		[]ParamSpec{{"s", S{"id", "Str"}}},
		S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value { return Str(strings.ToLower(ctx.Arg("s").Data.(string))) },
	)
	setBuiltinDoc(target, "toLower", `Lowercase conversion (Unicode aware).

Params:
  s: Str

Returns:
  Str`)

	ip.RegisterRuntimeBuiltin(
		target,
		"toUpper",
		[]ParamSpec{{"s", S{"id", "Str"}}},
		S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value { return Str(strings.ToUpper(ctx.Arg("s").Data.(string))) },
	)
	setBuiltinDoc(target, "toUpper", `Uppercase conversion (Unicode aware).

Params:
  s: Str

Returns:
  Str`)

	trimFunc := func(left, right bool) func(string) string {
		return func(s string) string {
			if left && right {
				return strings.TrimSpace(s)
			}
			if left {
				return strings.TrimLeftFunc(s, unicode.IsSpace)
			}
			return strings.TrimRightFunc(s, unicode.IsSpace)
		}
	}

	ip.RegisterRuntimeBuiltin(
		target,
		"strip",
		[]ParamSpec{{"s", S{"id", "Str"}}}, S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value {
			return Str(trimFunc(true, true)(ctx.Arg("s").Data.(string)))
		},
	)
	setBuiltinDoc(target, "strip", `Remove leading and trailing whitespace (Unicode).

Params:
  s: Str

Returns:
  Str`)

	ip.RegisterRuntimeBuiltin(
		target,
		"lstrip",
		[]ParamSpec{{"s", S{"id", "Str"}}}, S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value {
			return Str(trimFunc(true, false)(ctx.Arg("s").Data.(string)))
		},
	)
	setBuiltinDoc(target, "lstrip", `Remove leading whitespace (Unicode).

Params:
  s: Str

Returns:
  Str`)

	ip.RegisterRuntimeBuiltin(
		target,
		"rstrip",
		[]ParamSpec{{"s", S{"id", "Str"}}}, S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value {
			return Str(trimFunc(false, true)(ctx.Arg("s").Data.(string)))
		},
	)
	setBuiltinDoc(target, "rstrip", `Remove trailing whitespace (Unicode).

Params:
  s: Str

Returns:
  Str`)

	ip.RegisterRuntimeBuiltin(
		target,
		"split",
		[]ParamSpec{{"s", S{"id", "Str"}}, {"sep", S{"id", "Str"}}},
		S{"array", S{"id", "Str"}},
		func(_ *Interpreter, ctx CallCtx) Value {
			s := ctx.Arg("s").Data.(string)
			sep := ctx.Arg("sep").Data.(string)
			parts := strings.Split(s, sep)
			out := make([]Value, len(parts))
			for i := range parts {
				out[i] = Str(parts[i])
			}
			return Arr(out)
		},
	)
	setBuiltinDoc(target, "split", `Split a string on a separator (no regex).

If sep is empty (""), splits between UTF-8 code points.

Params:
  s: Str   — source string
  sep: Str — separator

Returns:
  [Str]`)

	ip.RegisterRuntimeBuiltin(
		target,
		"join",
		[]ParamSpec{{"xs", S{"array", S{"id", "Str"}}}, {"sep", S{"id", "Str"}}},
		S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value {
			xs := ctx.Arg("xs").Data.(*ArrayObject).Elems
			sep := ctx.Arg("sep").Data.(string)
			strs := make([]string, len(xs))
			for i := range xs {
				strs[i] = xs[i].Data.(string)
			}
			return Str(strings.Join(strs, sep))
		},
	)
	setBuiltinDoc(target, "join", `Join strings with a separator.

Params:
  xs: [Str] — pieces to join
  sep: Str  — separator

Returns:
  Str`)

	// match(pattern: Str, s: Str) -> [Str]
	ip.RegisterRuntimeBuiltin(
		target,
		"match",
		[]ParamSpec{{"pattern", S{"id", "Str"}}, {"string", S{"id", "Str"}}},
		S{"array", S{"id", "Str"}},
		func(_ *Interpreter, ctx CallCtx) Value {
			pat := ctx.Arg("pattern").Data.(string)
			s := ctx.Arg("string").Data.(string)
			re, err := regexp.Compile(pat)
			if err != nil {
				// Soft error: invalid regex
				return annotNull("invalid regex: " + err.Error())
			}
			ms := re.FindAllString(s, -1)
			out := make([]Value, len(ms))
			for i := range ms {
				out[i] = Str(ms[i])
			}
			return Arr(out)
		},
	)
	setBuiltinDoc(target, "match", `Find all non-overlapping matches of a regex.

Params:
  pattern: Str — RE2-compatible regular expression
  string:  Str — input

Returns:
  [Str] — matched substrings (no capture groups)`)

	// replace(pattern: Str, repl: Str, s: Str) -> Str
	ip.RegisterRuntimeBuiltin(
		target,
		"replace",
		[]ParamSpec{{"pattern", S{"id", "Str"}}, {"replace", S{"id", "Str"}}, {"string", S{"id", "Str"}}},
		S{"id", "Str"},
		func(_ *Interpreter, ctx CallCtx) Value {
			pat := ctx.Arg("pattern").Data.(string)
			rep := ctx.Arg("replace").Data.(string)
			s := ctx.Arg("string").Data.(string)
			re, err := regexp.Compile(pat)
			if err != nil {
				// Soft error: invalid regex
				return annotNull("invalid regex: " + err.Error())
			}
			return Str(re.ReplaceAllString(s, rep))
		},
	)
	setBuiltinDoc(target, "replace", `Replace all non-overlapping regex matches.

Params:
  pattern: Str — RE2-compatible regular expression
  replace: Str — replacement (no backrefs)
  string:  Str — input

Returns:
  Str`)
}

=== END FILE: ./mindscript/builtin_strings.go ===

=== BEGIN FILE: ./mindscript/lexer.go ===
// lexer.go: provides a whitespace-sensitive, UTF-8–aware lexer for the
// MindScript language. It converts a source string into a linear stream of
// tokens with accurate source positions and rich literal decoding.
//
// ──────────────────────────────────────────────────────────────────────────────
// HIGH-LEVEL OVERVIEW
//
// The lexer scans left→right and emits Token values, always ending with EOF.
// Each token carries:
//   - Type     — a TokenType enum
//   - Lexeme   — the exact source slice (verbatim characters from the input)
//   - Literal  — the decoded value for literal tokens (e.g., bool/int/float/string)
//   - Line/Col — 1-based line and 0-based column of the token’s *start*
//   - StartByte/EndByte — byte offsets [start, end) for the token’s source span
//
// Whitespace-sensitive delimiters:
//
//	The lexer decides between LROUND/CLROUND (and LSQUARE/CLSQUARE) solely by
//	whether there is immediate whitespace before the delimiter:
//
//	  '('  → LROUND  if there IS preceding whitespace
//	         CLROUND if there is NO preceding whitespace
//	  '['  → LSQUARE if there IS preceding whitespace
//	         CLSQUARE if there is NO preceding whitespace
//
// Consequences (user-facing syntax):
//   - Calls and parameter lists require NO space before '(':
//     f(x)           // call: uses CLROUND
//     fun(x: T)      // function params: uses CLROUND
//     oracle(x: T)   // oracle params: uses CLROUND
//     With a space ("fun (x: T)"), '(' becomes LROUND and is NOT treated as a
//     parameter list; the parser will error.
//   - Indexing requires NO space before '[':
//     arr[i]         // indexing: uses CLSQUARE
//     With a space ("arr [i]"), '[' is LSQUARE and is NOT treated as indexing.
//   - Grouping "(expr)" is produced regardless of LROUND/CLROUND, but only
//     CLROUND participates in call/juxtaposition chains.
//
// The '.' character is context-sensitive:
//   - If it begins a number (e.g., “.5” or “1.” or “1.2e3”), a NUMBER/INTEGER is
//     produced.
//   - Otherwise it is PERIOD (typically for property access).
//
// IDENTIFIERS & KEYWORDS
//
//	Identifiers match [A-Za-z_][A-Za-z0-9_]* and normally produce ID.
//	Reserved words produce dedicated TokenTypes (e.g., IF, LET, FUNCTION, etc.).
//	After a PERIOD, both identifiers *and* quoted strings are treated as
//	property names and forced to ID (even if the text is a keyword). This allows:
//	    obj."then"   // ID with Literal="then"
//	    obj.then     // ID with Literal="then"
//
// LITERALS
//   - STRING — single or double quotes, JSON-style escapes, including \uXXXX with
//     optional UTF-16 surrogate pair handling. The decoded STRING literal is a
//     raw byte string (Go string as bytes): it may contain arbitrary bytes and
//     need not be valid UTF-8. Source text is still required to be valid UTF-8.
//     \uXXXX inserts the UTF-8 encoding of the code point; \xHH inserts a single raw byte.
//   - INTEGER — 64-bit signed (ParseInt base 10), when no dot/exp part.
//   - NUMBER  — 64-bit float (ParseFloat), for forms with '.' and/or exponent.
//   - BOOLEAN — “true” or “false” (Literal: bool).
//   - NULL    — “null” (Literal: nil).
//
// ANNOTATIONS
//   - Hash-line annotations: one or more consecutive lines where, after optional
//     indentation, the first non-space is '#'. The leading '#' (and at most one
//     optional following space) are stripped; lines are joined with '\n', and a
//     single ANNOTATION token is emitted. A blank/non-# line ends the block.
//
// ERRORS (start-of-token anchoring)
//   - Lexical errors (e.g., bad escape, invalid UTF-8, unexpected character) are
//     reported as *Error* with precise location anchored to the **start** of the
//     offending token (the token that is being scanned).
//   - Interactive/REPL mode: if enabled via NewLexerInteractive, unterminated
//     strings produce *IncompleteError* (still anchored to the token start).
//
// OUTPUT
//   - Scan returns the full token slice *including* the terminal EOF token.
//   - Each token’s Lexeme is the exact source text (e.g., a STRING’s lexeme
//     includes the quotes and escapes), while Literal carries the decoded value.
//
// ──────────────────────────────────────────────────────────────────────────────
//
// FILE ORGANIZATION
//  1. PUBLIC API  — exported enums/types/constructors/methods & their docs.
//  2. PRIVATE     — all non-exported helpers, internal tables, and scanning.
//
// The PUBLIC API docs below are intentionally exhaustive so the behavior is
// understandable without reading the implementation.
//
// ──────────────────────────────────────────────────────────────────────────────
package mindscript

import (
	"errors"
	"fmt"
	"strconv"
	"strings"
	"unicode/utf16"
	"unicode/utf8"
)

////////////////////////////////////////////////////////////////////////////////
//                               PUBLIC API
////////////////////////////////////////////////////////////////////////////////

// TokenType is the enumeration of all token kinds the lexer can emit.
// Most names are self-explanatory; groups are listed for clarity.
//
// Special:
//
//	EOF     — end-of-file sentinel (always the final token)
//	ILLEGAL — produced only for unrecoverable internal conditions (not used by Scan)
//
// Punctuation (some are whitespace-sensitive, see '(' and '[' notes below):
//
//	LROUND, CLROUND   — '(' with/without preceding whitespace respectively
//	RROUND            — ')'
//	LSQUARE, CLSQUARE — '[' with/without preceding whitespace respectively
//	RSQUARE           — ']'
//	LCURLY, RCURLY    — '{', '}'
//	COLON, COMMA, PERIOD, QUESTION — ':', ',', '.', '?'
//
// Operators:
//
//	PLUS, MINUS, MULT, DIV, MOD — '+', '-', '*', '/', '%'
//	ASSIGN                      — '='
//	EQ, NEQ                     — '==', '!='
//	LESS, LESS_EQ               — '<',  '<='
//	GREATER, GREATER_EQ         — '>',  '>='
//	BANG                        — '!' (used by the language in object/type literals)
//	ARROW                       — '->'
//
// Literals & identifiers:
//
//	ID, STRING, INTEGER, NUMBER, BOOLEAN, NULL
//
// Keywords (produced when the identifier text equals these words, except when
// forced to an ID after PERIOD/property access):
//
//	AND, OR, NOT,
//	LET, DO, END, RETURN, BREAK, CONTINUE,
//	IF, THEN, ELIF, ELSE,
//	FUNCTION, ORACLE, MODULE,
//	FOR, IN, FROM, WHILE,
//	TYPECONS, TYPE, ENUM
//
// Annotation:
//
//	ANNOTATION — emitted for multi-line blocks starting with '#'.
type TokenType int

const (
	// Special
	EOF TokenType = iota
	ILLEGAL

	// Punctuation
	LROUND   // "(" when preceded by whitespace
	CLROUND  // "(" when not preceded by whitespace (juxtaposition/call form)
	RROUND   // ")"
	LSQUARE  // "["
	CLSQUARE // "[" when not preceded by whitespace (index-close)
	RSQUARE  // "]"
	LCURLY   // "{"
	RCURLY   // "}"
	COLON    // ":"
	COMMA    // ","
	PERIOD   // "."
	QUESTION // "?"

	// Operators
	PLUS
	MINUS
	MULT
	DIV
	MOD
	POW    // "**"
	ASSIGN // "="
	EQ     // "=="
	NEQ    // "!="
	LESS
	LESS_EQ
	GREATER
	GREATER_EQ
	LSHIFT // "<<"
	RSHIFT // ">>"
	BITAND // "&"
	BITXOR // "^"
	BITOR  // "|"
	BITNOT // "~" (unary)
	BANG   // "!" (required-field marker in object/type literals)
	ARROW  // "->"

	// Literals & identifiers
	ID
	STRING
	INTEGER
	NUMBER
	BOOLEAN
	NULL

	// Keywords
	AND
	OR
	NOT
	LET
	DO
	END
	RETURN
	BREAK
	CONTINUE
	IF
	THEN
	ELIF
	ELSE
	FUNCTION
	ORACLE
	MODULE
	FOR
	IN
	FROM
	WHILE
	TYPECONS
	TYPE
	ENUM

	// Annotation token (from lines starting with '#')
	ANNOTATION
	NOOP
)

// Token is a single lexical unit produced by the lexer.
//
// Fields:
//
//	Type    — the TokenType kind.
//	Lexeme  — the exact source slice comprising the token (verbatim, including
//	          quotes for strings, escape sequences, etc.).
//	Literal — a decoded value for literal tokens:
//	          • STRING  → Go string treated as raw bytes (escapes resolved; may be invalid UTF-8)
//	          • INTEGER → int64
//	          • NUMBER  → float64
//	          • BOOLEAN → bool
//	          • NULL    → nil
//	          Non-literal tokens usually carry nil or an unmodified string
//	          (keywords may store their text; property IDs store the property name).
//	Line    — 1-based line number at which this token starts.
//	Col     — 0-based column index at which this token starts.
//	StartByte / EndByte — byte offsets [start, end) for the token’s slice.
type Token struct {
	Type      TokenType
	Lexeme    string
	Literal   interface{}
	Line      int
	Col       int
	StartByte int
	EndByte   int
}

// HARD errors produced by the lexer use the unified *Error type defined in
// errors.go. See DiagLex and DiagIncomplete.
//
// In interactive mode, the lexer returns *Error with Kind=DiagIncomplete
// when input ends inside an unterminated construct (e.g., string literal).
//
// **Error locations:** All lexing errors (both hard and incomplete) are anchored
// to the **start of the offending token** (the token whose scanning raised
// the error). This makes diagnostics stable and easy to map to spans.

// Lexer is a streaming tokenizer for MindScript.
//
// Construction:
//   - NewLexer(src)            — normal mode. Unterminated constructs produce LexError.
//   - NewLexerInteractive(src) — REPL-friendly mode. Unterminated constructs
//     produce IncompleteError.
//
// Semantics:
//   - Scan() returns the full token slice including EOF. It never panics for
//     malformed input; instead it returns (nil, error).
//   - Whitespace is skipped, but influences '(' and '[' classification (see TokenType docs).
//   - PERIOD vs number: a '.' followed by digits begins a number IFF there is
//     either preceding whitespace *or* the previous token cannot be a left operand.
//     Otherwise '.' is PERIOD used for property access.
//   - After PERIOD, the *next* identifier or quoted string is forced to ID,
//     even if it matches a keyword.
//
// Positioning:
//   - Line numbers are 1-based; column indices are 0-based.
//   - A token’s position is captured at the start of scanning that token.
type Lexer struct {
	// public type with no exported fields; use constructors + Scan()
	src    string
	start  int // start index of current token
	cur    int // current index
	line   int // 1-based
	col    int // 0-based column within line
	tokens []Token

	// precise token start position
	tokStartLine int
	tokStartCol  int

	// interactive mode: produce IncompleteError for unterminated constructs at EOF
	interactive bool
}

// NewLexer creates a new lexer for the given source in normal mode.
func NewLexer(src string) *Lexer {
	return &Lexer{
		src: src,
		// very rough guess to reduce reslices on big files
		tokens: make([]Token, 0, len(src)/4),
		line:   1,
		col:    0,
	}
}

// NewLexerInteractive creates a lexer in interactive mode.
// Unterminated strings return IncompleteError at EOF, allowing REPLs to request more input.
func NewLexerInteractive(src string) *Lexer {
	return &Lexer{
		src:         src,
		tokens:      make([]Token, 0, len(src)/4),
		line:        1,
		col:         0,
		interactive: true,
	}
}

// Scan tokenizes the entire source string and returns the resulting slice of
// tokens. The returned slice always ends with EOF. On error, it returns (nil, err).
//
// Error behavior summary:
//   - Normal mode: returns *LexError on malformed input or unterminated constructs.
//   - Interactive mode: returns *IncompleteError at EOF if a construct is
//     unterminated; other issues still return *LexError.
//
// Note: Token.Lexeme is the exact source span; Token.Literal contains decoded
// values for STRING/INTEGER/NUMBER/BOOLEAN/NULL as described in Token docs.
func (l *Lexer) Scan() ([]Token, error) {
	for {
		tok, err := l.scanToken()
		if err != nil {
			return nil, err
		}
		if tok.Type == EOF {
			return l.tokens, nil
		}
	}
}

//// END_OF_PUBLIC

////////////////////////////////////////////////////////////////////////////////
//                            PRIVATE IMPLEMENTATION
////////////////////////////////////////////////////////////////////////////////

// ---------------- keywords map (private) ----------------

var keywords = map[string]TokenType{
	"null":     NULL,
	"false":    BOOLEAN,
	"true":     BOOLEAN,
	"and":      AND,
	"or":       OR,
	"not":      NOT,
	"let":      LET,
	"do":       DO,
	"end":      END,
	"return":   RETURN,
	"break":    BREAK,
	"continue": CONTINUE,
	"if":       IF,
	"then":     THEN,
	"elif":     ELIF,
	"else":     ELSE,
	"fun":      FUNCTION,
	"oracle":   ORACLE,
	"module":   MODULE,
	"for":      FOR,
	"in":       IN,
	"from":     FROM,
	"while":    WHILE,
	"type":     TYPECONS,
	"Type":     TYPE,
	"Null":     TYPE,
	"Str":      TYPE,
	"Int":      TYPE,
	"Num":      TYPE,
	"Bool":     TYPE,
	"Any":      TYPE,
	"Handle":   TYPE,
	"Enum":     ENUM,
}

// ---------------- core scanning helpers ----------------

func (l *Lexer) isAtEnd() bool { return l.cur >= len(l.src) }

func (l *Lexer) peek() (byte, bool) {
	if l.isAtEnd() {
		return 0, false
	}
	return l.src[l.cur], true
}

func (l *Lexer) peekN(n int) (byte, bool) {
	idx := l.cur + n
	if idx >= len(l.src) {
		return 0, false
	}
	return l.src[idx], true
}

func (l *Lexer) advance() (byte, bool) {
	if l.isAtEnd() {
		return 0, false
	}
	ch := l.src[l.cur]
	l.cur++
	if ch == '\n' {
		l.line++
		l.col = 0
	} else {
		l.col++
	}
	return ch, true
}

func (l *Lexer) rewindToStart() {
	// We rewind only within the bounds of the current token start; line/col are kept
	// for error arrows (OK since we set tokStartLine/Col before scanning).
	l.cur = l.start
}

func (l *Lexer) addToken(tt TokenType, lit interface{}) Token {
	lex := l.src[l.start:l.cur]
	tok := Token{
		Type:      tt,
		Lexeme:    lex,
		Literal:   lit,
		Line:      l.tokStartLine,
		Col:       l.tokStartCol,
		StartByte: l.start,
		EndByte:   l.cur,
	}
	l.tokens = append(l.tokens, tok)
	l.start = l.cur
	return tok
}

func (l *Lexer) previousToken() *Token {
	if len(l.tokens) == 0 {
		return nil
	}
	return &l.tokens[len(l.tokens)-1]
}

func (l *Lexer) skipHorizontalWhitespace() {
	for !l.isAtEnd() {
		ch, _ := l.peek()
		switch ch {
		case ' ', '\t', '\r':
			l.advance()
			l.start = l.cur
		default:
			return
		}
	}
}

// immediateWhitespaceBefore reports whether the byte immediately preceding the
// current token start is ASCII whitespace. Called *after* skipWhitespace.
func (l *Lexer) immediateWhitespaceBefore() bool {
	if l.start == 0 {
		return false
	}
	switch l.src[l.start-1] {
	case ' ', '\t', '\n', '\r', ';':
		return true
	default:
		return false
	}
}

// ---------------- small predicates ----------------

func canBeLeftOperand(t TokenType) bool {
	switch t {
	case ID, STRING, INTEGER, NUMBER, BOOLEAN, NULL,
		TYPE, ENUM,
		RROUND, RSQUARE, RCURLY,
		QUESTION:
		return true
	default:
		return false
	}
}

func isDigit(b byte) bool { return b >= '0' && b <= '9' }
func isHex(b byte) bool {
	return (b >= '0' && b <= '9') || (b >= 'a' && b <= 'f') || (b >= 'A' && b <= 'F')
}
func isBin(b byte) bool { return b == '0' || b == '1' }
func isOct(b byte) bool { return b >= '0' && b <= '7' }
func isAlpha(b byte) bool {
	return (b >= 'a' && b <= 'z') || (b >= 'A' && b <= 'Z') || b == '_'
}
func isAlphaNum(b byte) bool {
	return isAlpha(b) || (b >= '0' && b <= '9')
}

// readDigitsUS reads a run of digits (per pred) allowing single underscores
// between digits. It returns (hadDigits, endedOnUnderscore, err).
// Rules enforced:
//   - first char must be a digit (no leading '_')
//   - underscores must be single and between digits (no "__", no trailing '_')
func (l *Lexer) readDigitsUS(pred func(byte) bool) (bool, bool, error) {
	had := false
	lastUnd := false
	for {
		b, ok := l.peek()
		if !ok {
			break
		}
		if b == '_' {
			// underscore must follow a digit and be followed by a digit
			if !had || lastUnd {
				return had, true, l.err("misplaced '_' in number")
			}
			// lookahead must be a digit
			if b2, ok2 := l.peekN(1); !ok2 || !pred(b2) {
				return had, true, l.err("misplaced '_' in number")
			}
			lastUnd = true
			l.advance()
			continue
		}
		if !pred(b) {
			break
		}
		l.advance()
		had = true
		lastUnd = false
	}
	return had, lastUnd, nil
}

func stripUnderscores(s string) string {
	// small, allocation-friendly strip
	out := make([]byte, 0, len(s))
	for i := 0; i < len(s); i++ {
		if s[i] != '_' {
			out = append(out, s[i])
		}
	}
	return string(out)
}

func (l *Lexer) afterDotIsProperty() bool {
	p := l.previousToken()
	return p != nil && p.Type == PERIOD
}

// ---------------- error builders (start-of-token anchored) ----------------

func (l *Lexer) err(msg string) error {
	// All lexer diagnostics are anchored to the start of the *current* token,
	// which is the most stable and helpful position for tools and users.
	return &Error{Kind: DiagLex, Msg: msg, Src: nil, Line: l.tokStartLine, Col: l.tokStartCol + 1}
}

func (l *Lexer) errIncomplete(msg string) error {
	// Report need-more-input conditions in interactive mode, anchored to token start.
	return &Error{Kind: DiagIncomplete, Msg: msg, Src: nil, Line: l.tokStartLine, Col: l.tokStartCol + 1}
}

// ---------------- scanners ----------------

// scanString parses a JSON-style string literal (single or double quotes).
// The decoded result is a raw byte string (Go string as bytes). It may contain
// arbitrary bytes (including NUL) and need not be valid UTF-8.
func (l *Lexer) scanString() (string, error) {
	del := l.src[l.start]
	if del != '"' && del != '\'' {
		return "", l.err("internal: scanString without quote")
	}
	// consume the delimiter
	l.advance()

	out := make([]byte, 0, 32)

	appendRune := func(r rune) {
		var buf [utf8.UTFMax]byte
		n := utf8.EncodeRune(buf[:], r)
		out = append(out, buf[:n]...)
	}
	toHex := func(b byte) int {
		switch {
		case b >= '0' && b <= '9':
			return int(b - '0')
		case b >= 'a' && b <= 'f':
			return 10 + int(b-'a')
		case b >= 'A' && b <= 'F':
			return 10 + int(b-'A')
		default:
			return -1
		}
	}

	for !l.isAtEnd() {
		ch, _ := l.advance()
		if ch == byte(del) {
			return string(out), nil
		}
		if ch == '\\' {
			if l.isAtEnd() {
				if l.interactive {
					return "", l.errIncomplete("unfinished escape sequence")
				}
				return "", l.err("unfinished escape sequence")
			}
			esc, _ := l.advance()
			switch esc {
			case '"':
				out = append(out, '"')
			case '\'':
				out = append(out, '\'')
			case '\\':
				out = append(out, '\\')
			case '/':
				out = append(out, '/')
			case 'b':
				out = append(out, '\b')
			case 'f':
				out = append(out, '\f')
			case 'n':
				out = append(out, '\n')
			case 'r':
				out = append(out, '\r')
			case 't':
				out = append(out, '\t')

			case 'x':
				// \xHH inserts exactly one raw byte.
				b1, ok1 := l.peek()
				b2, ok2 := l.peekN(1)
				if !ok1 || !ok2 {
					if l.interactive {
						return "", l.errIncomplete("hex escape was not terminated (expect 2 hex digits)")
					}
					return "", l.err("hex escape was not terminated (expect 2 hex digits)")
				}
				d1, d2 := toHex(b1), toHex(b2)
				if d1 < 0 || d2 < 0 {
					return "", l.err("invalid hex escape")
				}
				out = append(out, byte((d1<<4)|d2))
				l.advance()
				l.advance()

			case 'u':
				// expect 4 hex digits
				val := 0
				for i := 0; i < 4; i++ {
					b, ok := l.peek()
					if !ok {
						if l.interactive {
							return "", l.errIncomplete("unicode escape was not terminated (expect 4 hex digits)")
						}
						return "", l.err("unicode escape was not terminated (expect 4 hex digits)")
					}
					d := toHex(b)
					if d < 0 {
						return "", l.err("invalid unicode escape")
					}
					val = (val << 4) | d
					l.advance()
				}
				r := rune(val)

				// handle surrogate pair \uD800-\uDBFF followed by \uDC00-\uDFFF
				if 0xD800 <= r && r <= 0xDBFF {
					saveCur := l.cur
					saveLine, saveCol := l.line, l.col
					if b1, ok := l.peek(); ok && b1 == '\\' {
						l.advance()
						if b2, ok := l.peek(); ok && b2 == 'u' {
							l.advance()
							val2 := 0
							for i := 0; i < 4; i++ {
								b, ok := l.peek()
								if !ok {
									if l.interactive {
										return "", l.errIncomplete("unicode surrogate pair low was not terminated")
									}
									return "", l.err("unicode surrogate pair low was not terminated")
								}
								d := toHex(b)
								if d < 0 {
									return "", l.err("invalid unicode surrogate pair low")
								}
								val2 = (val2 << 4) | d
								l.advance()
							}
							r2 := rune(val2)
							if 0xDC00 <= r2 && r2 <= 0xDFFF {
								cp := utf16.DecodeRune(r, r2)
								appendRune(cp)
								continue
							}
						}
					}
					// not a valid pair; rewind and just emit r
					l.cur = saveCur
					l.line, l.col = saveLine, saveCol
				}
				appendRune(r)

			default:
				return "", l.err(fmt.Sprintf("invalid escape sequence: \\%c", esc))
			}
			continue
		}

		// normal char; keep source UTF-8 validity, but emit raw bytes
		if ch < utf8.RuneSelf {
			out = append(out, ch)
			continue
		}

		// Non-ASCII: validate UTF-8 in *source* and copy the exact bytes.
		l.cur-- // step back 1 byte to decode from the rune boundary
		r, size := utf8.DecodeRuneInString(l.src[l.cur:])
		if r == utf8.RuneError && size == 1 {
			return "", l.err("invalid UTF-8 in source")
		}
		out = append(out, l.src[l.cur:l.cur+size]...)
		l.cur += size
		l.col += size - 1
	}

	if l.interactive {
		return "", l.errIncomplete("string was not terminated")
	}
	return "", l.err("string was not terminated")
}

// scanIdentifier parses [A-Za-z_][A-Za-z0-9_]*
func (l *Lexer) scanIdentifier() string {
	for {
		b, ok := l.peek()
		if !ok || !isAlphaNum(b) {
			break
		}
		l.advance()
	}
	return l.src[l.start:l.cur]
}

// scanNumber parses integer or float; supports .5, 1., 1.23e-4, etc.
// Precondition: current position is at l.start and the first rune is either a digit
// or a '.' that has already been verified to start a number.
func (l *Lexer) scanNumber() (tok TokenType, lit interface{}, err error) {
	sawDigits := false
	sawDot := false
	sawExp := false

	startCur := l.cur

	// --- Base-prefixed integers: 0b..., 0o..., 0x...
	if b0, ok := l.peek(); ok && b0 == '0' {
		// Tentatively consume '0' and inspect next char
		l.advance()
		if pfx, ok2 := l.peek(); ok2 && (pfx == 'b' || pfx == 'B' || pfx == 'o' || pfx == 'O' || pfx == 'x' || pfx == 'X') {
			l.advance()
			var pred func(byte) bool
			var base int
			switch pfx {
			case 'b', 'B':
				pred, base = isBin, 2
			case 'o', 'O':
				pred, base = isOct, 8
			default:
				pred, base = isHex, 16
			}
			had, endedUnd, usErr := l.readDigitsUS(pred)
			if usErr != nil {
				return ILLEGAL, nil, usErr
			}
			if !had || endedUnd {
				return ILLEGAL, nil, l.err("invalid integer literal")
			}
			lex := l.src[l.start:l.cur]
			valStr := stripUnderscores(lex[2:]) // strip digits after prefix
			v, convErr := strconv.ParseInt(valStr, base, 64)
			if convErr != nil {
				return ILLEGAL, nil, l.err("invalid integer literal")
			}
			return INTEGER, v, nil
		}
		// Not a base prefix → decimal starting with 0.
		// If next is another digit or underscore, it's a leading-zeros error
		// unless we immediately form a float/exponent (e.g., "0.5", "0e10").
		if nb, ok3 := l.peek(); ok3 && (nb == '_' || isDigit(nb)) {
			return ILLEGAL, nil, l.err("leading zeros not allowed in decimal integer")
		}
		// Else keep sawDigits=true and let fraction/exp logic run (for "0.5", "0e10"),
		// or finalize as integer zero if neither appears.
		sawDigits = true
	}

	// integral part (optional for leading '.'); allow underscores
	if !sawDigits {
		had, endedUnd, usErr := l.readDigitsUS(isDigit)
		if usErr != nil {
			return ILLEGAL, nil, usErr
		}
		sawDigits = had
		if endedUnd {
			return ILLEGAL, nil, l.err("misplaced '_' in number")
		}
	}

	// fractional part
	if b, ok := l.peek(); ok && b == '.' {
		// Do NOT consume '.' if after property or doubled '..'
		if !(l.afterDotIsProperty()) {
			if b2, ok2 := l.peekN(1); !(ok2 && b2 == '.') {
				// disallow underscore immediately before '.'
				if l.cur > startCur && l.src[l.cur-1] == '_' {
					return ILLEGAL, nil, l.err("misplaced '_' in number")
				}
				l.advance() // '.'
				sawDot = true
				// fraction digits with underscores; must start with a digit (no leading '_')
				had, endedUnd, usErr := l.readDigitsUS(isDigit)
				if usErr != nil {
					return ILLEGAL, nil, usErr
				}
				// It's valid to have no fractional digits only if we will have an exponent later.
				if endedUnd {
					return ILLEGAL, nil, l.err("misplaced '_' in number")
				}
				if had {
					sawDigits = true
				}
			}
		}
	}

	// exponent part: ([eE][+-]?digits with underscores)?
	if b, ok := l.peek(); ok && (b == 'e' || b == 'E') {
		save := l.cur
		l.advance() // e/E
		if b2, ok := l.peek(); ok && (b2 == '+' || b2 == '-') {
			l.advance()
		}
		// disallow '_' immediately after 'e' or sign
		if b3, ok := l.peek(); ok && b3 == '_' {
			return ILLEGAL, nil, l.err("invalid float literal")
		}
		// first exponent char must be a digit
		if b3, ok := l.peek(); ok && isDigit(b3) {
			had, endedUnd, usErr := l.readDigitsUS(isDigit)
			if usErr != nil {
				return ILLEGAL, nil, usErr
			}
			if !had || endedUnd {
				return ILLEGAL, nil, l.err("invalid float literal")
			}
			sawExp = true
		} else {
			// no exponent digits → backtrack
			l.cur = save
		}
	}

	if !sawDigits {
		return ILLEGAL, nil, l.err("malformed number")
	}

	lex := l.src[l.start:l.cur]

	// If it's a pure decimal integer (no dot/exp), enforce no leading zeros (except "0").
	if !sawDot && !sawExp {
		decStr := stripUnderscores(lex)
		if len(decStr) > 1 && decStr[0] == '0' {
			return ILLEGAL, nil, l.err("leading zeros not allowed in decimal integer")
		}
		v, convErr := strconv.ParseInt(decStr, 10, 64)
		if convErr != nil {
			return ILLEGAL, nil, l.err("invalid integer literal")
		}
		return INTEGER, v, nil
	}

	// Float: strip underscores before ParseFloat
	vf, convErr := strconv.ParseFloat(stripUnderscores(lex), 64)
	if convErr != nil {
		return ILLEGAL, nil, l.err("invalid float literal")
	}
	return NUMBER, vf, nil
}

// scanAnnotation captures consecutive lines that start with '#' (ignoring leading spaces).
// Terminates on blank line or a line that does not begin (after spaces) with '#'.
func (l *Lexer) scanAnnotation() (string, error) {
	// current '#' is line-start iff only hspace since previous '\n'
	lineStart := true
	for i := l.start - 1; i >= 0 && l.src[i] != '\n'; i-- {
		if l.src[i] != ' ' && l.src[i] != '\t' && l.src[i] != '\r' {
			lineStart = false
			break
		}
	}
	var bldr strings.Builder

	// helper: check if the *next* line (after current '\n') starts with '#'
	nextLineStartsWithHash := func() bool {
		// we are currently positioned at the '\n' (unconsumed)
		probe := l.cur + 1
		for probe < len(l.src) {
			c := l.src[probe]
			if c == ' ' || c == '\t' || c == '\r' {
				probe++
				continue
			}
			break
		}
		return probe < len(l.src) && l.src[probe] == '#'
	}

	// Trim at most one ASCII space (NOT tab) after the first '#'.
	if b, ok := l.peek(); ok && b == ' ' {
		l.advance()
	}

	// ----- capture first line, but DO NOT consume its trailing '\n' -----
	for {
		b, ok := l.peek()
		if !ok || b == '\n' {
			// do not l.advance() here; leave '\n' in the stream
			bldr.WriteByte('\n')
			break
		}
		bldr.WriteByte(b)
		l.advance()
	}

	// Only coalesce with following '#'-lines when *this* '#' was line-start.
	// (Inline '# ...' must not merge with the next line's '# ...')
	if b, ok := l.peek(); ok && b == '\n' && lineStart && nextLineStartsWithHash() {
		l.advance() // consume newline to move to start of the next line
	} else {
		// single-line block; leave the '\n' unconsumed
		s := bldr.String()
		if len(s) == 0 {
			return "", errors.New("incomplete annotation")
		}
		for len(s) > 0 && s[len(s)-1] == '\n' {
			s = s[:len(s)-1]
		}
		return s, nil
	}

	// ----- capture subsequent '#'-prefixed lines -----
	consumeHashOnLine := func() (bool, error) {
		for {
			b, ok := l.peek()
			if !ok || b == '\n' {
				break
			}
			if b == ' ' || b == '\t' || b == '\r' {
				l.advance()
				continue
			}
			break
		}
		b, ok := l.peek()
		if !ok || b != '#' {
			return false, nil
		}
		l.advance() // '#'
		// Trim at most one ASCII space (NOT tab) after '#'
		if b2, ok2 := l.peek(); ok2 && b2 == ' ' {
			l.advance()
		}
		return true, nil
	}

	for {
		save := l.cur
		cont, err := consumeHashOnLine()
		if err != nil {
			return "", err
		}
		if !cont {
			l.cur = save
			break
		}

		// read rest of line, but DO NOT consume trailing '\n'
		for {
			b, ok := l.peek()
			if !ok || b == '\n' {
				bldr.WriteByte('\n')
				break
			}
			bldr.WriteByte(b)
			l.advance()
		}

		// If another '#'-line follows, eat this '\n' and continue; else stop (leave '\n').
		if b, ok := l.peek(); ok && b == '\n' && nextLineStartsWithHash() {
			l.advance() // consume newline to move to next line
		} else {
			break // leave final '\n' unconsumed
		}
	}

	s := bldr.String()
	if len(s) == 0 {
		return "", errors.New("incomplete annotation")
	}
	for len(s) > 0 && s[len(s)-1] == '\n' {
		s = s[:len(s)-1]
	}
	return s, nil
}

// --- hash/comment helpers ---

// handleSingleHash processes '#' annotations (inline or multiline).
// Returns (producedAnnotation, text, err).
func (l *Lexer) handleSingleHash() (bool, string, error) {
	annot, err := l.scanAnnotation()
	if err != nil {
		return false, "", l.err("incomplete annotation")
	}
	return true, annot, nil
}

// scanNoopIfPresent emits a NOOP token if the source at the current position
// matches: '\n' ( hws* '\n' )+  where hws ∈ {' ', '\r', '\t'}.
func (l *Lexer) scanNoopIfPresent() (Token, bool) {
	b, ok := l.peek()
	if !ok || b != '\n' {
		return Token{}, false
	}

	// Snapshot so we can roll back on non-match (i.e., if we don't see at least one repetition).
	saveCur, saveLine, saveCol, saveStart := l.cur, l.line, l.col, l.start

	// Token starts here.
	l.tokStartLine = l.line
	l.tokStartCol = l.col
	l.start = l.cur

	// Consume the initial newline.
	l.advance()

	matchedReps := 0
	for {
		// Consume zero or more horizontal whitespace chars.
		for {
			nb, ok := l.peek()
			if !ok || (nb != ' ' && nb != '\t' && nb != '\r') {
				break
			}
			l.advance()
		}
		// Require a newline to complete one (hws* '\n') repetition.
		nb, ok := l.peek()
		if ok && nb == '\n' {
			l.advance()
			matchedReps++
			continue
		}
		break
	}

	if matchedReps >= 1 {
		return l.addToken(NOOP, nil), true
	}

	// Not a full match: restore and report no-op not present.
	l.cur, l.line, l.col, l.start = saveCur, saveLine, saveCol, saveStart
	return Token{}, false
}

// --- misc helpers ---

func (l *Lexer) dotStartsNumber() bool {
	b, ok := l.peek()
	if !ok || !isDigit(b) {
		return false
	}
	prev := l.previousToken()
	if l.immediateWhitespaceBefore() || prev == nil || !canBeLeftOperand(prev.Type) {
		return true
	}
	return false
}

// ---------------- main tokenization ----------------

func (l *Lexer) scanToken() (Token, error) {
	for {
		l.skipHorizontalWhitespace()
		l.tokStartLine = l.line
		l.tokStartCol = l.col
		l.start = l.cur

		if l.isAtEnd() {
			return l.addToken(EOF, nil), nil
		}

		// No-op (blank lines)
		if b, _ := l.peek(); b == '\n' {
			if tok, ok := l.scanNoopIfPresent(); ok {
				return tok, nil
			}
			// Lone newline (or newline followed by non-hws) → skip it as whitespace and loop.
			l.advance()
			l.start = l.cur
			continue
		}

		// Treat semicolons exactly like newlines for layout/same-line logic.
		// They are NOT tokens; we "lower" them to line breaks.
		if b, _ := l.peek(); b == ';' {
			// Consume ';' and advance line/col as if it were '\n'.
			l.advance()     // move past ';'
			l.line++        // start a new line
			l.col = 0       // reset column
			l.start = l.cur // next token starts after the semicolon
			continue
		}

		ch, _ := l.advance()

		// Single-char tokens & punctuation with whitespace-sensitive "(" and "["
		switch ch {
		case '(':
			if l.immediateWhitespaceBefore() {
				return l.addToken(LROUND, "("), nil
			}
			return l.addToken(CLROUND, "("), nil
		case ')':
			return l.addToken(RROUND, ")"), nil
		case '[':
			if l.immediateWhitespaceBefore() {
				return l.addToken(LSQUARE, "["), nil
			}
			return l.addToken(CLSQUARE, "["), nil
		case ']':
			return l.addToken(RSQUARE, "]"), nil
		case '{':
			return l.addToken(LCURLY, "{"), nil
		case '}':
			return l.addToken(RCURLY, "}"), nil
		case '+':
			return l.addToken(PLUS, "+"), nil
		case '*':
			// Support '**' (power) before single '*' (MULT)
			if b, ok := l.peek(); ok && b == '*' {
				l.advance()
				return l.addToken(POW, "**"), nil
			}
			return l.addToken(MULT, "*"), nil
		case '/':
			return l.addToken(DIV, "/"), nil
		case '%':
			return l.addToken(MOD, "%"), nil
		case '&':
			return l.addToken(BITAND, "&"), nil
		case '|':
			return l.addToken(BITOR, "|"), nil
		case '^':
			return l.addToken(BITXOR, "^"), nil
		case '~':
			return l.addToken(BITNOT, "~"), nil
		case ':':
			return l.addToken(COLON, ":"), nil
		case ',':
			return l.addToken(COMMA, ","), nil
		case '?':
			return l.addToken(QUESTION, "?"), nil
		}

		// '.' : either decimal-starting float or PERIOD
		if ch == '.' {
			if l.dotStartsNumber() {
				l.rewindToStart()
				tt, lit, err := l.scanNumber()
				if err != nil {
					return Token{}, err
				}
				return l.addToken(tt, lit), nil
			}
			return l.addToken(PERIOD, "."), nil
		}

		// Two-char operators and fallbacks
		switch ch {
		case '-':
			if b, ok := l.peek(); ok && b == '>' {
				l.advance()
				return l.addToken(ARROW, "->"), nil
			}
			return l.addToken(MINUS, "-"), nil
		case '=':
			if b, ok := l.peek(); ok && b == '=' {
				l.advance()
				return l.addToken(EQ, "=="), nil
			}
			return l.addToken(ASSIGN, "="), nil
		case '!':
			if b, ok := l.peek(); ok && b == '=' {
				l.advance()
				return l.addToken(NEQ, "!="), nil
			}
			return l.addToken(BANG, "!"), nil
		case '<':
			// Prefer shift '<<' over '<=' when present
			if b, ok := l.peek(); ok {
				if b == '<' {
					l.advance()
					return l.addToken(LSHIFT, "<<"), nil
				}
				if b == '=' {
					l.advance()
					return l.addToken(LESS_EQ, "<="), nil
				}
			}
			return l.addToken(LESS, "<"), nil
		case '>':
			// Prefer shift '>>' over '>=' when present
			if b, ok := l.peek(); ok {
				if b == '>' {
					l.advance()
					return l.addToken(RSHIFT, ">>"), nil
				}
				if b == '=' {
					l.advance()
					return l.addToken(GREATER_EQ, ">="), nil
				}
			}
			return l.addToken(GREATER, ">"), nil
		}

		// Annotations
		if ch == '#' {
			ok, text, err := l.handleSingleHash()
			if err != nil {
				return Token{}, err
			}
			if ok {
				return l.addToken(ANNOTATION, text), nil
			}
		}

		// Strings
		if ch == '"' || ch == '\'' {
			l.rewindToStart()
			text, err := l.scanString()
			if err != nil {
				return Token{}, err
			}
			// After '.' a quoted key becomes ID (property name)
			if l.afterDotIsProperty() {
				return l.addToken(ID, text), nil
			}
			return l.addToken(STRING, text), nil
		}

		// Numbers (starting with digit)
		if isDigit(ch) {
			l.rewindToStart()
			tt, lit, err := l.scanNumber()
			if err != nil {
				return Token{}, err
			}
			return l.addToken(tt, lit), nil
		}

		// Identifiers / Keywords
		if isAlpha(ch) {
			l.rewindToStart()
			lex := l.scanIdentifier()
			// After '.', treat as property name (ID)
			if l.afterDotIsProperty() {
				return l.addToken(ID, lex), nil
			}
			if tt, ok := keywords[lex]; ok {
				switch tt {
				case NULL:
					return l.addToken(NULL, nil), nil
				case BOOLEAN:
					if lex == "true" {
						return l.addToken(BOOLEAN, true), nil
					}
					return l.addToken(BOOLEAN, false), nil
				default:
					return l.addToken(tt, nil), nil
				}
			}
			return l.addToken(ID, lex), nil
		}

		return Token{}, l.err(fmt.Sprintf("unexpected character: %q", ch))
	}
}

=== END FILE: ./mindscript/lexer.go ===

=== BEGIN FILE: ./mindscript/printer.go ===
// printer.go: pretty-printers for MindScript ASTs, types, and runtime values.
//
// What this file does
// -------------------
// This module provides the formatting layer for MindScript. It renders two
// kinds of data to human-readable, stable strings:
//
//  1. Parsed source ASTs (S-expressions) → MindScript source code.
//     - Entry points: Pretty, Standardize, FormatSExpr.
//     - Produces whitespace- and newline-stable output with minimal
//     parentheses, based on operator precedence. It understands all
//     statement and expression tags emitted by the parser (e.g. "fun",
//     "oracle", "for", "if/elif/else", "type", "block", "assign",
//     "return/break/continue", arrays, maps, calls, indexing, properties,
//     unary and binary operators).
//     - Annotation nodes use the simplified 3-ary form:
//     ("annot", ("str", text), wrappedNode)
//     PRE/POST is *not* encoded by the parser anymore. All annotations are
//     attached to values (or noops), and the pretty-printer decides PRE vs POST
//     based on layout at binding sites.
//     - Formatting emits no space before '(' for calls and for 'fun(...)'
//     and 'oracle(...)' parameter lists, matching the lexer’s CLROUND rule.
//     - Control keywords render without parens:
//     return expr
//     break expr
//     continue [expr]
//     A `null` payload prints as the bare keyword (e.g., `continue`).
//
//  2. Type ASTs (S-expressions) → compact type strings.
//     - Entry point: FormatType.
//     - Supported forms:
//     ("id", "Any"|"Null"|"Bool"|"Int"|"Num"|"Str"|"Type")
//     ("unop","?", T)         → prints as `T?`
//     ("array", T)            → prints as `[T]`
//     ("map", ("pair"| "pair!", ("str",k), T) ...)
//     Required fields print with a trailing `!` on the key.
//     Value annotations (if wrapped in "annot") are respected and decided
//     PRE vs POST by the same centralized policy as expressions.
//     ("enum", literalS... )  → prints as `Enum[ ... ]`, where members
//     may be scalars, arrays, or maps.
//     ("binop","->", A, B)    → prints as `(A) -> B`, flattened across
//     right-associated chains.
//     - Output is stable. Multi-line maps are rendered with sorted keys to
//     avoid visual churn.
//     - When the last field ends with a POST, the closing `}` appears on the
//     next line without an extra blank line.
//
// Dependencies (other files)
// --------------------------
// • parser.go
//   - S = []any (AST payload shape)
//   - ParseSExpr(string) (used by Pretty/Standardize)
//   - AST tags: "block", "fun", "oracle", "for", "if",
//     "type", "return", "break", "continue", "assign", "let", "array", "map",
//     "pair"/"pair!", "get", "idx", "call", "id", "str", "int", "num", "bool",
//     "null", "unop", "binop", "annot", "noop".
//
// • interpreter.go (runtime model)
//   - Value, ValueTag (VTNull, VTBool, VTInt, VTNum, VTStr, VTArray, VTMap,
//     VTFun, VTType, VTModule, VTHandle)
//   - Fun, TypeValue, MapObject (Entries/Keys).
//
// • modules.go (module loader)
//   - Module struct and prettySpec(string) (used for VTModule display).
//
// • errors.go (shared errors)
//   - WrapErrorWithSource(err, src) (used by Pretty/Standardize).
//
// PUBLIC vs PRIVATE layout
// ------------------------
// This file is organized in two blocks:
//  1. PUBLIC: the user-facing constants & functions with thorough docstrings.
//  2. PRIVATE: helper types and functions that implement the printers.
//
// Formatting policy highlights
// ----------------------------
//   - Indentation uses **tabs** only (gofmt-style).
//   - Canonical output (`Standardize`) ends with exactly one trailing '\n'.
//
// Requiredness in value maps
// --------------------------
// The printer never emits required fields ("pair!") in **expression/value maps**:
// requiredness is a **type-level** concept only. If the AST carried "pair!" in a
// value map (e.g., via parser sugar), it is dropped in the printed code.
//
// Canonicalizations & Omissions (parser ↔ printer contract)
// ---------------------------------------------------------
// These are deliberate simplifications made by the parser and normalized by
// the printer; users may not see certain syntactic sugar re-emitted:
//   - Param types default to `Any` and are not printed (e.g., `fun(x)` not `x: Any`).
//   - Function return type defaults to `Any` and is not printed (`fun(...) do ... end`
//     without `-> Any`).
//   - `oracle(...)` without `from` carries an empty default source; `from ...` is omitted.
//   - Bare `return` / `break` / `continue` carry an implicit `null` value and print
//     as the bare keyword (no `null`).
//   - Redundant parentheses are removed; only minimal parentheses are emitted.
//   - Calls print with no space before '(' (canonical `f(x)` form).
//   - Property indices written as `obj.(expr)` or `obj.12` are printed canonically
//     as `obj[expr]` / `obj[12]`.
//   - Trailing commas in arrays/maps/parameter lists are dropped in output.
//   - Map keys that are identifier-like print without quotes; others are quoted.
//   - **Expression maps** ignore the required marker `!` at runtime; the printer
//     therefore **drops `!` in value maps** (e.g., `{ id!: 1 }` → `{ id: 1 }`).
//     (Type maps still print required keys as `key!`.)
package mindscript

import (
	"fmt"
	"strconv"
	"strings"
	"unicode/utf8"
)

// ==============================
// ========== PUBLIC ============
// ==============================

// MaxInlineWidth controls when arrays/maps are rendered on a single line by
// FormatValue / FormatType / FormatSExpr. The single-line decision accounts for
// the current indentation; i.e., it uses the remaining space on the line after
// tabs (tab width = 4) and any preceding text.
var MaxInlineWidth = 80

// Pretty parses a MindScript source string and returns a formatted version.
//
// Behavior:
//   - Parses src via ParseSExpr. If parsing fails, the error is wrapped with
//     source context via WrapErrorWithSource.
//   - On success, pretty-prints the AST using FormatSExpr, producing stable,
//     whitespace-normalized code with minimal parentheses.
//   - Supports annotations using the 3-ary form:
//     ("annot", ("str", text), X)
//     PRE/POST is chosen by the pretty-printer at binding sites.
//
// Errors:
//   - Returns a non-nil error if parsing fails; otherwise returns the formatted text.
func Pretty(src string) (string, error) {
	ast, err := ParseSExpr(src)
	if err != nil {
		if e, ok := err.(*Error); ok {
			if e.Src == nil {
				e.Src = &SourceRef{Name: "<main>", Src: src}
			}
			return "", fmt.Errorf("%s", FormatError(e))
		}
		return "", err
	}
	return FormatSExpr(ast), nil
}

// Standardize returns the canonical source form:
//   - deterministic layout
//   - indentation using tabs
//   - exactly one trailing newline
//
// It is equivalent to Pretty(src), but ensures precisely one '\n' at the end.
func Standardize(src string) (string, error) {
	ast, err := ParseSExpr(src)
	if err != nil {
		if e, ok := err.(*Error); ok {
			if e.Src == nil {
				e.Src = &SourceRef{Name: "<standardize>", Src: src}
			}
			return "", fmt.Errorf("%s", FormatError(e))
		}
		return "", err
	}
	out := FormatSExpr(ast)
	if !strings.HasSuffix(out, "\n") {
		out += "\n"
	} else {
		out = strings.TrimRight(out, "\n") + "\n"
	}
	return out, nil
}

// FormatSExpr renders a parsed MindScript AST (S-expr) to a stable source string.
//
// Inputs:
//   - n: an AST produced by parser.go (e.g., the result of ParseSExpr).
//
// Output policy:
//   - Statements (fun/oracle/for/if/type/block/return/break/continue/assign)
//     are rendered with keywords and indentation.
//   - Expressions use minimal parentheses according to a fixed precedence table;
//     property access vs calls/indexing binds tightly.
//   - Arrays and maps are printed inline (AST form).
//   - Annotation nodes wrap the printed construct; PRE vs POST is chosen centrally.
//   - **POST-after-separator rule** is enforced for inline cases.
//
// This function does not parse; it strictly formats the provided AST.
func FormatSExpr(n S) string {
	doc := docProgram(n)
	var b strings.Builder
	r := renderer{
		out:      &b,
		maxWidth: MaxInlineWidth,
		tabWidth: 4,
	}
	r.render(doc)
	return strings.TrimRight(b.String(), "\n")
}

// FormatType renders a type S-expression into a compact, human-readable string.
// It uses the same centralized PRE/POST policy for value annotations inside
// type maps and enum literals.
func FormatType(t S) string {
	doc := docType(t)
	var b strings.Builder
	r := renderer{
		out:      &b,
		maxWidth: MaxInlineWidth,
		tabWidth: 4,
	}
	r.render(doc)
	return b.String()
}

// FormatValue renders a runtime Value by first adapting it to the printer’s AST
// (with cycle guards and opaque fallbacks) and then delegating to FormatSExpr.
func FormatValue(v Value) string {
	ast := ValueToAST(v)
	return FormatSExpr(ast)
}

//// END_OF_PUBLIC

// ===============================
// ========= PRIVATE =============
// ===============================

/* ---------- small globals & utilities ---------- */

func isIdent(s string) bool {
	if s == "" {
		return false
	}
	b := []byte(s)
	c := b[0]
	if !((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') || c == '_') {
		return false
	}
	for i := 1; i < len(b); i++ {
		c = b[i]
		if !((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') || (c >= '0' && c <= '9') || c == '_') {
			return false
		}
	}
	return true
}

// isUnsafeLiteralRune reports runes that must always be escaped in string
// *source code* for correctness and security. Normal printable Unicode
// (letters, emoji, CJK, etc.) is left as UTF-8.
func isUnsafeLiteralRune(r rune) bool {
	// ASCII control characters (except those handled by short escapes above)
	if r < 0x20 || r == 0x7F {
		return true
	}
	// C1 controls
	if r >= 0x80 && r <= 0x9F {
		return true
	}
	// Unicode line / paragraph separators
	if r == 0x2028 || r == 0x2029 {
		return true
	}
	// Bidi controls
	if (r >= 0x202A && r <= 0x202E) || (r >= 0x2066 && r <= 0x2069) {
		return true
	}
	// Directional marks
	if r == 0x200E || r == 0x200F {
		return true
	}
	// Zero-width / invisible format characters
	switch r {
	case 0x200B, // ZWSP
		0x200C, // ZWNJ
		0x200D, // ZWJ
		0xFEFF: // BOM / ZWNBSP
		return true
	}
	return false
}

func quoteString(s string) string {
	var b strings.Builder
	b.WriteByte('"')
	// Unified printer: walk bytes, decoding UTF-8 when possible.
	// Invalid UTF-8 bytes are emitted as \xHH so Str can round-trip as raw bytes.
	for i := 0; i < len(s); {
		c := s[i]

		// Fast-path ASCII (also covers all short escapes).
		if c < 0x80 {
			switch c {
			case '\\':
				b.WriteString(`\\`)
			case '"':
				b.WriteString(`\"`)
			case '\n':
				b.WriteString(`\n`)
			case '\r':
				b.WriteString(`\r`)
			case '\t':
				b.WriteString(`\t`)
			case '\b':
				b.WriteString(`\b`)
			case '\f':
				b.WriteString(`\f`)
			default:
				// Printable ASCII.
				if c >= 0x20 && c <= 0x7E {
					b.WriteByte(c)
				} else {
					// Control bytes must be escaped for visibility/safety.
					b.WriteString(fmt.Sprintf(`\x%02X`, c))
				}
			}
			i++
			continue
		}

		// Non-ASCII: decode a rune if valid UTF-8; otherwise treat as a raw byte.
		r, size := utf8.DecodeRuneInString(s[i:])
		if r == utf8.RuneError && size == 1 {
			b.WriteString(fmt.Sprintf(`\x%02X`, c))
			i++
			continue
		}
		i += size

		// For string *literals* in source, escape control / security-sensitive
		// runes so they are visible and cannot be smuggled into the code.
		if isUnsafeLiteralRune(r) {
			if r <= 0xFFFF {
				b.WriteString(fmt.Sprintf(`\u%04X`, r))
			} else {
				b.WriteRune(r)
			}
		} else {
			b.WriteRune(r)
		}
	}
	b.WriteByte('"')
	return b.String()
}

func oneLine(s string) string {
	s = strings.ReplaceAll(s, "\n", " ")
	return strings.TrimSpace(s)
}

// unwrap VTType payload to its AST (supports legacy S too).
func typeAst(data any) S {
	switch tv := data.(type) {
	case *TypeValue:
		return tv.Ast
	case S:
		return tv
	default:
		return S{}
	}
}

// NOTE: The parser no longer encodes PRE vs POST; all annotations live on values.
// The pretty-printer chooses PRE vs POST purely by layout at binding sites.

/* ---------- Doc engine (tiny) ---------- */

type docKind int

const (
	dText     docKind = iota
	dLine             // space if flat, newline if broken
	dSoftLine         // empty if flat, newline if broken
	dHardLine         // always newline
	dGroup
	dNest
	dConcat
)

type Doc struct {
	k      docKind
	s      string
	a      *Doc
	kids   []*Doc
	indent int // for Nest
}

func Text(s string) *Doc      { return &Doc{k: dText, s: s} }
func LineDoc() *Doc           { return &Doc{k: dLine} }
func SoftLineDoc() *Doc       { return &Doc{k: dSoftLine} }
func HardLineDoc() *Doc       { return &Doc{k: dHardLine} }
func Group(d *Doc) *Doc       { return &Doc{k: dGroup, a: d} }
func Nest(n int, d *Doc) *Doc { return &Doc{k: dNest, a: d, indent: n} }
func Concat(ds ...*Doc) *Doc  { return &Doc{k: dConcat, kids: ds} }

func Join(sep *Doc, items []*Doc) *Doc {
	if len(items) == 0 {
		return Concat()
	}
	out := make([]*Doc, 0, len(items)*2-1)
	for i, it := range items {
		if i > 0 {
			out = append(out, sep)
		}
		out = append(out, it)
	}
	return Concat(out...)
}

type renderer struct {
	out      *strings.Builder
	maxWidth int
	tabWidth int

	col         int  // current column in characters (tabs count as tabWidth)
	depth       int  // indentation depth (tabs)
	atLineStart bool // just after newline
}

func (r *renderer) writeIndentIfNeeded() {
	if r.atLineStart {
		for i := 0; i < r.depth; i++ {
			r.out.WriteByte('\t')
		}
		r.col = r.depth * r.tabWidth
		r.atLineStart = false
	}
}
func (r *renderer) writeString(s string) {
	if s == "" {
		return
	}
	r.writeIndentIfNeeded()
	r.out.WriteString(s)
	r.col += len(s)
}
func (r *renderer) newline() {
	r.out.WriteByte('\n')
	r.atLineStart = true
	// col will be set when indent is written
}

func (r *renderer) render(d *Doc) {
	r.atLineStart = false // caller controls leading indentation
	r.renderGroup(d)
}

func (r *renderer) renderGroup(d *Doc) {
	// Render a group with "flat if fits" policy.
	if r.fitsFlat(d, r.maxWidth-r.col) {
		r.renderFlat(d)
	} else {
		r.renderBroken(d)
	}
}

func (r *renderer) renderFlat(d *Doc) {
	switch d.k {
	case dText:
		r.writeString(d.s)
	case dLine:
		r.writeString(" ")
	case dSoftLine:
		// nothing
	case dHardLine:
		// hard line cannot appear in flat mode if fitsFlat was true,
		// but guard just in case: break the line.
		r.newline()
	case dGroup:
		r.renderFlat(d.a)
	case dNest:
		old := r.depth
		r.depth += d.indent
		r.renderFlat(d.a)
		r.depth = old
	case dConcat:
		for _, k := range d.kids {
			r.renderFlat(k)
		}
	}
}

func (r *renderer) renderBroken(d *Doc) {
	switch d.k {
	case dText:
		r.writeString(d.s)
	case dLine:
		r.newline()
	case dSoftLine:
		r.newline()
	case dHardLine:
		r.newline()
	case dGroup:
		// In broken mode, nested groups still try flat if they fit at this point.
		r.renderGroup(d.a)
	case dNest:
		old := r.depth
		r.depth += d.indent
		r.renderBroken(d.a)
		r.depth = old
	case dConcat:
		for _, k := range d.kids {
			r.renderBroken(k)
		}
	}
}

// fitsFlat reports whether the doc can be rendered flat within the given budget.
// Any HardLine inside makes it not flat-fit.
func (r *renderer) fitsFlat(d *Doc, budget int) bool {
	if budget < 0 {
		return false
	}
	switch d.k {
	case dText:
		return len(d.s) <= budget
	case dLine:
		return 1 <= budget
	case dSoftLine:
		return 0 <= budget
	case dHardLine:
		return false
	case dGroup:
		return r.fitsFlat(d.a, budget)
	case dNest:
		return r.fitsFlat(d.a, budget)
	case dConcat:
		for _, k := range d.kids {
			if !r.fitsFlat(k, budget) {
				return false
			}
			// reduce budget by flat width of k
			budget -= flatWidth(k)
		}
		return true
	default:
		return false
	}
}

func flatWidth(d *Doc) int {
	switch d.k {
	case dText:
		return len(d.s)
	case dLine:
		return 1
	case dSoftLine:
		return 0
	case dHardLine:
		return 1 // arbitrary; but any hardline makes fitsFlat false before using this
	case dGroup:
		return flatWidth(d.a)
	case dNest:
		return flatWidth(d.a)
	case dConcat:
		sum := 0
		for _, k := range d.kids {
			sum += flatWidth(k)
		}
		return sum
	default:
		return 0
	}
}

/* ---------- shared Doc helpers ---------- */

func idOrQuoted(name string) *Doc {
	if isIdent(name) {
		return Text(name)
	}
	return Text(quoteString(name))
}

// PRE annotations (block/head) — prints as lines above current position.
func annotPre(text string) *Doc {
	if strings.TrimSpace(text) == "" {
		return Concat()
	}
	lines := strings.Split(text, "\n")
	ds := make([]*Doc, 0, len(lines)*2)
	for _, ln := range lines {
		ln = strings.TrimSpace(ln)
		ds = append(ds, Text("# "+ln), HardLineDoc())
	}
	return Concat(ds...)
}

// POST annotations (inline/trailing) — prints on the same line.
// IMPORTANT: POST captures the rest of the line, so we force a newline here.
func annotInline(text string) *Doc {
	trim := oneLine(text)
	if trim == "" {
		return Concat()
	}
	return Concat(Text(" # "+trim), HardLineDoc())
}

func braced(open string, inside *Doc, close string) *Doc {
	return Concat(Text(open), inside, Text(close))
}

// Minimal entry builder; annotation handling is centralized elsewhere.
func kvEntry(keyDoc *Doc, valDoc *Doc) *Doc {
	return Concat(keyDoc, Text(": "), valDoc)
}

/* ---------- Comma-aware joining (centralized POST-after-comma logic) ---------- */

type sepItem struct {
	before *Doc // layout that should appear immediately before main
	main   *Doc // rendered item (element or entry) without its trailing POST
	post   string
	after  *Doc // layout that should appear after this item (before next item/closer)
}

// concatDocs flattens two doc fragments, avoiding nested empty concats.
func concatDocs(a, b *Doc) *Doc {
	if a == nil {
		return b
	}
	if b == nil {
		return a
	}
	return Concat(a, b)
}

// layoutDocForNode returns a pure-layout doc for nodes that represent blank
// lines or comment-only entries inside arrays/maps/enums/patterns.
func layoutDocForNode(n S) *Doc {
	switch tag(n) {
	case "noop":
		// Blank line inside a collection.
		return HardLineDoc()
	case "annot":
		txt, wrapped, _ := asAnnotASTRaw(n)
		if tag(wrapped) == "noop" {
			// Comment-only line inside a collection.
			return annotPre(txt)
		}
	}
	return nil
}

// buildSepItems walks a list of AST children, separating value entries from
// layout-only nodes. Layout preceding the first value becomes .before on that
// value; trailing layout after the last value becomes .after on the last item.
// If the list contains only layout nodes, it produces a single sepItem whose
// .before holds the layout and whose main is empty.
func buildSepItems(nodes []S, makeVal func(S) sepItem) []sepItem {
	var items []sepItem
	var pendingLayout *Doc

	for _, n := range nodes {
		if ld := layoutDocForNode(n); ld != nil {
			pendingLayout = concatDocs(pendingLayout, ld)
			continue
		}
		it := makeVal(n)
		if pendingLayout != nil {
			it.before = concatDocs(it.before, pendingLayout)
			pendingLayout = nil
		}
		items = append(items, it)
	}

	if pendingLayout != nil {
		if len(items) == 0 {
			items = append(items, sepItem{before: pendingLayout, main: Concat()})
		} else {
			last := &items[len(items)-1]
			last.after = concatDocs(last.after, pendingLayout)
		}
	}
	return items
}

// joinCommaWithPost joins items with commas, printing any item's POST
// *after the comma that follows that item*. The last item's POST (if any)
// prints after the item (no comma). POST forces newline via annotInline.
// Layout in .before/.after is emitted around the item as appropriate.
func joinCommaWithPost(items []sepItem) *Doc {
	if len(items) == 0 {
		return Concat()
	}
	out := make([]*Doc, 0, len(items)*4)
	for i, it := range items {
		if it.before != nil {
			out = append(out, it.before)
		}
		out = append(out, it.main)

		isLast := i == len(items)-1
		if isLast {
			if it.post != "" {
				out = append(out, annotInline(it.post))
			}
			if it.after != nil {
				out = append(out, it.after)
			}
			continue
		}

		// Not last: comma, then either inline-post or normal separator,
		// then any "after" layout that belongs between this and the next item.
		out = append(out, Text(","))
		if it.post != "" {
			out = append(out, annotInline(it.post))
		} else {
			out = append(out, LineDoc())
		}
		if it.after != nil {
			out = append(out, it.after)
		}
	}
	return Concat(out...)
}

/* ---------- AST helpers: tags, shapes, precedence ---------- */

func tag(n S) string   { return n[0].(string) }
func getId(n S) string { return n[1].(string) }
func getStr(n S) string {
	// Used for ("str", s), but safe for ("id", name) too.
	return n[1].(string)
}
func listS(n S, from int) []S {
	if len(n) <= from {
		return nil
	}
	out := make([]S, 0, len(n)-from)
	for i := from; i < len(n); i++ {
		out = append(out, n[i].(S))
	}
	return out
}

// Keys/names are not annotated; unwrap name only.
func unwrapKeyName(n S) string { return n[1].(string) }

var binPrec = map[string]struct {
	p     int
	right bool
}{
	"->": {15, true},
	"*":  {70, false}, "/": {70, false}, "%": {70, false},
	"+": {60, false}, "-": {60, false},
	"<": {50, false}, "<=": {50, false}, ">": {50, false}, ">=": {50, false},
	"==": {40, false}, "!=": {40, false},
	"and": {30, false},
	"or":  {20, false},
}

func exprPrec(n S) int {
	switch tag(n) {
	case "assign":
		return 10
	case "binop":
		if pr, ok := binPrec[n[1].(string)]; ok {
			return pr.p
		}
		return 60
	case "unop":
		if n[1].(string) == "?" {
			return 90
		}
		return 80
	case "call", "idx", "get":
		return 90
	default:
		return 100
	}
}

func parenIf(need int, d *Doc, n S) *Doc {
	if exprPrec(n) < need {
		return Concat(Text("("), d, Text(")"))
	}
	return d
}

func parenIfLE(need int, d *Doc, n S) *Doc {
	if exprPrec(n) <= need {
		return Concat(Text("("), d, Text(")"))
	}
	return d
}

/* ---------- AST → Doc ---------- */

// docSeq renders a sequence of statements, preserving explicit blank-line
// separators represented as "noop" nodes:
//
//	stmt1, noop, stmt2  →  stmt1\n\nstmt2
//
// while keeping the old behavior when no "noop" is present:
//
//	stmt1, stmt2        →  stmt1\nstmt2
//
// Leading and trailing noops in a block are ignored.
func docSeq(kids []S) *Doc {
	var ds []*Doc
	firstStmt := true
	pendingNoop := false

	for _, k := range kids {
		if tag(k) == "noop" {
			// A noop only has effect when it appears between real statements.
			if !firstStmt {
				pendingNoop = true
			}
			continue
		}

		if firstStmt {
			ds = append(ds, docStmt(k))
			firstStmt = false
			pendingNoop = false
			continue
		}

		// Always separate statements with at least one newline, and add an
		// extra blank line when an explicit noop was present between them.
		ds = append(ds, HardLineDoc())
		if pendingNoop {
			ds = append(ds, HardLineDoc())
		}
		ds = append(ds, docStmt(k))
		pendingNoop = false
	}
	return Concat(ds...)
}

func docProgram(n S) *Doc {
	if tag(n) != "block" {
		return docStmt(n)
	}
	return docSeq(listS(n, 1))
}

func docStmt(n S) *Doc {
	switch tag(n) {
	case "noop":
		return Concat()

	case "annot":
		text, wrapped, _ := asAnnotASTRaw(n)

		// Special case: ("annot", text, "noop") represents a floating header
		// annotation with a blank-line run afterwards, e.g.:
		//
		//   # a
		//
		//   x
		//
		// Render it as a PRE-style comment block here; docSeq/docBlock will
		// supply the blank line between this and the next real statement.
		if tag(wrapped) == "noop" {
			return annotPre(text)
		}

		body := docStmt(wrapped)
		main, post := attachInlineOrPre(body, text)
		if post != "" {
			// Inline statement annotation: keep it on the same line as the
			// wrapped statement; line-breaking between statements is handled
			// centrally by docSeq/docBlock.
			return Concat(main, Text(" # "+oneLine(post)))
		}
		// PRE-style or multi-line annotations are already baked into main
		// by attachInlineOrPre (via annotPre).
		return main

	case "fun":
		params, ret, body := n[1].(S), n[2].(S), n[3].(S)
		header := Concat(Text("fun("), docParams(params), Text(")"))
		if !(tag(ret) == "id" && getId(ret) == "Any") {
			header = Concat(header, Text(" -> "), docType(ret))
		}
		return Concat(
			header, Text(" do"), HardLineDoc(),
			Nest(1, docBlock(body)), HardLineDoc(),
			Text("end"),
		)

	case "oracle":
		params, outT, src := n[1].(S), n[2].(S), n[3].(S)
		header := Concat(Text("oracle("), docParams(params), Text(")"))
		if !(tag(outT) == "id" && getId(outT) == "Any") {
			header = Concat(header, Text(" -> "), docType(outT))
		}
		if !(tag(src) == "array" && len(src) == 1) {
			header = Concat(header, Text(" from "), docExpr(src))
		}
		return header

	case "for":
		tgt, iter, body := n[1].(S), n[2].(S), n[3].(S)
		// Target never prints "let" — it's implied in the surface syntax.
		head := Concat(Text("for "), docPattern(tgt), Text(" in "), docExpr(iter), Text(" do"))
		return Concat(head, HardLineDoc(), Nest(1, docBlock(body)), HardLineDoc(), Text("end"))

	case "while":
		cond, body := n[1].(S), n[2].(S)
		head := Concat(Text("while "), docExpr(cond), Text(" do"))
		return Concat(head, HardLineDoc(), Nest(1, docBlock(body)), HardLineDoc(), Text("end"))

	case "if":
		arms := listS(n, 1)
		first := arms[0]
		d := Concat(
			Text("if "), docExpr(first[1].(S)), Text(" then"), HardLineDoc(),
			Nest(1, docBlock(first[2].(S))),
		)
		for i := 1; i < len(arms) && tag(arms[i]) == "pair"; i++ {
			arm := arms[i]
			d = Concat(d, HardLineDoc(),
				Text("elif "), docExpr(arm[1].(S)), Text(" then"), HardLineDoc(),
				Nest(1, docBlock(arm[2].(S))),
			)
		}
		// possible else block
		if last := arms[len(arms)-1]; tag(last) != "pair" {
			d = Concat(d, HardLineDoc(), Text("else"), HardLineDoc(), Nest(1, docBlock(last)))
		}
		return Concat(d, HardLineDoc(), Text("end"))

	case "module":
		nameExpr, body := n[1].(S), n[2].(S)
		return Concat(Text("module "), docExpr(nameExpr), Text(" do"), HardLineDoc(),
			Nest(1, docBlock(body)), HardLineDoc(), Text("end"))

	case "type":
		return Concat(Text("type "), docType(n[1].(S)))

	case "return":
		arg := n[1].(S)
		if tag(arg) == "null" {
			return Text("return")
		}
		return Concat(Text("return "), docExpr(arg))
	case "break":
		arg := n[1].(S)
		if tag(arg) == "null" {
			return Text("break")
		}
		return Concat(Text("break "), docExpr(arg))
	case "continue":
		arg := n[1].(S)
		if tag(arg) == "null" {
			return Text("continue")
		}
		return Concat(Text("continue "), docExpr(arg))

	case "let":
		// Declaration-only: let P
		if len(n) >= 2 {
			return Concat(Text("let "), docPattern(n[1].(S)))
		}
		return Text("let")

	case "assign":
		// Decide PRE vs POST for the whole binding (let-or-assign).
		lhs, rhs := n[1].(S), n[2].(S)
		var head *Doc
		if isDeclPattern(lhs) {
			// Declarative assignment: let P = E
			// Strip the syntactic 'let' wrapper from the pattern.
			var pat S
			if tag(lhs) == "let" && len(lhs) >= 2 {
				pat = lhs[1].(S)
			} else {
				pat = lhs
			}
			head = Concat(Text("let "), docPattern(pat), Text(" = "))
		} else {
			head = Concat(docExprMin(lhs, 10), Text(" = "))
		}
		if txt, inner, ok := asAnnotASTRaw(rhs); ok && strings.TrimSpace(txt) != "" {
			val := docExprMin(inner, 10)
			probe := Concat(head, val)
			main, post := attachInlineOrPre(probe, txt)
			if post != "" {
				// Inline binding-level annotation: same behavior as docStmt("annot")
				return Concat(main, Text(" # "+oneLine(post)))
			}
			// PRE-style: annotPre already baked into main.
			return main
		}
		return Concat(head, docExprMin(rhs, 10))

	case "block":
		return Concat(Text("do"), HardLineDoc(), Nest(1, docBlock(n)), HardLineDoc(), Text("end"))

	default:
		return docExpr(n)
	}
}

func docBlock(n S) *Doc {
	if tag(n) != "block" {
		return docStmt(n)
	}
	return docSeq(listS(n, 1))
}

func docParams(arr S) *Doc {
	if tag(arr) != "array" || len(arr) == 1 {
		return Concat()
	}
	items := listS(arr, 1)
	var parts []*Doc
	for i, pi := range items {
		name := getId(pi[1].(S))
		ty := pi[2].(S)
		if !(tag(ty) == "id" && getId(ty) == "Any") {
			parts = append(parts, Concat(Text(name), Text(": "), docType(ty)))
		} else {
			parts = append(parts, Text(name))
		}
		if i < len(items)-1 {
			parts = append(parts, Text(", "))
		}
	}
	return Concat(parts...)
}

func docExprMin(n S, need int) *Doc {
	return parenIf(need, docExpr(n), n)
}

// (old trailing-post helpers removed; annotation policy is centralized)

func docExpr(n S) *Doc {
	switch tag(n) {
	case "id":
		return Text(getId(n))
	case "int":
		return Text(fmt.Sprint(n[1]))
	case "num":
		s := strconv.FormatFloat(n[1].(float64), 'g', -1, 64)
		if !strings.ContainsAny(s, ".eE") {
			s += ".0"
		}
		return Text(s)
	case "str":
		return Text(quoteString(getStr(n)))
	case "bool":
		if n[1].(bool) {
			return Text("true")
		}
		return Text("false")
	case "null":
		return Text("null")

	case "unop":
		op, rhs := n[1].(string), n[2].(S)
		if op == "?" {
			return Concat(docExprMin(rhs, 90), Text("?"))
		}
		if op == "not" {
			return Concat(Text("not "), docExprMin(rhs, 80))
		}
		return Concat(Text(op), docExprMin(rhs, 80))

	case "binop":
		op, l, r := n[1].(string), n[2].(S), n[3].(S)
		my, right := 60, false
		if pr, ok := binPrec[op]; ok {
			my, right = pr.p, pr.right
		}
		// Associativity-aware parentheses:
		//  - right-assoc:  paren LEFT if prec(left) <= my; RIGHT if prec(right) < my
		//  - left-assoc:   paren LEFT if prec(left) <  my; RIGHT if prec(right) <= my
		lDoc := docExpr(l)
		rDoc := docExpr(r)
		if right {
			lDoc = parenIfLE(my, lDoc, l) // inclusive on left
			rDoc = parenIf(my, rDoc, r)   // exclusive on right
		} else {
			lDoc = parenIf(my, lDoc, l)   // exclusive on left
			rDoc = parenIfLE(my, rDoc, r) // inclusive on right
		}
		return Concat(lDoc, Text(" "+op+" "), rDoc)

	case "assign":
		l, r := n[1].(S), n[2].(S)
		if isDeclPattern(l) {
			// let P = E in expression position
			var pat S
			if tag(l) == "let" && len(l) >= 2 {
				pat = l[1].(S)
			} else {
				pat = l
			}
			return Concat(Text("let "), docPattern(pat), Text(" = "), docExprMin(r, 10))
		}
		return Concat(docExprMin(l, 10), Text(" = "), docExprMin(r, 10))

	case "call":
		recv := n[1].(S)
		args := listS(n, 2)
		var argDocs []*Doc
		for _, a := range args {
			argDocs = append(argDocs, docExpr(a))
		}
		return Concat(docExprMin(recv, 90), Text("("), Join(Text(", "), argDocs), Text(")"))

	case "idx":
		recv, ix := n[1].(S), n[2].(S)
		// Be careful with array indices: this is indexing, not array literal.
		return Concat(docExprMin(recv, 90), Text("["), docExpr(ix), Text("]"))

	case "get":
		recv, name := n[1].(S), n[2].(S)[1].(string)
		if isIdent(name) {
			return Concat(docExprMin(recv, 90), Text("."+name))
		}
		return Concat(docExprMin(recv, 90), Text("."+quoteString(name)))

	case "array":
		elems := listS(n, 1)
		if len(elems) == 0 {
			return Text("[]")
		}
		items := buildSepItems(elems, func(e S) sepItem {
			if txt, inner, ok := asAnnotASTRaw(e); ok {
				main, post := attachInlineOrPre(docExpr(inner), txt)
				return sepItem{main: main, post: post}
			}
			return sepItem{main: docExpr(e)}
		})
		inside := joinCommaWithPost(items)
		lastEnds := items[len(items)-1].post != ""
		return Group(braced("[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))

	case "map":
		items := listS(n, 1)
		if len(items) == 0 {
			return Text("{}")
		}
		joined := buildSepItems(items, func(pr S) sepItem {
			key := unwrapKeyName(pr[1].(S))
			val := pr[2].(S)
			if txt, inner, ok := asAnnotASTRaw(val); ok {
				return entryWithAnn(idOrQuoted(key), docExpr(inner), txt)
			}
			return sepItem{main: kvEntry(idOrQuoted(key), docExpr(val))}
		})
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("{", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "}"))

	case "enum":
		elems := listS(n, 1)
		if len(elems) == 0 {
			return Text("Enum[]")
		}
		items := buildSepItems(elems, func(e S) sepItem {
			if txt, inner, ok := asAnnotASTRaw(e); ok {
				main, post := attachInlineOrPre(docExpr(inner), txt)
				return sepItem{main: main, post: post}
			}
			return sepItem{main: docExpr(e)}
		})
		inside := joinCommaWithPost(items)
		lastEnds := items[len(items)-1].post != ""
		return Group(braced("Enum[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))

	case "let":
		// let P used as an expression
		if len(n) >= 2 {
			return Concat(Text("let "), docPattern(n[1].(S)))
		}
		return Text("let")

	case "return", "break", "continue", "fun", "oracle", "for", "while", "if", "type", "block", "annot", "module":
		return docStmt(n)

	case "opaque":
		return Text(getStr(n))

	default:
		return Text("<" + tag(n) + ">")
	}
}

/* ---------- patterns ---------- */

func isDeclPattern(n S) bool {
	switch tag(n) {
	case "let":
		return true
	case "annot":
		return isDeclPattern(n[2].(S))
	default:
		return false
	}
}

func docPattern(n S) *Doc {
	switch tag(n) {
	case "let":
		// Pattern wrapper: strip 'let' and print the inner shape.
		if len(n) >= 2 {
			return docPattern(n[1].(S))
		}
		return Text("<pattern>")

	case "id":
		return Text(getId(n))

	case "array":
		items := listS(n, 1)
		if len(items) == 0 {
			return Text("[]")
		}
		joined := buildSepItems(items, func(it S) sepItem {
			if txt, inner, ok := asAnnotASTRaw(it); ok {
				main, post := attachInlineOrPre(docPattern(inner), txt)
				return sepItem{main: main, post: post}
			}
			return sepItem{main: docPattern(it)}
		})
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))

	case "map":
		items := listS(n, 1)
		if len(items) == 0 {
			return Text("{}")
		}
		joined := buildSepItems(items, func(it S) sepItem {
			key := unwrapKeyName(it[1].(S))
			val := it[2].(S)
			if txt, inner, ok := asAnnotASTRaw(val); ok {
				return entryWithAnn(idOrQuoted(key), docPattern(inner), txt)
			}
			return sepItem{main: kvEntry(idOrQuoted(key), docPattern(val))}
		})
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("{", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "}"))

	case "annot":
		text, wrapped, _ := asAnnotASTRaw(n)
		// Pattern wrapper: render as PRE. Entry/element sites decide inline.
		return Concat(annotPre(text), docPattern(wrapped))

	default:
		return docExpr(n)
	}
}

/* ---------- AST "annot" helpers ---------- */

// Neutral unwrap: ("annot", ("str", txt), wrapped) → (txt, wrapped, true)
func asAnnotASTRaw(n S) (text string, wrapped S, ok bool) {
	if tag(n) == "annot" {
		return n[1].(S)[1].(string), n[2].(S), true
	}
	return "", n, false
}

/* ---------- Type pretty-printer (as Doc) ---------- */

func docType(t S) *Doc {
	if len(t) == 0 {
		return Text("<type>")
	}
	switch tag(t) {
	case "id":
		return Text(getStr(t))

	case "get":
		recv := t[1].(S)
		prop := t[2].(S)[1].(string)
		// Reuse docType for the receiver so nested gets print as a.b.c
		// If the receiver were ever non-type-ish, docType will fall back gracefully.
		return Concat(docType(recv), Text("."), idOrQuoted(prop))

	case "unop":
		if t[1].(string) == "?" {
			return Concat(docType(t[2].(S)), Text("?"))
		}
		return Text("<unop>")

	case "array":
		items := listS(t, 1)
		if len(items) == 0 {
			return Text("[]")
		}
		joined := buildSepItems(items, func(it S) sepItem {
			if txt, inner, ok := asAnnotASTRaw(it); ok {
				main, post := attachInlineOrPre(docType(inner), txt)
				return sepItem{main: main, post: post}
			}
			return sepItem{main: docType(it)}
		})
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))

	case "enum":
		elems := listS(t, 1)
		if len(elems) == 0 {
			return Text("Enum[]")
		}
		joined := buildSepItems(elems, func(e S) sepItem {
			if txt, inner, ok := asAnnotASTRaw(e); ok {
				main, post := attachInlineOrPre(docTypeLiteral(inner), txt)
				return sepItem{main: main, post: post}
			}
			return sepItem{main: docTypeLiteral(e)}
		})
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("Enum[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))

	case "map":
		items := listS(t, 1)
		if len(items) == 0 {
			return Text("{}")
		}
		joined := buildSepItems(items, func(raw S) sepItem {
			switch tag(raw) {
			case "pair", "pair!":
				key := unwrapKeyName(raw[1].(S))
				val := raw[2].(S)
				keyDoc := idOrQuoted(key)
				if tag(raw) == "pair!" {
					keyDoc = Concat(keyDoc, Text("!"))
				}
				if txt, inner, ok := asAnnotASTRaw(val); ok {
					return entryWithAnn(keyDoc, docType(inner), txt)
				}
				return sepItem{main: kvEntry(keyDoc, docType(val))}
			default:
				// Fallback: render unknown nodes in place.
				return sepItem{main: docType(raw)}
			}
		})
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("{", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "}"))

	case "binop":
		if t[1].(string) == "->" && len(t) >= 4 {
			left := t[2].(S)
			right := t[3].(S)
			// Right-associative printing:
			// - If LEFT is itself an arrow, parenthesize it.
			// - Always render RIGHT via docType (which will continue the chain).
			leftDoc := docType(left)
			if tag(left) == "binop" && left[1].(string) == "->" {
				leftDoc = Concat(Text("("), leftDoc, Text(")"))
			}
			return Concat(leftDoc, Text(" -> "), docType(right))
		}
		return Text("<binop>")

	case "annot":
		txt, wrapped, _ := asAnnotASTRaw(t)
		// Outside binding sites, render as PRE.
		return Concat(annotPre(txt), docType(wrapped))

	default:
		return Text("<type>")
	}
}

func docTypeLiteral(lit S) *Doc {
	switch tag(lit) {
	case "null":
		return Text("null")
	case "bool":
		if lit[1].(bool) {
			return Text("true")
		}
		return Text("false")
	case "int":
		return Text(fmt.Sprint(lit[1]))
	case "num":
		return Text(strconv.FormatFloat(lit[1].(float64), 'g', -1, 64))
	case "str":
		return Text(quoteString(getStr(lit)))
	case "array":
		items := listS(lit, 1)
		if len(items) == 0 {
			return Text("[]")
		}
		joined := buildSepItems(items, func(s S) sepItem {
			return sepItem{main: docTypeLiteral(s)}
		})
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))
	case "map":
		items := listS(lit, 1)
		if len(items) == 0 {
			return Text("{}")
		}
		joined := buildSepItems(items, func(pr S) sepItem {
			k := pr[1].(S)[1].(string)
			val := docTypeLiteral(pr[2].(S))
			entry := kvEntry(idOrQuoted(k), val)
			return sepItem{main: entry}
		})
		inside := joinCommaWithPost(joined)
		return Group(braced("{", Nest(1, Concat(SoftLineDoc(), inside, SoftLineDoc())), "}"))
	default:
		return Text("<lit>")
	}
}

/* ---------- Central annotation policy (single place) ---------- */

// Decide inline vs PRE for a candidate: if `candidate + " # ann"` fits flat
// under a conservative width budget, return (candidate, ann) so callers can
// trail as inline (or after-comma). Otherwise return (PRE+candidate, "").
func attachInlineOrPre(candidate *Doc, ann string) (*Doc, string) {
	ann = strings.TrimSpace(ann)
	if ann == "" {
		return candidate, ""
	}
	// Multi-line annotations always become PRE headers.
	if strings.Contains(ann, "\n") {
		return Concat(annotPre(ann), candidate), ""
	}
	// Conservative budget: avoid over-inlining.
	r := renderer{maxWidth: MaxInlineWidth - 8, tabWidth: 4}
	if r.fitsFlat(Concat(candidate, Text(" # "+oneLine(ann))), r.maxWidth) {
		return candidate, ann
	}
	return Concat(annotPre(ann), candidate), ""
}

// Entry helper applying the policy to `key: value`.
// If inline, the POST returns via .post (rendered after the following comma).
// If PRE, the header is part of .main (no trailing POST).
func entryWithAnn(keyDoc, valDoc *Doc, ann string) sepItem {
	probe := kvEntry(keyDoc, valDoc)
	main, post := attachInlineOrPre(probe, ann)
	if post != "" {
		return sepItem{main: probe, post: post}
	}
	return sepItem{main: main}
}

/* ---------- Value → AST adapter (single source of truth) ---------- */

// ValueToAST converts a runtime Value into the printer/parser AST (S), preserving:
//   - annotations (as ("annot", ("str", ...), node))
//   - insertion order of maps (MapObject.Keys)
//   - cycle guards (arrays, maps) with python-style markers via ("opaque", "[...]") / ("opaque", "{...}")
//
// Non-source forms (functions, types, modules, handles, unknown) render as ("opaque", "<...>").
func ValueToAST(v Value) S {
	seenA := make(map[*ArrayObject]bool)
	seenM := make(map[*MapObject]bool)
	n := valueToASTRec(v, seenA, seenM)
	if s := strings.TrimSpace(v.Annot); s != "" {
		return S{"annot", S{"str", s}, n}
	}
	return n
}

func valueToASTRec(v Value, seenA map[*ArrayObject]bool, seenM map[*MapObject]bool) S {
	switch v.Tag {
	case VTNull:
		return S{"null"}
	case VTBool:
		return S{"bool", v.Data.(bool)}
	case VTInt:
		return S{"int", v.Data.(int64)}
	case VTNum:
		return S{"num", v.Data.(float64)}
	case VTStr:
		return S{"str", v.Data.(string)}

	case VTArray:
		ao := v.Data.(*ArrayObject)
		if ao == nil {
			return S{"array"} // treat nil as empty
		}
		if seenA[ao] {
			return S{"opaque", "[...]"}
		}
		seenA[ao] = true
		out := S{"array"}
		for _, ev := range ao.Elems {
			node := valueToASTRec(ev, seenA, seenM)
			if ann := strings.TrimSpace(ev.Annot); ann != "" {
				node = S{"annot", S{"str", ann}, node}
			}
			out = append(out, node)
		}
		return out

	case VTMap:
		mo := v.Data.(*MapObject)
		if mo == nil {
			return S{"map"}
		}
		if seenM[mo] {
			return S{"opaque", "{...}"}
		}
		seenM[mo] = true
		out := S{"map"}
		for _, k := range mo.Keys {
			val := mo.Entries[k]
			node := valueToASTRec(val, seenA, seenM)
			// Annotations live on the VALUE.
			if ann := strings.TrimSpace(val.Annot); ann != "" {
				node = S{"annot", S{"str", ann}, node}
			}
			out = append(out, S{"pair", S{"str", k}, node})
		}
		return out

	case VTFun, VTType, VTModule, VTHandle:
		return S{"opaque", valueOpaqueString(v)}
	default:
		return S{"opaque", valueOpaqueString(v)}
	}
}

func valueOpaqueString(v Value) string {
	switch v.Tag {
	case VTFun:
		if f, ok := v.Data.(*Fun); ok && f != nil {
			label := "fun"
			if f.IsOracle {
				label = "oracle"
			}
			var parts []string
			if len(f.ParamTypes) == 0 {
				parts = append(parts, "_:Null")
			} else {
				for i := range f.ParamTypes {
					name := "_"
					if i < len(f.Params) && f.Params[i] != "" {
						name = f.Params[i]
					}
					pt := FormatType(f.ParamTypes[i])
					// Parenthesize arrow types in param position for readability.
					if len(f.ParamTypes[i]) >= 4 && f.ParamTypes[i][0] == "binop" && f.ParamTypes[i][1] == "->" {
						pt = "(" + pt + ")"
					}
					if i > 0 {
						parts = append(parts, "-> "+name+":"+pt)
					} else {
						parts = append(parts, name+":"+pt)
					}
				}
			}
			ret := FormatType(f.ReturnType)
			if len(parts) > 0 {
				return "<" + label + ": " + strings.Join(parts, " ") + " -> " + ret + ">"
			}
			return "<" + label + ": " + ret + ">"
		}
		return "<fun>"
	case VTType:
		return "<type: " + FormatType(typeAst(v.Data)) + ">"
	case VTModule:
		name := "<module>"
		if m, ok := v.Data.(*Module); ok && m != nil && m.Name != "" {
			disp := m.Name
			if disp == "" {
				disp = m.Name
			}
			name = "<module: " + disp + ">"
		}
		return name
	case VTHandle:
		if h, ok := v.Data.(*Handle); ok && h != nil && h.Kind != "" {
			return "<handle: " + h.Kind + ">"
		}
		return "<handle>"
	case VTNull:
		return "null"
	case VTBool:
		if b, _ := v.Data.(bool); b {
			return "true"
		}
		return "false"
	case VTInt:
		return strconv.FormatInt(v.Data.(int64), 10)
	case VTNum:
		s := strconv.FormatFloat(v.Data.(float64), 'g', -1, 64)
		if !strings.ContainsAny(s, ".eE") {
			s += ".0"
		}
		return s
	case VTStr:
		// Reuse the same quoting policy for visibility; the opaque string is expected raw.
		return quoteString(v.Data.(string))
	default:
		return "<unknown>"
	}
}

=== END FILE: ./mindscript/printer.go ===

