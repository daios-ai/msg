=== BEGIN FILE: ./mindscript/lexer.go ===
// lexer.go: provides a whitespace-sensitive, UTF-8–aware lexer for the
// MindScript language. It converts a source string into a linear stream of
// tokens with accurate source positions and rich literal decoding.
//
// ──────────────────────────────────────────────────────────────────────────────
// HIGH-LEVEL OVERVIEW
//
// The lexer scans left→right and emits Token values, always ending with EOF.
// Each token carries:
//   - Type     — a TokenType enum
//   - Lexeme   — the exact source slice (verbatim characters from the input)
//   - Literal  — the decoded value for literal tokens (e.g., bool/int/float/string)
//   - Line/Col — 1-based line and 0-based column of the token’s *start*
//   - StartByte/EndByte — byte offsets [start, end) for the token’s source span
//
// Whitespace-sensitive delimiters:
//
//	The lexer decides between LROUND/CLROUND (and LSQUARE/CLSQUARE) solely by
//	whether there is immediate whitespace before the delimiter:
//
//	  '('  → LROUND  if there IS preceding whitespace
//	         CLROUND if there is NO preceding whitespace
//	  '['  → LSQUARE if there IS preceding whitespace
//	         CLSQUARE if there is NO preceding whitespace
//
// Consequences (user-facing syntax):
//   - Calls and parameter lists require NO space before '(':
//     f(x)           // call: uses CLROUND
//     fun(x: T)      // function params: uses CLROUND
//     oracle(x: T)   // oracle params: uses CLROUND
//     With a space ("fun (x: T)"), '(' becomes LROUND and is NOT treated as a
//     parameter list; the parser will error.
//   - Indexing requires NO space before '[':
//     arr[i]         // indexing: uses CLSQUARE
//     With a space ("arr [i]"), '[' is LSQUARE and is NOT treated as indexing.
//   - Grouping "(expr)" is produced regardless of LROUND/CLROUND, but only
//     CLROUND participates in call/juxtaposition chains.
//
// The '.' character is context-sensitive:
//   - If it begins a number (e.g., “.5” or “1.” or “1.2e3”), a NUMBER/INTEGER is
//     produced.
//   - Otherwise it is PERIOD (typically for property access).
//
// IDENTIFIERS & KEYWORDS
//
//	Identifiers match [A-Za-z_][A-Za-z0-9_]* and normally produce ID.
//	Reserved words produce dedicated TokenTypes (e.g., IF, LET, FUNCTION, etc.).
//	After a PERIOD, both identifiers *and* quoted strings are treated as
//	property names and forced to ID (even if the text is a keyword). This allows:
//	    obj."then"   // ID with Literal="then"
//	    obj.then     // ID with Literal="then"
//
// LITERALS
//   - STRING — single or double quotes, JSON-style escapes, including \uXXXX with
//     optional UTF-16 surrogate pair handling. Source must be valid UTF-8; non-ASCII
//     bytes in the lexeme are validated and decoded.
//   - INTEGER — 64-bit signed (ParseInt base 10), when no dot/exp part.
//   - NUMBER  — 64-bit float (ParseFloat), for forms with '.' and/or exponent.
//   - BOOLEAN — “true” or “false” (Literal: bool).
//   - NULL    — “null” (Literal: nil).
//
// ANNOTATIONS
//   - Hash-line annotations: one or more consecutive lines where, after optional
//     indentation, the first non-space is '#'. The leading '#' (and at most one
//     optional following space) are stripped; lines are joined with '\n', and a
//     single ANNOTATION token is emitted. A blank/non-# line ends the block.
//
// ERRORS (start-of-token anchoring)
//   - Lexical errors (e.g., bad escape, invalid UTF-8, unexpected character) are
//     reported as *Error* with precise location anchored to the **start** of the
//     offending token (the token that is being scanned).
//   - Interactive/REPL mode: if enabled via NewLexerInteractive, unterminated
//     strings produce *IncompleteError* (still anchored to the token start).
//
// OUTPUT
//   - Scan returns the full token slice *including* the terminal EOF token.
//   - Each token’s Lexeme is the exact source text (e.g., a STRING’s lexeme
//     includes the quotes and escapes), while Literal carries the decoded value.
//
// ──────────────────────────────────────────────────────────────────────────────
//
// FILE ORGANIZATION
//  1. PUBLIC API  — exported enums/types/constructors/methods & their docs.
//  2. PRIVATE     — all non-exported helpers, internal tables, and scanning.
//
// The PUBLIC API docs below are intentionally exhaustive so the behavior is
// understandable without reading the implementation.
//
// ──────────────────────────────────────────────────────────────────────────────
package mindscript

import (
	"errors"
	"fmt"
	"strconv"
	"strings"
	"unicode/utf16"
	"unicode/utf8"
)

////////////////////////////////////////////////////////////////////////////////
//                               PUBLIC API
////////////////////////////////////////////////////////////////////////////////

// TokenType is the enumeration of all token kinds the lexer can emit.
// Most names are self-explanatory; groups are listed for clarity.
//
// Special:
//
//	EOF     — end-of-file sentinel (always the final token)
//	ILLEGAL — produced only for unrecoverable internal conditions (not used by Scan)
//
// Punctuation (some are whitespace-sensitive, see '(' and '[' notes below):
//
//	LROUND, CLROUND   — '(' with/without preceding whitespace respectively
//	RROUND            — ')'
//	LSQUARE, CLSQUARE — '[' with/without preceding whitespace respectively
//	RSQUARE           — ']'
//	LCURLY, RCURLY    — '{', '}'
//	COLON, COMMA, PERIOD, QUESTION — ':', ',', '.', '?'
//
// Operators:
//
//	PLUS, MINUS, MULT, DIV, MOD — '+', '-', '*', '/', '%'
//	ASSIGN                      — '='
//	EQ, NEQ                     — '==', '!='
//	LESS, LESS_EQ               — '<',  '<='
//	GREATER, GREATER_EQ         — '>',  '>='
//	BANG                        — '!' (used by the language in object/type literals)
//	ARROW                       — '->'
//
// Literals & identifiers:
//
//	ID, STRING, INTEGER, NUMBER, BOOLEAN, NULL
//
// Keywords (produced when the identifier text equals these words, except when
// forced to an ID after PERIOD/property access):
//
//	AND, OR, NOT,
//	LET, DO, END, RETURN, BREAK, CONTINUE,
//	IF, THEN, ELIF, ELSE,
//	FUNCTION, ORACLE, MODULE,
//	FOR, IN, FROM, WHILE,
//	TYPECONS, TYPE, ENUM
//
// Annotation:
//
//	ANNOTATION — emitted for multi-line blocks starting with '#'.
type TokenType int

const (
	// Special
	EOF TokenType = iota
	ILLEGAL

	// Punctuation
	LROUND   // "(" when preceded by whitespace
	CLROUND  // "(" when not preceded by whitespace (juxtaposition/call form)
	RROUND   // ")"
	LSQUARE  // "["
	CLSQUARE // "[" when not preceded by whitespace (index-close)
	RSQUARE  // "]"
	LCURLY   // "{"
	RCURLY   // "}"
	COLON    // ":"
	COMMA    // ","
	PERIOD   // "."
	QUESTION // "?"

	// Operators
	PLUS
	MINUS
	MULT
	DIV
	MOD
	POW    // "**"
	ASSIGN // "="
	EQ     // "=="
	NEQ    // "!="
	LESS
	LESS_EQ
	GREATER
	GREATER_EQ
	LSHIFT // "<<"
	RSHIFT // ">>"
	BITAND // "&"
	BITXOR // "^"
	BITOR  // "|"
	BITNOT // "~" (unary)
	BANG   // "!" (required-field marker in object/type literals)
	ARROW  // "->"

	// Literals & identifiers
	ID
	STRING
	INTEGER
	NUMBER
	BOOLEAN
	NULL

	// Keywords
	AND
	OR
	NOT
	LET
	DO
	END
	RETURN
	BREAK
	CONTINUE
	IF
	THEN
	ELIF
	ELSE
	FUNCTION
	ORACLE
	MODULE
	FOR
	IN
	FROM
	WHILE
	TYPECONS
	TYPE
	ENUM

	// Annotation token (from lines starting with '#')
	ANNOTATION
	NOOP
)

// Token is a single lexical unit produced by the lexer.
//
// Fields:
//
//	Type    — the TokenType kind.
//	Lexeme  — the exact source slice comprising the token (verbatim, including
//	          quotes for strings, escape sequences, etc.).
//	Literal — a decoded value for literal tokens:
//	          • STRING  → Go string with escapes and surrogate pairs resolved
//	          • INTEGER → int64
//	          • NUMBER  → float64
//	          • BOOLEAN → bool
//	          • NULL    → nil
//	          Non-literal tokens usually carry nil or an unmodified string
//	          (keywords may store their text; property IDs store the property name).
//	Line    — 1-based line number at which this token starts.
//	Col     — 0-based column index at which this token starts.
//	StartByte / EndByte — byte offsets [start, end) for the token’s slice.
type Token struct {
	Type      TokenType
	Lexeme    string
	Literal   interface{}
	Line      int
	Col       int
	StartByte int
	EndByte   int
}

// HARD errors produced by the lexer use the unified *Error type defined in
// errors.go. See DiagLex and DiagIncomplete.
//
// In interactive mode, the lexer returns *Error with Kind=DiagIncomplete
// when input ends inside an unterminated construct (e.g., string literal).
//
// **Error locations:** All lexing errors (both hard and incomplete) are anchored
// to the **start of the offending token** (the token whose scanning raised
// the error). This makes diagnostics stable and easy to map to spans.

// Lexer is a streaming tokenizer for MindScript.
//
// Construction:
//   - NewLexer(src)            — normal mode. Unterminated constructs produce LexError.
//   - NewLexerInteractive(src) — REPL-friendly mode. Unterminated constructs
//     produce IncompleteError.
//
// Semantics:
//   - Scan() returns the full token slice including EOF. It never panics for
//     malformed input; instead it returns (nil, error).
//   - Whitespace is skipped, but influences '(' and '[' classification (see TokenType docs).
//   - PERIOD vs number: a '.' followed by digits begins a number IFF there is
//     either preceding whitespace *or* the previous token cannot be a left operand.
//     Otherwise '.' is PERIOD used for property access.
//   - After PERIOD, the *next* identifier or quoted string is forced to ID,
//     even if it matches a keyword.
//
// Positioning:
//   - Line numbers are 1-based; column indices are 0-based.
//   - A token’s position is captured at the start of scanning that token.
type Lexer struct {
	// public type with no exported fields; use constructors + Scan()
	src    string
	start  int // start index of current token
	cur    int // current index
	line   int // 1-based
	col    int // 0-based column within line
	tokens []Token

	// precise token start position
	tokStartLine int
	tokStartCol  int

	// interactive mode: produce IncompleteError for unterminated constructs at EOF
	interactive bool
}

// NewLexer creates a new lexer for the given source in normal mode.
func NewLexer(src string) *Lexer {
	return &Lexer{
		src: src,
		// very rough guess to reduce reslices on big files
		tokens: make([]Token, 0, len(src)/4),
		line:   1,
		col:    0,
	}
}

// NewLexerInteractive creates a lexer in interactive mode.
// Unterminated strings return IncompleteError at EOF, allowing REPLs to request more input.
func NewLexerInteractive(src string) *Lexer {
	return &Lexer{
		src:         src,
		tokens:      make([]Token, 0, len(src)/4),
		line:        1,
		col:         0,
		interactive: true,
	}
}

// Scan tokenizes the entire source string and returns the resulting slice of
// tokens. The returned slice always ends with EOF. On error, it returns (nil, err).
//
// Error behavior summary:
//   - Normal mode: returns *LexError on malformed input or unterminated constructs.
//   - Interactive mode: returns *IncompleteError at EOF if a construct is
//     unterminated; other issues still return *LexError.
//
// Note: Token.Lexeme is the exact source span; Token.Literal contains decoded
// values for STRING/INTEGER/NUMBER/BOOLEAN/NULL as described in Token docs.
func (l *Lexer) Scan() ([]Token, error) {
	for {
		tok, err := l.scanToken()
		if err != nil {
			return nil, err
		}
		if tok.Type == EOF {
			return l.tokens, nil
		}
	}
}

//// END_OF_PUBLIC

////////////////////////////////////////////////////////////////////////////////
//                            PRIVATE IMPLEMENTATION
////////////////////////////////////////////////////////////////////////////////

// ---------------- keywords map (private) ----------------

var keywords = map[string]TokenType{
	"null":     NULL,
	"false":    BOOLEAN,
	"true":     BOOLEAN,
	"and":      AND,
	"or":       OR,
	"not":      NOT,
	"let":      LET,
	"do":       DO,
	"end":      END,
	"return":   RETURN,
	"break":    BREAK,
	"continue": CONTINUE,
	"if":       IF,
	"then":     THEN,
	"elif":     ELIF,
	"else":     ELSE,
	"fun":      FUNCTION,
	"oracle":   ORACLE,
	"module":   MODULE,
	"for":      FOR,
	"in":       IN,
	"from":     FROM,
	"while":    WHILE,
	"type":     TYPECONS,
	"Type":     TYPE,
	"Null":     TYPE,
	"Str":      TYPE,
	"Int":      TYPE,
	"Num":      TYPE,
	"Bool":     TYPE,
	"Any":      TYPE,
	"Handle":   TYPE,
	"Enum":     ENUM,
}

// ---------------- core scanning helpers ----------------

func (l *Lexer) isAtEnd() bool { return l.cur >= len(l.src) }

func (l *Lexer) peek() (byte, bool) {
	if l.isAtEnd() {
		return 0, false
	}
	return l.src[l.cur], true
}

func (l *Lexer) peekN(n int) (byte, bool) {
	idx := l.cur + n
	if idx >= len(l.src) {
		return 0, false
	}
	return l.src[idx], true
}

func (l *Lexer) advance() (byte, bool) {
	if l.isAtEnd() {
		return 0, false
	}
	ch := l.src[l.cur]
	l.cur++
	if ch == '\n' {
		l.line++
		l.col = 0
	} else {
		l.col++
	}
	return ch, true
}

func (l *Lexer) rewindToStart() {
	// We rewind only within the bounds of the current token start; line/col are kept
	// for error arrows (OK since we set tokStartLine/Col before scanning).
	l.cur = l.start
}

func (l *Lexer) addToken(tt TokenType, lit interface{}) Token {
	lex := l.src[l.start:l.cur]
	tok := Token{
		Type:      tt,
		Lexeme:    lex,
		Literal:   lit,
		Line:      l.tokStartLine,
		Col:       l.tokStartCol,
		StartByte: l.start,
		EndByte:   l.cur,
	}
	l.tokens = append(l.tokens, tok)
	l.start = l.cur
	return tok
}

func (l *Lexer) previousToken() *Token {
	if len(l.tokens) == 0 {
		return nil
	}
	return &l.tokens[len(l.tokens)-1]
}

func (l *Lexer) skipHorizontalWhitespace() {
	for !l.isAtEnd() {
		ch, _ := l.peek()
		switch ch {
		case ' ', '\t', '\r':
			l.advance()
			l.start = l.cur
		default:
			return
		}
	}
}

// immediateWhitespaceBefore reports whether the byte immediately preceding the
// current token start is ASCII whitespace. Called *after* skipWhitespace.
func (l *Lexer) immediateWhitespaceBefore() bool {
	if l.start == 0 {
		return false
	}
	switch l.src[l.start-1] {
	case ' ', '\t', '\n', '\r', ';':
		return true
	default:
		return false
	}
}

// ---------------- small predicates ----------------

func canBeLeftOperand(t TokenType) bool {
	switch t {
	case ID, STRING, INTEGER, NUMBER, BOOLEAN, NULL,
		TYPE, ENUM,
		RROUND, RSQUARE, RCURLY,
		QUESTION:
		return true
	default:
		return false
	}
}

func isDigit(b byte) bool { return b >= '0' && b <= '9' }
func isHex(b byte) bool {
	return (b >= '0' && b <= '9') || (b >= 'a' && b <= 'f') || (b >= 'A' && b <= 'F')
}
func isBin(b byte) bool { return b == '0' || b == '1' }
func isOct(b byte) bool { return b >= '0' && b <= '7' }
func isAlpha(b byte) bool {
	return (b >= 'a' && b <= 'z') || (b >= 'A' && b <= 'Z') || b == '_'
}
func isAlphaNum(b byte) bool {
	return isAlpha(b) || (b >= '0' && b <= '9')
}

// readDigitsUS reads a run of digits (per pred) allowing single underscores
// between digits. It returns (hadDigits, endedOnUnderscore, err).
// Rules enforced:
//   - first char must be a digit (no leading '_')
//   - underscores must be single and between digits (no "__", no trailing '_')
func (l *Lexer) readDigitsUS(pred func(byte) bool) (bool, bool, error) {
	had := false
	lastUnd := false
	for {
		b, ok := l.peek()
		if !ok {
			break
		}
		if b == '_' {
			// underscore must follow a digit and be followed by a digit
			if !had || lastUnd {
				return had, true, l.err("misplaced '_' in number")
			}
			// lookahead must be a digit
			if b2, ok2 := l.peekN(1); !ok2 || !pred(b2) {
				return had, true, l.err("misplaced '_' in number")
			}
			lastUnd = true
			l.advance()
			continue
		}
		if !pred(b) {
			break
		}
		l.advance()
		had = true
		lastUnd = false
	}
	return had, lastUnd, nil
}

func stripUnderscores(s string) string {
	// small, allocation-friendly strip
	out := make([]byte, 0, len(s))
	for i := 0; i < len(s); i++ {
		if s[i] != '_' {
			out = append(out, s[i])
		}
	}
	return string(out)
}

func (l *Lexer) afterDotIsProperty() bool {
	p := l.previousToken()
	return p != nil && p.Type == PERIOD
}

// ---------------- error builders (start-of-token anchored) ----------------

func (l *Lexer) err(msg string) error {
	// All lexer diagnostics are anchored to the start of the *current* token,
	// which is the most stable and helpful position for tools and users.
	return &Error{Kind: DiagLex, Msg: msg, Src: nil, Line: l.tokStartLine, Col: l.tokStartCol + 1}
}

func (l *Lexer) errIncomplete(msg string) error {
	// Report need-more-input conditions in interactive mode, anchored to token start.
	return &Error{Kind: DiagIncomplete, Msg: msg, Src: nil, Line: l.tokStartLine, Col: l.tokStartCol + 1}
}

// ---------------- scanners ----------------

// scanString parses a JSON-style string literal (single or double quotes).
func (l *Lexer) scanString() (string, error) {
	del := l.src[l.start]
	if del != '"' && del != '\'' {
		return "", l.err("internal: scanString without quote")
	}
	// consume the delimiter
	l.advance()

	var out []rune
	for !l.isAtEnd() {
		ch, _ := l.advance()
		if ch == byte(del) {
			return string(out), nil
		}
		if ch == '\\' {
			if l.isAtEnd() {
				if l.interactive {
					return "", l.errIncomplete("unfinished escape sequence")
				}
				return "", l.err("unfinished escape sequence")
			}
			esc, _ := l.advance()
			switch esc {
			case '"':
				out = append(out, '"')
			case '\'':
				out = append(out, '\'')
			case '\\':
				out = append(out, '\\')
			case '/':
				out = append(out, '/')
			case 'b':
				out = append(out, '\b')
			case 'f':
				out = append(out, '\f')
			case 'n':
				out = append(out, '\n')
			case 'r':
				out = append(out, '\r')
			case 't':
				out = append(out, '\t')
			case 'u':
				// expect 4 hex digits
				toHex := func(b byte) int {
					switch {
					case b >= '0' && b <= '9':
						return int(b - '0')
					case b >= 'a' && b <= 'f':
						return 10 + int(b-'a')
					case b >= 'A' && b <= 'F':
						return 10 + int(b-'A')
					default:
						return -1
					}
				}
				val := 0
				for i := 0; i < 4; i++ {
					b, ok := l.peek()
					if !ok {
						if l.interactive {
							return "", l.errIncomplete("unicode escape was not terminated (expect 4 hex digits)")
						}
						return "", l.err("unicode escape was not terminated (expect 4 hex digits)")
					}
					d := toHex(b)
					if d < 0 {
						return "", l.err("invalid unicode escape")
					}
					val = (val << 4) | d
					l.advance()
				}
				r := rune(val)

				// handle surrogate pair \uD800-\uDBFF followed by \uDC00-\uDFFF
				if 0xD800 <= r && r <= 0xDBFF {
					saveCur := l.cur
					saveLine, saveCol := l.line, l.col
					if b1, ok := l.peek(); ok && b1 == '\\' {
						l.advance()
						if b2, ok := l.peek(); ok && b2 == 'u' {
							l.advance()
							val2 := 0
							for i := 0; i < 4; i++ {
								b, ok := l.peek()
								if !ok {
									if l.interactive {
										return "", l.errIncomplete("unicode surrogate pair low was not terminated")
									}
									return "", l.err("unicode surrogate pair low was not terminated")
								}
								d := toHex(b)
								if d < 0 {
									return "", l.err("invalid unicode surrogate pair low")
								}
								val2 = (val2 << 4) | d
								l.advance()
							}
							r2 := rune(val2)
							if 0xDC00 <= r2 && r2 <= 0xDFFF {
								cp := utf16.DecodeRune(r, r2)
								out = append(out, cp)
								continue
							}
						}
					}
					// not a valid pair; rewind and just emit r
					l.cur = saveCur
					l.line, l.col = saveLine, saveCol
				}
				out = append(out, r)
			default:
				return "", l.err(fmt.Sprintf("invalid escape sequence: \\%c", esc))
			}
			continue
		}
		// normal char; ensure it’s valid UTF-8 boundary (source may be UTF-8)
		if ch < utf8.RuneSelf {
			out = append(out, rune(ch))
			continue
		}
		// If we see a non-ASCII byte here, the source itself is UTF-8; back up one byte and decode rune.
		l.cur-- // step back 1 byte to let utf8.DecodeRuneInString read from correct start
		r, size := utf8.DecodeRuneInString(l.src[l.cur:])
		if r == utf8.RuneError && size == 1 {
			return "", l.err("invalid UTF-8 in source")
		}
		out = append(out, r)
		l.cur += size
		l.col += size - 1
	}
	if l.interactive {
		return "", l.errIncomplete("string was not terminated")
	}
	return "", l.err("string was not terminated")
}

// scanIdentifier parses [A-Za-z_][A-Za-z0-9_]*
func (l *Lexer) scanIdentifier() string {
	for {
		b, ok := l.peek()
		if !ok || !isAlphaNum(b) {
			break
		}
		l.advance()
	}
	return l.src[l.start:l.cur]
}

// scanNumber parses integer or float; supports .5, 1., 1.23e-4, etc.
// Precondition: current position is at l.start and the first rune is either a digit
// or a '.' that has already been verified to start a number.
func (l *Lexer) scanNumber() (tok TokenType, lit interface{}, err error) {
	sawDigits := false
	sawDot := false
	sawExp := false

	startCur := l.cur

	// --- Base-prefixed integers: 0b..., 0o..., 0x...
	if b0, ok := l.peek(); ok && b0 == '0' {
		// Tentatively consume '0' and inspect next char
		l.advance()
		if pfx, ok2 := l.peek(); ok2 && (pfx == 'b' || pfx == 'B' || pfx == 'o' || pfx == 'O' || pfx == 'x' || pfx == 'X') {
			l.advance()
			var pred func(byte) bool
			var base int
			switch pfx {
			case 'b', 'B':
				pred, base = isBin, 2
			case 'o', 'O':
				pred, base = isOct, 8
			default:
				pred, base = isHex, 16
			}
			had, endedUnd, usErr := l.readDigitsUS(pred)
			if usErr != nil {
				return ILLEGAL, nil, usErr
			}
			if !had || endedUnd {
				return ILLEGAL, nil, l.err("invalid integer literal")
			}
			lex := l.src[l.start:l.cur]
			valStr := stripUnderscores(lex[2:]) // strip digits after prefix
			v, convErr := strconv.ParseInt(valStr, base, 64)
			if convErr != nil {
				return ILLEGAL, nil, l.err("invalid integer literal")
			}
			return INTEGER, v, nil
		}
		// Not a base prefix → decimal starting with 0.
		// If next is another digit or underscore, it's a leading-zeros error
		// unless we immediately form a float/exponent (e.g., "0.5", "0e10").
		if nb, ok3 := l.peek(); ok3 && (nb == '_' || isDigit(nb)) {
			return ILLEGAL, nil, l.err("leading zeros not allowed in decimal integer")
		}
		// Else keep sawDigits=true and let fraction/exp logic run (for "0.5", "0e10"),
		// or finalize as integer zero if neither appears.
		sawDigits = true
	}

	// integral part (optional for leading '.'); allow underscores
	if !sawDigits {
		had, endedUnd, usErr := l.readDigitsUS(isDigit)
		if usErr != nil {
			return ILLEGAL, nil, usErr
		}
		sawDigits = had
		if endedUnd {
			return ILLEGAL, nil, l.err("misplaced '_' in number")
		}
	}

	// fractional part
	if b, ok := l.peek(); ok && b == '.' {
		// Do NOT consume '.' if after property or doubled '..'
		if !(l.afterDotIsProperty()) {
			if b2, ok2 := l.peekN(1); !(ok2 && b2 == '.') {
				// disallow underscore immediately before '.'
				if l.cur > startCur && l.src[l.cur-1] == '_' {
					return ILLEGAL, nil, l.err("misplaced '_' in number")
				}
				l.advance() // '.'
				sawDot = true
				// fraction digits with underscores; must start with a digit (no leading '_')
				had, endedUnd, usErr := l.readDigitsUS(isDigit)
				if usErr != nil {
					return ILLEGAL, nil, usErr
				}
				// It's valid to have no fractional digits only if we will have an exponent later.
				if endedUnd {
					return ILLEGAL, nil, l.err("misplaced '_' in number")
				}
				if had {
					sawDigits = true
				}
			}
		}
	}

	// exponent part: ([eE][+-]?digits with underscores)?
	if b, ok := l.peek(); ok && (b == 'e' || b == 'E') {
		save := l.cur
		l.advance() // e/E
		if b2, ok := l.peek(); ok && (b2 == '+' || b2 == '-') {
			l.advance()
		}
		// disallow '_' immediately after 'e' or sign
		if b3, ok := l.peek(); ok && b3 == '_' {
			return ILLEGAL, nil, l.err("invalid float literal")
		}
		// first exponent char must be a digit
		if b3, ok := l.peek(); ok && isDigit(b3) {
			had, endedUnd, usErr := l.readDigitsUS(isDigit)
			if usErr != nil {
				return ILLEGAL, nil, usErr
			}
			if !had || endedUnd {
				return ILLEGAL, nil, l.err("invalid float literal")
			}
			sawExp = true
		} else {
			// no exponent digits → backtrack
			l.cur = save
		}
	}

	if !sawDigits {
		return ILLEGAL, nil, l.err("malformed number")
	}

	lex := l.src[l.start:l.cur]

	// If it's a pure decimal integer (no dot/exp), enforce no leading zeros (except "0").
	if !sawDot && !sawExp {
		decStr := stripUnderscores(lex)
		if len(decStr) > 1 && decStr[0] == '0' {
			return ILLEGAL, nil, l.err("leading zeros not allowed in decimal integer")
		}
		v, convErr := strconv.ParseInt(decStr, 10, 64)
		if convErr != nil {
			return ILLEGAL, nil, l.err("invalid integer literal")
		}
		return INTEGER, v, nil
	}

	// Float: strip underscores before ParseFloat
	vf, convErr := strconv.ParseFloat(stripUnderscores(lex), 64)
	if convErr != nil {
		return ILLEGAL, nil, l.err("invalid float literal")
	}
	return NUMBER, vf, nil
}

// scanAnnotation captures consecutive lines that start with '#' (ignoring leading spaces).
// Terminates on blank line or a line that does not begin (after spaces) with '#'.
func (l *Lexer) scanAnnotation() (string, error) {
	// current '#' is line-start iff only hspace since previous '\n'
	lineStart := true
	for i := l.start - 1; i >= 0 && l.src[i] != '\n'; i-- {
		if l.src[i] != ' ' && l.src[i] != '\t' && l.src[i] != '\r' {
			lineStart = false
			break
		}
	}
	var bldr strings.Builder

	// helper: check if the *next* line (after current '\n') starts with '#'
	nextLineStartsWithHash := func() bool {
		// we are currently positioned at the '\n' (unconsumed)
		probe := l.cur + 1
		for probe < len(l.src) {
			c := l.src[probe]
			if c == ' ' || c == '\t' || c == '\r' {
				probe++
				continue
			}
			break
		}
		return probe < len(l.src) && l.src[probe] == '#'
	}

	// Trim at most one ASCII space (NOT tab) after the first '#'.
	if b, ok := l.peek(); ok && b == ' ' {
		l.advance()
	}

	// ----- capture first line, but DO NOT consume its trailing '\n' -----
	for {
		b, ok := l.peek()
		if !ok || b == '\n' {
			// do not l.advance() here; leave '\n' in the stream
			bldr.WriteByte('\n')
			break
		}
		bldr.WriteByte(b)
		l.advance()
	}

	// Only coalesce with following '#'-lines when *this* '#' was line-start.
	// (Inline '# ...' must not merge with the next line's '# ...')
	if b, ok := l.peek(); ok && b == '\n' && lineStart && nextLineStartsWithHash() {
		l.advance() // consume newline to move to start of the next line
	} else {
		// single-line block; leave the '\n' unconsumed
		s := bldr.String()
		if len(s) == 0 {
			return "", errors.New("incomplete annotation")
		}
		for len(s) > 0 && s[len(s)-1] == '\n' {
			s = s[:len(s)-1]
		}
		return s, nil
	}

	// ----- capture subsequent '#'-prefixed lines -----
	consumeHashOnLine := func() (bool, error) {
		for {
			b, ok := l.peek()
			if !ok || b == '\n' {
				break
			}
			if b == ' ' || b == '\t' || b == '\r' {
				l.advance()
				continue
			}
			break
		}
		b, ok := l.peek()
		if !ok || b != '#' {
			return false, nil
		}
		l.advance() // '#'
		// Trim at most one ASCII space (NOT tab) after '#'
		if b2, ok2 := l.peek(); ok2 && b2 == ' ' {
			l.advance()
		}
		return true, nil
	}

	for {
		save := l.cur
		cont, err := consumeHashOnLine()
		if err != nil {
			return "", err
		}
		if !cont {
			l.cur = save
			break
		}

		// read rest of line, but DO NOT consume trailing '\n'
		for {
			b, ok := l.peek()
			if !ok || b == '\n' {
				bldr.WriteByte('\n')
				break
			}
			bldr.WriteByte(b)
			l.advance()
		}

		// If another '#'-line follows, eat this '\n' and continue; else stop (leave '\n').
		if b, ok := l.peek(); ok && b == '\n' && nextLineStartsWithHash() {
			l.advance() // consume newline to move to next line
		} else {
			break // leave final '\n' unconsumed
		}
	}

	s := bldr.String()
	if len(s) == 0 {
		return "", errors.New("incomplete annotation")
	}
	for len(s) > 0 && s[len(s)-1] == '\n' {
		s = s[:len(s)-1]
	}
	return s, nil
}

// --- hash/comment helpers ---

// handleSingleHash processes '#' annotations (inline or multiline).
// Returns (producedAnnotation, text, err).
func (l *Lexer) handleSingleHash() (bool, string, error) {
	annot, err := l.scanAnnotation()
	if err != nil {
		return false, "", l.err("incomplete annotation")
	}
	return true, annot, nil
}

// scanNoopIfPresent emits a NOOP token if the source at the current position
// matches: '\n' ( hws* '\n' )+  where hws ∈ {' ', '\r', '\t'}.
func (l *Lexer) scanNoopIfPresent() (Token, bool) {
	b, ok := l.peek()
	if !ok || b != '\n' {
		return Token{}, false
	}

	// Snapshot so we can roll back on non-match (i.e., if we don't see at least one repetition).
	saveCur, saveLine, saveCol, saveStart := l.cur, l.line, l.col, l.start

	// Token starts here.
	l.tokStartLine = l.line
	l.tokStartCol = l.col
	l.start = l.cur

	// Consume the initial newline.
	l.advance()

	matchedReps := 0
	for {
		// Consume zero or more horizontal whitespace chars.
		for {
			nb, ok := l.peek()
			if !ok || (nb != ' ' && nb != '\t' && nb != '\r') {
				break
			}
			l.advance()
		}
		// Require a newline to complete one (hws* '\n') repetition.
		nb, ok := l.peek()
		if ok && nb == '\n' {
			l.advance()
			matchedReps++
			continue
		}
		break
	}

	if matchedReps >= 1 {
		return l.addToken(NOOP, nil), true
	}

	// Not a full match: restore and report no-op not present.
	l.cur, l.line, l.col, l.start = saveCur, saveLine, saveCol, saveStart
	return Token{}, false
}

// --- misc helpers ---

func (l *Lexer) dotStartsNumber() bool {
	b, ok := l.peek()
	if !ok || !isDigit(b) {
		return false
	}
	prev := l.previousToken()
	if l.immediateWhitespaceBefore() || prev == nil || !canBeLeftOperand(prev.Type) {
		return true
	}
	return false
}

// ---------------- main tokenization ----------------

func (l *Lexer) scanToken() (Token, error) {
	for {
		l.skipHorizontalWhitespace()
		l.tokStartLine = l.line
		l.tokStartCol = l.col
		l.start = l.cur

		if l.isAtEnd() {
			return l.addToken(EOF, nil), nil
		}

		// No-op (blank lines)
		if b, _ := l.peek(); b == '\n' {
			if tok, ok := l.scanNoopIfPresent(); ok {
				return tok, nil
			}
			// Lone newline (or newline followed by non-hws) → skip it as whitespace and loop.
			l.advance()
			l.start = l.cur
			continue
		}

		// Treat semicolons exactly like newlines for layout/same-line logic.
		// They are NOT tokens; we "lower" them to line breaks.
		if b, _ := l.peek(); b == ';' {
			// Consume ';' and advance line/col as if it were '\n'.
			l.advance()     // move past ';'
			l.line++        // start a new line
			l.col = 0       // reset column
			l.start = l.cur // next token starts after the semicolon
			continue
		}

		ch, _ := l.advance()

		// Single-char tokens & punctuation with whitespace-sensitive "(" and "["
		switch ch {
		case '(':
			if l.immediateWhitespaceBefore() {
				return l.addToken(LROUND, "("), nil
			}
			return l.addToken(CLROUND, "("), nil
		case ')':
			return l.addToken(RROUND, ")"), nil
		case '[':
			if l.immediateWhitespaceBefore() {
				return l.addToken(LSQUARE, "["), nil
			}
			return l.addToken(CLSQUARE, "["), nil
		case ']':
			return l.addToken(RSQUARE, "]"), nil
		case '{':
			return l.addToken(LCURLY, "{"), nil
		case '}':
			return l.addToken(RCURLY, "}"), nil
		case '+':
			return l.addToken(PLUS, "+"), nil
		case '*':
			// Support '**' (power) before single '*' (MULT)
			if b, ok := l.peek(); ok && b == '*' {
				l.advance()
				return l.addToken(POW, "**"), nil
			}
			return l.addToken(MULT, "*"), nil
		case '/':
			return l.addToken(DIV, "/"), nil
		case '%':
			return l.addToken(MOD, "%"), nil
		case '&':
			return l.addToken(BITAND, "&"), nil
		case '|':
			return l.addToken(BITOR, "|"), nil
		case '^':
			return l.addToken(BITXOR, "^"), nil
		case '~':
			return l.addToken(BITNOT, "~"), nil
		case ':':
			return l.addToken(COLON, ":"), nil
		case ',':
			return l.addToken(COMMA, ","), nil
		case '?':
			return l.addToken(QUESTION, "?"), nil
		}

		// '.' : either decimal-starting float or PERIOD
		if ch == '.' {
			if l.dotStartsNumber() {
				l.rewindToStart()
				tt, lit, err := l.scanNumber()
				if err != nil {
					return Token{}, err
				}
				return l.addToken(tt, lit), nil
			}
			return l.addToken(PERIOD, "."), nil
		}

		// Two-char operators and fallbacks
		switch ch {
		case '-':
			if b, ok := l.peek(); ok && b == '>' {
				l.advance()
				return l.addToken(ARROW, "->"), nil
			}
			return l.addToken(MINUS, "-"), nil
		case '=':
			if b, ok := l.peek(); ok && b == '=' {
				l.advance()
				return l.addToken(EQ, "=="), nil
			}
			return l.addToken(ASSIGN, "="), nil
		case '!':
			if b, ok := l.peek(); ok && b == '=' {
				l.advance()
				return l.addToken(NEQ, "!="), nil
			}
			return l.addToken(BANG, "!"), nil
		case '<':
			// Prefer shift '<<' over '<=' when present
			if b, ok := l.peek(); ok {
				if b == '<' {
					l.advance()
					return l.addToken(LSHIFT, "<<"), nil
				}
				if b == '=' {
					l.advance()
					return l.addToken(LESS_EQ, "<="), nil
				}
			}
			return l.addToken(LESS, "<"), nil
		case '>':
			// Prefer shift '>>' over '>=' when present
			if b, ok := l.peek(); ok {
				if b == '>' {
					l.advance()
					return l.addToken(RSHIFT, ">>"), nil
				}
				if b == '=' {
					l.advance()
					return l.addToken(GREATER_EQ, ">="), nil
				}
			}
			return l.addToken(GREATER, ">"), nil
		}

		// Annotations
		if ch == '#' {
			ok, text, err := l.handleSingleHash()
			if err != nil {
				return Token{}, err
			}
			if ok {
				return l.addToken(ANNOTATION, text), nil
			}
		}

		// Strings
		if ch == '"' || ch == '\'' {
			l.rewindToStart()
			text, err := l.scanString()
			if err != nil {
				return Token{}, err
			}
			// After '.' a quoted key becomes ID (property name)
			if l.afterDotIsProperty() {
				return l.addToken(ID, text), nil
			}
			return l.addToken(STRING, text), nil
		}

		// Numbers (starting with digit)
		if isDigit(ch) {
			l.rewindToStart()
			tt, lit, err := l.scanNumber()
			if err != nil {
				return Token{}, err
			}
			return l.addToken(tt, lit), nil
		}

		// Identifiers / Keywords
		if isAlpha(ch) {
			l.rewindToStart()
			lex := l.scanIdentifier()
			// After '.', treat as property name (ID)
			if l.afterDotIsProperty() {
				return l.addToken(ID, lex), nil
			}
			if tt, ok := keywords[lex]; ok {
				switch tt {
				case NULL:
					return l.addToken(NULL, nil), nil
				case BOOLEAN:
					if lex == "true" {
						return l.addToken(BOOLEAN, true), nil
					}
					return l.addToken(BOOLEAN, false), nil
				default:
					return l.addToken(tt, nil), nil
				}
			}
			return l.addToken(ID, lex), nil
		}

		return Token{}, l.err(fmt.Sprintf("unexpected character: %q", ch))
	}
}
=== END FILE: ./mindscript/lexer.go ===

=== BEGIN FILE: ./mindscript/parser.go ===
// parser.go — Pratt parser for MindScript that produces compact S-expressions.
//
// OVERVIEW
// --------
// This module implements the newline-aware Pratt parser for the MindScript
// language. It consumes the token stream produced by the *whitespace-sensitive*
// lexer (see lexer.go) and builds a compact, Lisp-style S-expression (AST).
//
// Design goals:
//   - Keep the grammar readable via precedence rules (Pratt parser).
//   - Encode the AST in a tiny, serialisable structure (S-expressions).
//   - Respect whitespace-sensitive signals emitted by the lexer:
//   - '(' can be LROUND or CLROUND; only CLROUND participates in calls.
//   - '[' can be LSQUARE or CLSQUARE; only CLSQUARE participates in indexing.
//   - '.' is PERIOD unless it started a number in the lexer.
//   - multi-line '#' annotations become ANNOTATION tokens.
//   - blank-line runs may be emitted as NOOP tokens.
//   - Support an "interactive" mode that surfaces *Error{Kind:DiagIncomplete}
//     at EOF instead of hard parse errors, suitable for REPLs.
//
// Annotation model (lowest precedence):
//   - Annotations attach to **values** (not sites/names). The parser collects
//     all nearby annotation texts around a binding site and merges them onto
//     the value node as a single wrapper:
//     ("annot", ("str", mergedText), value)
//   - The AST does not preserve whether a note was written “pre” or “post”.
//     The pretty-printer is responsible for rendering (e.g., multiline → pre,
//     single line → post) and must be deterministic for idempotency:
//     pretty(src) == pretty(pretty(src))
//
// Nodes & Spans
// -------------
// The AST is a tree of S-expressions: []any whose first element is a string tag.
// **This list is the most important reference.**
//
//	("block", n1, n2, ...)
//	("noop")
//
// Literals & identifiers:
//
//	("id",   string)              // identifier (includes property names coerced to ID by lexer rules)
//	("int",  int64)               // from INTEGER
//	("num",  float64)             // from NUMBER
//	("str",  string)              // decoded literal
//	("bool", bool)                // from BOOLEAN
//	("null")                      // from NULL
//	("type", expr)                // from 'type' ...
//
// Operators / expressions:
//
//	("unop",  op,  rhs)           // prefix "-" or "not"; postfix "?"  (op is string)
//	("binop", op,  lhs, rhs)      // "+", "-", "*", "/", "%", comparisons, "==", "!=", "and", "or", "->"
//	("let",   patternExpr)        // "let P" declaration (pattern in expr form)
//	("assign", target, value)     // "=" (right-assoc)
//
// Property / call / index:
//
//	("call", callee, arg1, arg2, ...)
//	("get",  obj, ("str", name))             // obj.name or obj."name"
//	("idx",  obj, indexExpr)                 // obj[expr] or obj.(expr) or obj.12
//
// Collections:
//
//	("array", e1, e2, ...)
//	("map",   ("pair",  keyStrExpr, value)*)
//	("map",   ("pair!", keyStrExpr, value)*) // required-field (key! : value)
//	("enum",  item1, item2, ...)             // from Enum[ ... ]
//
// Functions, modules, control, loops:
//
//	("fun",     paramsArray, retTypeExprOrAny, bodyBlock)
//	("oracle",  paramsArray, outTypeExprOrAny, sourceExpr)
//	("module",  nameExpr, bodyBlock)
//	("if", ("pair", cond1, thenBlk1), ..., elseBlk?)
//	("while", cond, bodyBlock)
//	("for",   targetPatternOrLvalue, iterExpr, bodyBlock)
//	("return", value)  // value may be "null" per newline semantics
//	("break",  value)  // value may be "null"
//	("continue", value)// value may be "null"
//
// Annotations:
//
//	("annot", ("str", text), wrappedNode)   // merged text; no POST marker
//
// ─────────────────────────────────────────────────────────────────────────────
// SPAN EMISSION INVARIANT (CRITICAL)
// ----------------------------------
// **This file now centralizes AST construction and span emission.**
//
//   - Every AST node is constructed through IR first (no spans during parse).
//   - After parsing, we materialize the AST and emit spans in strict **post-order**
//     (children first, then parent), left-to-right among siblings.
//   - Wrapper nodes we create (e.g. "annot" from PRE/POST) obey the same rule.
//   - Nodes that are synthesized with no concrete tokens (e.g. default type
//     `Any`) still receive a placeholder `Span{}` (tok=-1).
//   - The root block’s span is appended last.
//
// Dependencies
// ------------
//   - lexer.go
//   - errors.go (*Error, DiagParse, DiagIncomplete, IsIncomplete)
//   - spans.go (Span, SpanIndex, BuildSpanIndexPostOrder)
package mindscript

import (
	"fmt"
	"strings"
)

////////////////////////////////////////////////////////////////////////////////
//                                  PUBLIC API
////////////////////////////////////////////////////////////////////////////////

type S = []any

func L(tag string, parts ...any) S { return append([]any{tag}, parts...) }

// IR (Intermediate Representation) — built during parsing without emitting spans.

// ---------------- IR TYPES ---------------------------------------------------
type NodeID int

type IRNode struct {
	Tag      string
	Kids     []NodeID
	Value    any
	TokStart int // inclusive token index, -1 if synthetic
	TokEnd   int // inclusive token index, -1 if synthetic
}

type IRArena struct {
	Nodes []IRNode
	Root  NodeID
}

// materializePostOrder converts IR into the public AST (S-expr) and a slice of
// spans in strict post-order (children first, then parent).
func materializePostOrder(toks []Token, ir IRArena) (S, []Span) {
	spans := make([]Span, 0, len(ir.Nodes))

	var build func(NodeID) S
	build = func(id NodeID) S {
		n := ir.Nodes[id]

		// Build children first (post-order span emission later).
		childNodes := make([]S, len(n.Kids))
		for i, k := range n.Kids {
			childNodes[i] = build(k)
		}

		// Convert []S -> []any for L(...).
		parts := make([]any, 0, len(childNodes)+1)
		if n.Value != nil {
			parts = append(parts, n.Value)
		}
		for _, c := range childNodes {
			parts = append(parts, c)
		}

		node := L(n.Tag, parts...)

		// Append this node’s span (post-order).
		span := Span{}
		if n.TokStart >= 0 && n.TokEnd >= n.TokStart &&
			n.TokStart < len(toks) && n.TokEnd < len(toks) {
			span.StartByte = toks[n.TokStart].StartByte
			span.EndByte = toks[n.TokEnd].EndByte
		}
		spans = append(spans, span)
		return node
	}

	ast := build(ir.Root)
	return ast, spans
}

// ParseSExpr parses a complete MindScript source string and returns its AST.
func ParseSExpr(src string) (S, error) {
	lex := NewLexer(src)
	toks, err := lex.Scan()
	if err != nil {
		return nil, err
	}
	p := &parser{toks: toks, src: src, lastSpanStartTok: -1, lastSpanEndTok: -1}
	root, err := p.programIR()
	if err != nil {
		return nil, err
	}
	p.ir.Root = root
	ast, _ := materializePostOrder(p.toks, p.ir)
	return ast, nil
}

// ParseSExprWithSpans parses like ParseSExpr and also returns a *SpanIndex,
// with spans recorded in strict post-order per the invariant.
func ParseSExprWithSpans(src string) (S, *SpanIndex, error) {
	lex := NewLexer(src)
	toks, err := lex.Scan()
	if err != nil {
		return nil, nil, err
	}
	p := &parser{toks: toks, src: src, lastSpanStartTok: -1, lastSpanEndTok: -1}
	root, perr := p.programIR()
	if perr != nil {
		return nil, nil, perr
	}
	p.ir.Root = root
	ast, spans := materializePostOrder(p.toks, p.ir)
	idx := BuildSpanIndexPostOrder(ast, spans)
	return ast, idx, nil
}

// ParseSExprInteractiveWithSpans parses in REPL-friendly mode and returns the AST plus
// a SpanIndex with post-order node spans. Unterminated constructs at EOF
// produce *Error{Kind:DiagIncomplete}.
func ParseSExprInteractiveWithSpans(src string) (S, *SpanIndex, error) {
	lex := NewLexerInteractive(src)
	toks, err := lex.Scan()
	if err != nil {
		return nil, nil, err
	}
	p := &parser{
		toks: toks, src: src,
		interactive:      true,
		lastSpanStartTok: -1, lastSpanEndTok: -1,
	}
	root, perr := p.programIR()
	if perr != nil {
		return nil, nil, perr
	}
	p.ir.Root = root
	ast, spans := materializePostOrder(p.toks, p.ir)
	idx := BuildSpanIndexPostOrder(ast, spans)
	return ast, idx, nil
}

//// END_OF_PUBLIC

////////////////////////////////////////////////////////////////////////////////
///////////////////////////// PRIVATE IMPLEMENTATION ///////////////////////////
////////////////////////////////////////////////////////////////////////////////

type parser struct {
	toks        []Token
	i           int
	interactive bool
	// (no span emission during parse)
	lastSpanStartTok int
	lastSpanEndTok   int
	src              string
	// IR arena (nodes + eventual root)
	ir IRArena
}

// New small shared helpers (each used ≥3 times):
// - parseSingleBetween: "( expr )" / "[ expr ]" single element with PRE + trailing gaps + need(close).
// - parseDoBlockWithLeadingGap: common "gap before `do` attaches to body".
// - parseExprAfterBP: require expression after an anchor token with PRE attachment, caller chooses BP.
// - onCommaPostAttachToValue: POST-after-comma attaches to VALUE inside a ("pair", key, value).
//
// All helpers reuse existing machinery: takeReqGap, collectGap, need, attachAnnotFrom, bracketed, etc.

// ─────────────────────── centralized annotation carrier ─────────────────────
type annSrc struct {
	txt      string
	startTok int // inclusive token index of the first contributing annotation token
	endTok   int // inclusive token index of the last contributing annotation token
	ok       bool
}

func annNone() annSrc { return annSrc{} }

func annFromSingle(tokIdx int, txt string) annSrc {
	return annSrc{txt: txt, startTok: tokIdx, endTok: tokIdx, ok: txt != ""}
}

func mergeAnn(a, b annSrc) annSrc {
	if !a.ok && !b.ok {
		return annSrc{}
	}
	if !a.ok {
		return b
	}
	if !b.ok {
		return a
	}
	txt := a.txt
	if b.txt != "" {
		if txt == "" {
			txt = b.txt
		} else {
			txt = txt + "\n" + b.txt
		}
	}
	// Compute first and last contributing token indexes, ignoring unknown (-1).
	start := a.startTok
	if start < 0 || (b.startTok >= 0 && b.startTok < start) {
		start = b.startTok
	}
	end := a.endTok
	if end < 0 || (b.endTok >= 0 && b.endTok > end) {
		end = b.endTok
	}
	return annSrc{
		txt: txt, startTok: start, endTok: end, ok: txt != "",
	}
}

// ─────────────────────────── token basics & helpers ─────────────────────────

func (p *parser) atEnd() bool { return p.peek().Type == EOF }
func (p *parser) peek() Token {
	if p.i >= len(p.toks) {
		return p.toks[len(p.toks)-1]
	}
	return p.toks[p.i]
}
func (p *parser) prev() Token { return p.toks[p.i-1] }

func (p *parser) match(tt ...TokenType) bool {
	if p.atEnd() {
		return false
	}
	for _, t := range tt {
		if p.peek().Type == t {
			p.i++
			return true
		}
	}
	return false
}

func (p *parser) need(t TokenType, msg string) (Token, error) {
	if p.match(t) {
		return p.prev(), nil
	}
	g := p.peek()
	if g.Type == EOF {
		line, col := p.posAfterLastSpan()
		kind := DiagParse
		if p.interactive {
			kind = DiagIncomplete
		}
		return Token{}, &Error{Kind: kind, Msg: msg, Line: line, Col: col}
	}
	line, col := p.posAtByte(g.StartByte)
	return Token{}, &Error{Kind: DiagParse, Msg: msg, Line: line, Col: col}
}

func (p *parser) posAtByte(b int) (int, int) {
	if b < 0 {
		g := p.peek()
		return g.Line, g.Col + 1
	}
	if b > len(p.src) {
		b = len(p.src)
	}
	line := 1 + strings.Count(p.src[:b], "\n")
	lastNL := strings.LastIndex(p.src[:b], "\n")
	if lastNL < 0 {
		return line, b + 1
	}
	return line, b - lastNL
}
func (p *parser) posAfterLastSpan() (int, int) {
	if p.lastSpanEndTok >= 0 && p.lastSpanEndTok < len(p.toks) {
		endB := p.toks[p.lastSpanEndTok].EndByte
		return p.posAtByte(endB)
	}
	g := p.peek()
	return g.Line, g.Col + 1
}

// takeReqGap consumes immediately-adjacent GAP (ANNOTATION/NOOP) that appear
// before something *required* after `anchor`. It emits these gaps (returned as
// annSrc) but NEVER lets them satisfy the requirement.
// If only gaps remain to EOF, it returns an error anchored to `anchor`:
//   - interactive: DiagIncomplete(msg)
//   - noninteractive: DiagParse(msg)
func (p *parser) takeReqGap(anchor Token, incMsg string) (annSrc, error) {
	// Coalesce sequential annotations first, then eat blank-line runs.
	pre := p.collectAnnotsSrc()
	p.skipNoops()
	// If after consuming those we only have gaps to EOF, this is an incomplete requirement.
	if p.onlyGapsToEOF() {
		line, col := p.posAtByte(anchor.StartByte)
		kind := DiagParse
		if p.interactive {
			kind = DiagIncomplete
		}
		return annNone(), &Error{
			Kind: kind,
			Msg:  incMsg,
			Line: line, Col: col,
		}
	}
	return pre, nil
}

// needNoGap enforces the "GAPLESS after dot" rule: immediately after `anchor`
// (which should be a PERIOD token) there must not be ANNOTATION/NOOP.
// If a GAP is present:
//   - interactive + only gaps to EOF → DiagIncomplete anchored to `anchor`;
//   - otherwise → hard parse error anchored at the first GAP token.
//
// It does NOT consume any token.
func (p *parser) needNoGap(anchor Token, msg string) error {
	if p.atEnd() {
		// Unify message across modes: same text, different kind.
		line, col := p.posAtByte(anchor.StartByte)
		kind := DiagParse
		if p.interactive {
			kind = DiagIncomplete
		}
		return &Error{Kind: kind, Msg: msg, Line: line, Col: col}
	}
	switch p.peek().Type {
	case ANNOTATION, NOOP:
		if p.interactive && p.onlyGapsToEOF() {
			line, col := p.posAtByte(anchor.StartByte)
			return &Error{Kind: DiagIncomplete, Msg: msg, Line: line, Col: col}
		}
		line, col := p.posAtByte(p.peek().StartByte)
		return &Error{Kind: DiagParse, Msg: msg, Line: line, Col: col}
	}
	return nil
}

func tokText(t Token) string {
	if s, ok := t.Literal.(string); ok {
		return s
	}
	return t.Lexeme
}

func (p *parser) skipNoops() {
	for !p.atEnd() && p.peek().Type == NOOP {
		p.i++
	}
}

// ───────────────────────── precedence / associativity ──────────────────────

func lbp(t TokenType) (int, bool) {
	switch t {
	case POW:
		return 80, true
	case ARROW:
		return 15, true
	case MULT, DIV, MOD:
		return 70, true
	case PLUS, MINUS:
		return 60, true
	case LSHIFT, RSHIFT:
		return 55, true
	case LESS, LESS_EQ, GREATER, GREATER_EQ:
		return 50, true
	case EQ, NEQ:
		return 40, true
	case BITAND:
		return 35, true
	case BITXOR:
		return 34, true
	case BITOR:
		return 33, true
	case AND:
		return 30, true
	case OR:
		return 20, true
	case ASSIGN:
		return 10, true
	}
	return 0, false
}
func isRightAssoc(tt TokenType) bool {
	// Assignment and arrow are right-associative; exponentiation is also commonly right-assoc.
	return tt == ASSIGN || tt == ARROW || tt == POW
}

// ───────────────────────────── IR node builders ─────────────────────────

func (p *parser) mkLeafIR(tag string, tok int, parts ...any) NodeID {
	id := NodeID(len(p.ir.Nodes))
	var val any
	if len(parts) > 0 {
		val = parts[0]
	}
	p.ir.Nodes = append(p.ir.Nodes, IRNode{
		Tag: tag, Kids: nil, Value: val,
		TokStart: tok, TokEnd: tok,
	})
	// Do not let synthetic tokens clobber anchors used for diagnostics.
	if tok >= 0 {
		p.lastSpanStartTok, p.lastSpanEndTok = tok, tok
	}
	return id
}

func (p *parser) mkIR(tag string, startTok, endTok int, kids ...NodeID) NodeID {
	id := NodeID(len(p.ir.Nodes))
	cp := make([]NodeID, len(kids))
	copy(cp, kids)
	p.ir.Nodes = append(p.ir.Nodes, IRNode{
		Tag: tag, Kids: cp,
		TokStart: startTok, TokEnd: endTok,
	})
	// Preserve last real-token anchor; ignore synthetic (-1).
	if startTok >= 0 && endTok >= 0 {
		p.lastSpanStartTok, p.lastSpanEndTok = startTok, endTok
	}
	return id
}

func (p *parser) mkIRVal(tag string, startTok, endTok int, value any, kids ...NodeID) NodeID {
	id := NodeID(len(p.ir.Nodes))
	cp := make([]NodeID, len(kids))
	copy(cp, kids)
	p.ir.Nodes = append(p.ir.Nodes, IRNode{
		Tag: tag, Kids: cp, Value: value,
		TokStart: startTok, TokEnd: endTok,
	})
	// Preserve last real-token anchor; ignore synthetic (-1).
	if startTok >= 0 && endTok >= 0 {
		p.lastSpanStartTok, p.lastSpanEndTok = startTok, endTok
	}
	return id
}

// ───────────────────────── NOOP / annotation utils ─────────────────────

// onlyGapsToEOF reports whether from the current lookahead to EOF there are
// no real tokens (only NOOP and/or ANNOTATION). This is the canonical
// "need more input" detector for interactive mode.
func (p *parser) onlyGapsToEOF() bool {
	j := p.i
	for j < len(p.toks) {
		switch p.toks[j].Type {
		case NOOP, ANNOTATION:
			j++
			continue
		case EOF:
			return true
		default:
			return false
		}
	}
	return true
}

// ───────────────────────────── shared mini-helpers ───────────────────────────

// opener already consumed. Parse exactly one expr between opener/closer,
// attach PRE (after opener) and trailing gaps (before closer), then need(closer).
// Returns the inner expression and the closer's token index.
func (p *parser) parseSingleBetween(closer TokenType, expect string) (NodeID, int, error) {
	openTok := p.toks[p.i-1]
	needMsg := "expected expression after '('"
	if closer == RSQUARE {
		needMsg = "expected index expression after '['"
	}
	pre, err := p.takeReqGap(openTok, needMsg)
	if err != nil {
		return 0, 0, err
	}
	inner, err := p.expr(0)
	if err != nil {
		return 0, 0, err
	}
	if pre.ok {
		inner = p.attachAnnotFrom(pre, inner)
	}
	if tail := p.collectGap(); tail.ok {
		inner = p.attachAnnotFrom(tail, inner)
	}
	if _, err := p.need(closer, expect); err != nil {
		return 0, 0, err
	}
	return inner, p.i - 1, nil
}

// Common "gap before do" handling for fun/oracle/module/loops.
func (p *parser) parseDoBlockWithLeadingGap() (NodeID, error) {
	pre := p.collectGap()
	b, err := p.parseBlock(true)
	if err != nil {
		return 0, err
	}
	if pre.ok {
		b = p.attachAnnotFrom(pre, b)
	}
	return b, nil
}

// Require an expression after a just-consumed anchor token, with configurable BP,
// attaching PRE to that expression.
func (p *parser) parseExprAfterBP(anchor Token, msg string, bp int) (NodeID, error) {
	pre, err := p.takeReqGap(anchor, msg)
	if err != nil {
		return 0, err
	}
	e, err := p.expr(bp)
	if err != nil {
		return 0, err
	}
	if pre.ok {
		e = p.attachAnnotFrom(pre, e)
	}
	return e, nil
}

// POST-after-comma attaches to VALUE inside ("pair", key, value)
func (p *parser) onCommaPostAttachToValue(last NodeID, comma Token) NodeID {
	n := p.ir.Nodes[last]
	switch n.Tag {
	case "pair", "pair!":
		// Re-attach POST to the value child, not the pair wrapper.
		if len(n.Kids) < 2 {
			// Defensive: malformed pair; fall back to prior behavior.
			return p.attachSameLineAnnots(last, comma.Line)
		}
		key := n.Kids[0]
		val := n.Kids[1]
		val = p.attachSameLineAnnots(val, comma.Line)
		// Rebuild the pair node with the updated value; preserve original span.
		return p.mkIR(n.Tag, n.TokStart, n.TokEnd, key, val)
	default:
		// Arrays or other lists: attach to the element itself.
		return p.attachSameLineAnnots(last, comma.Line)
	}
}

func (p *parser) nextTokenIsOnSameLine(as Token) bool {
	if p.atEnd() {
		return false
	}
	return p.peek().Line == as.Line
}

// Collect only *immediately adjacent* ANNOTATION tokens into a single source-carrying value.
// Do NOT skip NOOPs here; a blank-line run must break annotation groups.
func (p *parser) collectAnnotsSrc() annSrc {
	var out annSrc
	for !p.atEnd() && p.peek().Type == ANNOTATION {
		tok := p.peek()
		p.i++
		if s, ok := tok.Literal.(string); ok && s != "" {
			out = mergeAnn(out, annFromSingle(p.i-1, s))
		}
	}
	return out
}

// collectGap coalesces a "gap" at a join: it first consumes immediately-adjacent
// ANNOTATION tokens into a single annSrc and then skips any NOOPs (blank-line runs).
// Use this *before* requiring a keyword/delimiter (then/do/in/from/else/elif, closers, etc.).
// If callers then call need(...), EOF after this consumption will naturally become
// DiagIncomplete in interactive mode via need(...).
func (p *parser) collectGap() annSrc {
	a := p.collectAnnotsSrc()
	p.skipNoops()
	return a
}

// Attach one or more *single-line* ANNOTATION tokens that start on refLine.
func (p *parser) attachSameLineAnnots(n NodeID, refLine int) NodeID {
	if p.atEnd() || p.peek().Type != ANNOTATION {
		return n
	}
	var acc annSrc
	for !p.atEnd() && p.peek().Type == ANNOTATION && p.peek().Line == refLine {
		tok := p.peek()
		if s, ok := tok.Literal.(string); ok && s != "" && !strings.Contains(s, "\n") {
			acc = mergeAnn(acc, annFromSingle(p.i, s))
			p.i++ // consume
			continue
		}
		break
	}
	if !acc.ok {
		return n
	}
	return p.attachAnnotFrom(acc, n)
}

// IR-safe annotation attach: build ("annot", ("str", txt), base) in IR.
//   - child "str" spans = [src.startTok, src.endTok] (fallback to base span).
//   - Wrapper inherits base span.
func (p *parser) attachAnnotFrom(src annSrc, val NodeID) NodeID {
	if !src.ok || src.txt == "" {
		return val
	}

	bn := p.ir.Nodes[val]
	startTok := src.startTok
	endTok := src.endTok
	if startTok < 0 {
		startTok = bn.TokStart
	}
	if endTok < 0 {
		endTok = bn.TokEnd
	}
	str := p.mkLeafIR("str", startTok, src.txt)
	p.ir.Nodes[str].TokEnd = endTok

	return p.mkIR("annot", bn.TokStart, bn.TokEnd, str, val)
}

// ───────────────────────── program / blocks ────────────────────────────

func (p *parser) programIR() (NodeID, error) {
	var kids []NodeID
	for !p.atEnd() {
		e, err := p.expr(0)
		if err != nil {
			return 0, err
		}
		kids = append(kids, e)
	}
	if len(p.toks) <= 1 {
		return p.mkIR("block", -1, -1 /*empty*/), nil
	}
	return p.mkIR("block", 0, len(p.toks)-2, kids...), nil
}

// blockUntil parses statements until a stop token is seen.
func (p *parser) blockUntil(stops ...TokenType) (NodeID, error) {
	stop := map[TokenType]bool{}
	for _, s := range stops {
		stop[s] = true
	}
	var items []NodeID
	startTok := p.i
	consumedAny := false

	for !p.atEnd() && !stop[p.peek().Type] {
		e, err := p.expr(0)
		if err != nil {
			return 0, err
		}
		items = append(items, e)
		consumedAny = true
	}
	if consumedAny {
		return p.mkIR("block", startTok, p.i-1, items...), nil
	}
	return p.mkIR("block", -1, -1 /*empty*/), nil
}

func (p *parser) parseBlock(requireDo bool) (NodeID, error) {
	if requireDo {
		if _, err := p.need(DO, "expected 'do'"); err != nil {
			return 0, err
		}
	}
	b, err := p.blockUntil(END)
	if err != nil {
		return 0, err
	}
	if _, err := p.need(END, "expected 'end'"); err != nil {
		return 0, err
	}
	return b, nil
}

// ───────────────────────────── tiny node helpers ───────────────────────────

func (p *parser) tryLiteralOrId(t Token, start int) (NodeID, bool) {
	switch t.Type {
	case ID, TYPE:
		return p.mkLeafIR("id", start, tokText(t)), true
	case INTEGER:
		return p.mkLeafIR("int", start, t.Literal), true
	case NUMBER:
		return p.mkLeafIR("num", start, t.Literal), true
	case STRING:
		return p.mkLeafIR("str", start, t.Literal), true
	case BOOLEAN:
		return p.mkLeafIR("bool", start, t.Literal), true
	case NULL:
		return p.mkLeafIR("null", start), true
	}
	return 0, false
}

// ---- tiny shared helpers for delimited lists ----

// drainTrailingGapsUntil consumes any sequence of NOOPs and/or ANNOTATIONs
// until the given closing token is seen. Each NOOP becomes ("noop") and each
// annotation becomes ("annot", ("str", mergedText), ("noop")).
//
// It never crosses non-gap tokens. If a non-gap appears before the closer,
// it delegates to need(close, expectMsg) to produce the right diagnostic.
func (p *parser) drainTrailingGapsUntil(close TokenType, expectMsg string, out *[]NodeID) error {
	for {
		if p.atEnd() {
			// Let need(...) decide whether this is parse vs incomplete, with correct anchoring.
			if _, err := p.need(close, expectMsg); err != nil {
				return err
			}
			return nil
		}
		switch p.peek().Type {
		case close:
			return nil

		case NOOP:
			*out = append(*out, p.mkLeafIR("noop", p.i))
			p.i++

		case ANNOTATION:
			// Merge adjacent annotations and wrap a noop, preserving the annotation's source span.
			src := p.collectAnnotsSrc()
			*out = append(*out, p.attachAnnotFrom(src, p.mkLeafIR("noop", -1)))

		default:
			// Unexpected token before closer → canonical error.
			if _, err := p.need(close, expectMsg); err != nil {
				return err
			}
			return nil
		}
	}
}

// ───────────────────────────── prefix / postfix / infix ────────────────────

func (p *parser) expr(minBP int) (NodeID, error) {
	tokIndexOfThis := p.i
	t := p.peek()
	p.i++

	var left NodeID
	leftStartTok := tokIndexOfThis

	// ---- prefix ----
	if n, ok := p.tryLiteralOrId(t, tokIndexOfThis); ok {
		left = n
	} else {
		switch t.Type {
		case NOOP:
			left = p.mkLeafIR("noop", tokIndexOfThis)

		case ENUM:
			// Accept gap (annotations/newlines) between 'Enum' and '[' safely.
			saveI := p.i
			enumPre := p.collectGap()
			if p.peek().Type == LSQUARE || p.peek().Type == CLSQUARE {
				p.i++ // consume '[' or CLSQUARE
				arr, err := p.arrayLiteralAfterOpen()
				if err != nil {
					return 0, err
				}
				// Retag: ("array", e1, e2, ...) -> ("enum", e1, e2, ...)
				an := p.ir.Nodes[arr]
				n := p.mkIR("enum", tokIndexOfThis, p.i-1, an.Kids...)
				if enumPre.ok {
					n = p.attachAnnotFrom(enumPre, n)
				}
				left = n
			} else {
				// Not an Enum-literal; restore so gap remains for the next construct.
				p.i = saveI
				left = p.mkLeafIR("id", tokIndexOfThis, tokText(t))
			}

		case MINUS, NOT, BITNOT:
			r, err := p.parseExprAfterBP(t, "expected expression after unary operator", 80)
			if err != nil {
				return 0, err
			}
			endTok := p.lastSpanEndTok
			if endTok < 0 {
				endTok = tokIndexOfThis
			}
			left = p.mkIRVal("unop", tokIndexOfThis, endTok, t.Lexeme, r)

		case LROUND, CLROUND:
			inner, _, err := p.parseSingleBetween(RROUND, "expected ')'")
			if err != nil {
				return 0, err
			}
			left = inner
			leftStartTok = tokIndexOfThis

		case LSQUARE, CLSQUARE:
			a, err := p.arrayLiteralAfterOpen()
			if err != nil {
				return 0, err
			}
			left = a
			leftStartTok = tokIndexOfThis

		case LCURLY:
			mp, err := p.mapLiteralAfterOpen(tokIndexOfThis)
			if err != nil {
				return 0, err
			}
			left = mp
			leftStartTok = tokIndexOfThis

		case FUNCTION:
			fn, err := p.funExpr(tokIndexOfThis)
			if err != nil {
				return 0, err
			}
			left = fn
			leftStartTok = tokIndexOfThis

		case ORACLE:
			orc, err := p.oracleExpr(tokIndexOfThis)
			if err != nil {
				return 0, err
			}
			left = orc
			leftStartTok = tokIndexOfThis

		case MODULE:
			name, err := p.parseExprAfterBP(t, "expected module name expression", 0)
			if err != nil {
				return 0, err
			}
			body, err := p.parseDoBlockWithLeadingGap()
			if err != nil {
				return 0, err
			}
			left = p.mkIR("module", tokIndexOfThis, p.i-1, name, body)
			leftStartTok = tokIndexOfThis

		case RETURN, BREAK, CONTINUE:
			n, err := p.parseControl(t, tokIndexOfThis)
			if err != nil {
				return 0, err
			}
			left = n
			leftStartTok = tokIndexOfThis

		case IF:
			ifnode, err := p.ifExpr()
			if err != nil {
				return 0, err
			}
			in := p.ir.Nodes[ifnode]
			left = p.mkIR("if", tokIndexOfThis, p.i-1, in.Kids...)
			leftStartTok = tokIndexOfThis

		case DO:
			body, err := p.parseBlock(false)
			if err != nil {
				return 0, err
			}
			left = body
			leftStartTok = tokIndexOfThis

		case FOR:
			f, err := p.forExpr(tokIndexOfThis)
			if err != nil {
				return 0, err
			}
			left = f
			leftStartTok = tokIndexOfThis

		case WHILE:
			w, err := p.whileExpr(tokIndexOfThis)
			if err != nil {
				return 0, err
			}
			left = w
			leftStartTok = tokIndexOfThis

		case LET:
			// let P        → ("let", P)
			// let P = E    → ("assign", ("let", P), E) via infix ASSIGN
			pre, err := p.takeReqGap(t, "expected pattern after 'let'")
			if err != nil {
				return 0, err
			}
			const assignBP = 10
			pat, err := p.expr(assignBP + 1)
			if err != nil {
				return 0, err
			}
			if pre.ok {
				pat = p.attachAnnotFrom(pre, pat)
			}
			endTok := p.lastSpanEndTok
			if endTok < 0 {
				endTok = tokIndexOfThis
			}
			left = p.mkIR("let", tokIndexOfThis, endTok, pat)
			leftStartTok = tokIndexOfThis

		case TYPECONS:
			x, err := p.parseExprAfterBP(t, "expected type expression after 'type'", 1)
			if err != nil {
				return 0, err
			}
			left = p.mkIR("type", tokIndexOfThis, p.lastSpanEndTok, x)
			leftStartTok = tokIndexOfThis

		case ANNOTATION:
			// Build a source-carrying annotation payload
			src := annSrc{ok: true, txt: "", startTok: tokIndexOfThis, endTok: tokIndexOfThis}
			if s, ok := t.Literal.(string); ok {
				src.txt = s
			}

			// Floating annotation: if the very next token is a NOOP, consume exactly one
			// NOOP and return annot(noop) immediately.
			if p.match(NOOP) {
				return p.attachAnnotFrom(src, p.mkLeafIR("noop", p.i-1)), nil
			}

			// Standalone comment: after skipping NOOPs, if the next token cannot start a value,
			// synthesize annot(noop) without consuming any real NOOPs.
			{
				j := p.i
				for j < len(p.toks) && p.toks[j].Type == NOOP {
					j++
				}
				next := EOF
				if j < len(p.toks) {
					next = p.toks[j].Type
				}
				switch next {
				case EOF:
					// Interactive + only gaps to EOF ⇒ ask for more input instead of annot(noop).
					if p.interactive && p.onlyGapsToEOF() {
						line, col := p.posAtByte(t.StartByte)
						return 0, &Error{Kind: DiagIncomplete, Msg: "expected expression after annotation", Line: line, Col: col}
					}
					return p.attachAnnotFrom(src, p.mkLeafIR("noop", -1)), nil
				case END, ELSE, ELIF, THEN, RROUND, RSQUARE, RCURLY:
					return p.attachAnnotFrom(src, p.mkLeafIR("noop", -1)), nil
				}
			}

			// Interactive: only gaps after the annotation → incomplete
			if p.interactive && p.onlyGapsToEOF() {
				line, col := p.posAtByte(t.StartByte)
				return 0, &Error{Kind: DiagIncomplete, Msg: "expected expression after annotation", Line: line, Col: col}
			}
			// Non-interactive EOF → treat as annot(noop)
			if p.atEnd() && !p.interactive {
				left = p.attachAnnotFrom(src, p.mkLeafIR("noop", -1))
				leftStartTok = tokIndexOfThis
				break
			}

			// Normal case: parse the annotated operand
			operand, err := p.expr(0)
			if err != nil {
				return 0, err
			}
			left = p.attachAnnotFrom(src, operand)
			return left, nil

		default:
			if t.Type == EOF && p.interactive {
				line, col := p.posAfterLastSpan()
				return 0, &Error{Kind: DiagIncomplete, Msg: "unexpected end of input", Line: line, Col: col}
			}
			line, col := p.posAtByte(t.StartByte)
			return 0, &Error{Kind: DiagParse, Msg: fmt.Sprintf("unexpected token '%s'", t.Lexeme), Line: line, Col: col}
		}
	}

	// ---- postfix chain ----
	for {
		n, ok, err := p.parseOnePostfix(left, leftStartTok)
		if err != nil {
			return 0, err
		}
		if !ok {
			break
		}
		left = n
	}

	// ---- infix ops ----
	for {
		op := p.peek()
		bp, ok := lbp(op.Type)
		if !ok || bp < minBP {
			break
		}
		p.i++

		nextBP := bp + 1
		if isRightAssoc(op.Type) {
			nextBP = bp
		}

		if op.Type == ASSIGN && !p.assignableIR(left) {
			line, col := p.posAtByte(op.StartByte)
			return 0, &Error{Kind: DiagParse, Msg: "invalid assignment target", Line: line, Col: col}
		}

		// Allow GAP between operator and RHS, but don't let it satisfy the need.
		preOp, err := p.takeReqGap(op, "expected expression after operator")
		if err != nil {
			return 0, err
		}

		rightParsed, err := p.expr(nextBP)
		if err != nil {
			return 0, err
		}
		endTok := p.lastSpanEndTok

		if op.Type == ASSIGN {
			right := rightParsed
			if preOp.ok {
				right = p.attachAnnotFrom(preOp, right)
			}
			if endTok >= 0 && endTok < len(p.toks) {
				right = p.attachSameLineAnnots(right, p.toks[endTok].Line)
			}
			left = p.mkIR("assign", leftStartTok, endTok, left, right)
		} else {
			left = p.mkIRVal("binop", leftStartTok, endTok, op.Lexeme, left, rightParsed)
		}
	}

	// Attach trailing *same-line* annotations (POST) only at the outermost level.
	// This ensures `… * (8 # note)` doesn't consume `# note` inside the RHS,
	// allowing the outer expression to wrap the whole `(4+5)*8`.
	if minBP == 0 {
		if p.lastSpanEndTok >= 0 && p.lastSpanEndTok < len(p.toks) {
			// Never POST onto a NOOP (blank-line statement).
			if p.toks[p.lastSpanEndTok].Type != NOOP {
				left = p.attachSameLineAnnots(left, p.toks[p.lastSpanEndTok].Line)
			}
		}
	}
	return left, nil
}

// ───────────────────────── unified postfix dispatcher ──────────────────────
//
// Handles: QUESTION (optional), CLROUND (call), CLSQUARE (index), PERIOD (dot).
// **Span order** is enforced on IR materialization.

func (p *parser) parseOnePostfix(left NodeID, leftStartTok int) (NodeID, bool, error) {
	// Enforce GAPLESS *before* '.' : if we are sitting on GAPs and the next
	// non-gap token is PERIOD, that is an error (or incomplete at EOF).
	if !p.atEnd() && (p.peek().Type == ANNOTATION || p.peek().Type == NOOP) {
		j := p.i
		firstGap := p.peek()
		for j < len(p.toks) && (p.toks[j].Type == ANNOTATION || p.toks[j].Type == NOOP) {
			j++
		}
		if j < len(p.toks) && p.toks[j].Type == PERIOD {
			// There *is* a dot after a gap → forbidden.
			line, col := p.posAtByte(firstGap.StartByte)
			return 0, false, &Error{Kind: DiagParse, Msg: "no gap allowed before '.'", Line: line, Col: col}
		}
		// Not a dot next; postfix chain stops here.
		return 0, false, nil
	}

	switch p.peek().Type {
	case QUESTION:
		qtok := p.i
		p.i++
		n := p.mkIRVal("unop", leftStartTok, qtok, "?", left)
		return n, true, nil

	case CLROUND:
		p.i++
		if p.match(RROUND) {
			n := p.mkIR("call", leftStartTok, p.i-1, left)
			return n, true, nil
		}
		// Parse argument list
		args, _, closeTok, err := p.bracketed(
			RROUND, "expected ')'",
			func(pending annSrc) (NodeID, error) {
				a, err := p.expr(0)
				if err != nil {
					return 0, err
				}
				if pending.ok {
					a = p.attachAnnotFrom(pending, a)
				}
				return a, nil
			},
			func(last NodeID, comma Token, _ string) NodeID { return p.onCommaPostAttachToValue(last, comma) },
		)
		if err != nil {
			return 0, false, err
		}
		kids := make([]NodeID, 0, 1+len(args))
		kids = append(kids, left)
		kids = append(kids, args...)
		n := p.mkIR("call", leftStartTok, closeTok, kids...)
		return n, true, nil

	case CLSQUARE:
		p.i++
		idx, closeTok, perr := p.parseSingleBetween(RSQUARE, "expected ']'")
		if perr != nil {
			return 0, false, perr
		}
		n := p.mkIR("idx", leftStartTok, closeTok, left, idx)
		return n, true, nil

	case PERIOD:
		p.i++ // consume '.'
		dotTok := p.toks[p.i-1]
		// GAPLESS after '.': forbid ANNOTATION/NOOP immediately after dot.
		if err := p.needNoGap(dotTok, "expected property name, integer, or '(expr)' immediately after '.' (no gap allowed)"); err != nil {
			return 0, false, err
		}
		if p.match(LROUND) || p.match(CLROUND) {
			ex, closeTok, perr := p.parseSingleBetween(RROUND, "expected ')' after computed property")
			if perr != nil {
				return 0, false, perr
			}
			n := p.mkIR("idx", leftStartTok, closeTok, left, ex)
			return n, true, nil
		}
		// .<int> -> idx
		if p.match(INTEGER) {
			intTok := p.i - 1
			intNode := p.mkLeafIR("int", intTok, p.prev().Literal)
			n := p.mkIR("idx", leftStartTok, intTok, left, intNode)
			return n, true, nil
		}
		// .id or coerced quoted-name (lexer coerces after PERIOD) -> get
		if p.match(ID) {
			propTok := p.i - 1
			prop := p.mkLeafIR("str", propTok, tokText(p.prev()))
			n := p.mkIR("get", leftStartTok, propTok, left, prop)
			return n, true, nil
		}
		// Fallback diagnostic (not a valid follower)
		g := p.peek()
		line, col := p.posAtByte(g.StartByte)
		return 0, false, &Error{Kind: DiagParse, Msg: "expected property name, integer, or '(expr)' after '.'", Line: line, Col: col}
	}
	return 0, false, nil
}

// ───────────────────────── collections / lists / maps ─────────────────────

// bracketed parses a comma-separated list between an already-consumed opener
// and its 'close'. It carries interstitial PRE as annSrc and emits annot(noop) with correct spans.
func (p *parser) bracketed(
	close TokenType,
	expectMsg string,
	parseElem func(pendingPRE annSrc) (NodeID, error),
	onCommaPost func(last NodeID, comma Token, txt string) NodeID,
) ([]NodeID, int, int, error) {
	openTok := p.i - 1

	// Empty only when the very next token is the closer (no gap skipping).
	if p.match(close) {
		return nil, openTok, p.i - 1, nil
	}

	var out []NodeID
	pending := annNone()

	for {
		// Close: emit dangling PRE as annot(noop)
		if p.peek().Type == close {
			if pending.ok {
				out = append(out, p.attachAnnotFrom(pending, p.mkLeafIR("noop", -1)))
				pending = annNone()
			}
			break
		}

		// Interstitial gaps
		switch p.peek().Type {
		case ANNOTATION:
			pending = mergeAnn(pending, p.collectAnnotsSrc())
			continue
		case NOOP:
			if pending.ok {
				out = append(out, p.attachAnnotFrom(pending, p.mkLeafIR("noop", -1)))
				pending = annNone()
				p.i++ // consume the NOOP we wrapped
				continue
			}
			out = append(out, p.mkLeafIR("noop", p.i))
			p.i++
			continue
		}

		// Element with current pending PRE (caller applies it appropriately)
		elem, err := parseElem(pending)
		if err != nil {
			return nil, 0, 0, err
		}
		// Same-line POST handled by IR attachment here:
		if end := p.lastSpanEndTok; end >= 0 && end < len(p.toks) {
			elem = p.attachSameLineAnnots(elem, p.toks[end].Line)
		}
		pending = annNone() // Clear pending PRE

		out = append(out, elem)

		// Optional comma; allow trailing comma
		if !p.match(COMMA) {
			break
		}
		comma := p.prev()
		last := out[len(out)-1]
		if onCommaPost != nil {
			last = onCommaPost(last, comma, "")
		} else {
			last = p.attachSameLineAnnots(last, comma.Line)
		}
		out[len(out)-1] = last
	}

	// Drain trailing gaps before closer (NOOP/ANNOTATION), then require closer.
	if err := p.drainTrailingGapsUntil(close, expectMsg, &out); err != nil {
		return nil, 0, 0, err
	}
	if _, err := p.need(close, expectMsg); err != nil {
		return nil, 0, 0, err
	}
	return out, openTok, p.i - 1, nil
}

func (p *parser) arrayLiteralAfterOpen() (NodeID, error) {
	if p.match(RSQUARE) {
		return p.mkIR("array", p.i-2, p.i-1 /* '[]' */), nil
	}
	items, openTok, closeTok, err := p.bracketed(
		RSQUARE, "expected ']'",
		func(pending annSrc) (NodeID, error) {
			e, err := p.expr(0)
			if err != nil {
				return 0, err
			}
			if pending.ok {
				e = p.attachAnnotFrom(pending, e)
			}
			return e, nil
		},
		nil, // POST-after-comma attaches to element itself
	)
	if err != nil {
		return 0, err
	}
	return p.mkIR("array", openTok, closeTok, items...), nil
}

// params parses (CLROUND ... RROUND) parameter pairs; preserves NOOPs and
// PRE/POST rules uniformly across entries, while *ignoring* NOOPs that appear
// between ':' and the type expression (so annotations there still bind to the type).
// Critically, it NORMALIZES annotations at the binding site by merging pending/A/B/C/D
// onto the VALUE inside each ("pair", name, value), never onto the pair node itself.
func (p *parser) params() (NodeID, error) {
	if _, perr := p.need(CLROUND, "expected '(' to start parameters"); perr != nil {
		return 0, perr
	}
	openTok := p.i - 1

	// Immediate close → empty params array
	if p.match(RROUND) {
		return p.mkIR("array", openTok, p.i-1), nil
	}

	// Guard: preserve the same diagnostic if a non-element appears before ')'.
	switch p.peek().Type {
	case ANNOTATION, NOOP, RROUND, ID, EOF:
	default:
		g := p.peek()
		line, col := p.posAtByte(g.StartByte)
		return 0, &Error{Kind: DiagParse, Msg: "expected ')' after parameters", Line: line, Col: col}
	}

	entries, _, closeTok, err := p.bracketed(
		RROUND, "expected ')' after parameters",
		func(pendingPRE annSrc) (NodeID, error) {
			idTok, err := p.need(ID, "expected parameter name")
			if err != nil {
				return 0, err
			}
			idIdx := p.i - 1
			nameLeaf := p.mkLeafIR("id", idIdx, tokText(idTok))

			var val NodeID
			if p.match(COLON) {
				b, err := p.takeReqGap(p.prev(), "expected type after ':'")
				if err != nil {
					return 0, err
				}
				tExpr, err := p.expr(0)
				if err != nil {
					return 0, err
				}
				ann := mergeAnn(pendingPRE, b)
				if ann.ok {
					tExpr = p.attachAnnotFrom(ann, tExpr)
				}
				val = tExpr
			} else {
				base := p.mkLeafIR("id", -1, "Any")
				val = p.attachAnnotFrom(pendingPRE, base)
			}

			pair := p.mkIR("pair", -1, -1, nameLeaf, val)
			return pair, nil
		},
		func(last NodeID, comma Token, _ string) NodeID { return p.onCommaPostAttachToValue(last, comma) },
	)
	if err != nil {
		return 0, err
	}
	return p.mkIR("array", openTok, closeTok, entries...), nil
}

func (p *parser) mapLiteralAfterOpen(openTok int) (NodeID, error) {
	p.skipNoops()
	if p.match(RCURLY) {
		return p.mkIR("map", openTok, p.i-1), nil
	}

	pairs, _, closeTok, err := p.bracketed(
		RCURLY, "expected '}'",
		func(pendingPRE annSrc) (NodeID, error) {
			elemStartTok := p.i // current token before reading key/annots
			k, aKey, err := p.readKeyString()
			if err != nil {
				return 0, err
			}

			req := p.match(BANG)
			colonTok, err := p.need(COLON, "expected ':' after key")
			if err != nil {
				return 0, err
			}

			// B = GAP after ':'; it cannot satisfy the value requirement.
			b, err := p.takeReqGap(colonTok, "expected value after ':'")
			if err != nil {
				return 0, err
			}
			v, err := p.expr(0)
			if err != nil {
				return 0, err
			}

			val := v
			ann := mergeAnn(mergeAnn(pendingPRE, aKey), b)
			if ann.ok {
				val = p.attachAnnotFrom(ann, val)
			}

			// Pair span must start at the earliest PRE (pending or key PRE) if present.
			pairStartTok := elemStartTok
			if pendingPRE.ok && (pairStartTok < 0 || pendingPRE.startTok < pairStartTok) {
				pairStartTok = pendingPRE.startTok
			}
			if aKey.ok && (pairStartTok < 0 || aKey.startTok < pairStartTok) {
				pairStartTok = aKey.startTok
			}

			tag := "pair"
			if req {
				tag = "pair!"
			}
			return p.mkIR(tag, pairStartTok, p.i-1, k, val), nil
		},
		func(last NodeID, comma Token, _ string) NodeID { return p.onCommaPostAttachToValue(last, comma) },
	)
	if err != nil {
		return 0, err
	}
	return p.mkIR("map", openTok, closeTok, pairs...), nil
}

// ───────────────────────── control / loops / if ───────────────────────────

func (p *parser) parseControl(t Token, startTok int) (NodeID, error) {
	// tag: "return" | "break" | "continue"
	tag := "return"
	switch t.Type {
	case BREAK:
		tag = "break"
	case CONTINUE:
		tag = "continue"
	}

	// Newline after the control word → no value (Null).
	if !p.nextTokenIsOnSameLine(t) {
		return p.mkIR(tag, startTok, startTok, p.mkLeafIR("null", -1)), nil
	}

	p.skipNoops()
	// Same line but next token can't start a value → also Null.
	// NOTE: An inline annotation after a control word (e.g. `return # note`)
	// is treated as a trailing comment on the bare control, not as the start
	// of a value expression on the next line.
	switch p.peek().Type {
	case END, ELSE, ELIF, THEN, RROUND, RSQUARE, RCURLY, ANNOTATION:
		return p.mkIR(tag, startTok, startTok, p.mkLeafIR("null", -1)), nil
	}

	// Parse value and attach any adjacent annotations.
	x, err := p.expr(0)
	if err != nil {
		return 0, err
	}
	// Attach any adjacent annotations to the control's value (source-aware).
	if tail := p.collectAnnotsSrc(); tail.ok {
		x = p.attachAnnotFrom(tail, x)
	}
	return p.mkIR(tag, startTok, p.lastSpanEndTok, x), nil
}

func (p *parser) ifExpr() (NodeID, error) {
	ifTok := p.toks[p.i-1]
	condStartTok := p.i
	cond, err := p.parseExprAfterBP(ifTok, "expected condition after 'if'", 0)
	if err != nil {
		return 0, err
	}
	between := p.collectGap()
	if _, err := p.need(THEN, "expected 'then'"); err != nil {
		return 0, err
	}
	thenBlk, err := p.blockUntil(END, ELIF, ELSE)
	if err != nil {
		return 0, err
	}
	if between.ok {
		thenBlk = p.attachAnnotFrom(between, thenBlk)
	}
	arm := p.mkIR("pair", condStartTok, p.lastSpanEndTok, cond, thenBlk)
	arms := []NodeID{arm}

	// Repeated elif arms with gap-aware detection and attachment.
	for {
		saveI := p.i
		lead := p.collectGap() // annotations just before 'elif' attach to that arm's block
		if !p.match(ELIF) {
			// Not an elif: restore so annotations remain for 'else' or next construct.
			p.i = saveI
			break
		}
		elifTok := p.toks[p.i-1]
		condStartTok = p.i
		c, err := p.parseExprAfterBP(elifTok, "expected condition after 'elif'", 0)
		if err != nil {
			return 0, err
		}
		between := p.collectGap()
		if _, err := p.need(THEN, "expected 'then'"); err != nil {
			return 0, err
		}
		b, err := p.blockUntil(END, ELIF, ELSE)
		if err != nil {
			return 0, err
		}
		if lead.ok {
			b = p.attachAnnotFrom(lead, b)
		}
		if between.ok {
			b = p.attachAnnotFrom(between, b)
		}
		arm := p.mkIR("pair", condStartTok, p.lastSpanEndTok, c, b)
		arms = append(arms, arm)
	}

	var elseTail []NodeID
	// Optional else with gap-aware attachment.
	saveI := p.i
	preElse := p.collectGap()
	if !p.match(ELSE) {
		// No else: restore so annotations are not lost.
		p.i = saveI
	} else {
		b, err := p.blockUntil(END)
		if err != nil {
			return 0, err
		}
		if preElse.ok {
			b = p.attachAnnotFrom(preElse, b)
		}
		elseTail = []NodeID{b}
	}
	if _, err := p.need(END, "expected 'end'"); err != nil {
		return 0, err
	}
	return p.mkIR("if", -1, -1, append(arms, elseTail...)...), nil
}

func (p *parser) forExpr(openTok int) (NodeID, error) {
	// Ensure gaps right after 'for' are treated uniformly (including NOOPs)
	preFor, err := p.takeReqGap(p.toks[openTok], "expected for-target after 'for'")
	if err != nil {
		return 0, err
	}
	tgt, err := p.forTarget()
	if err != nil {
		return 0, err
	}
	if preFor.ok {
		tgt = p.attachAnnotFrom(preFor, tgt)
	}
	between := p.collectGap()
	inTok, err := p.need(IN, "expected 'in'")
	if err != nil {
		return 0, err
	}
	postIn, err := p.takeReqGap(inTok, "expected expression after 'in'")
	if err != nil {
		return 0, err
	}
	iter, err := p.expr(0)
	if err != nil {
		return 0, err
	}
	// Attach both the pre-'in' gap (between target and 'in') and the post-'in' gap.
	iter = p.attachAnnotFrom(mergeAnn(between, postIn), iter)
	body, err := p.parseDoBlockWithLeadingGap()
	if err != nil {
		return 0, err
	}
	return p.mkIR("for", openTok, p.i-1, tgt, iter, body), nil
}

func (p *parser) whileExpr(openTok int) (NodeID, error) {
	cond, err := p.parseExprAfterBP(p.toks[openTok], "expected condition after 'while'", 0)
	if err != nil {
		return 0, err
	}
	body, err := p.parseDoBlockWithLeadingGap()
	if err != nil {
		return 0, err
	}
	return p.mkIR("while", openTok, p.i-1, cond, body), nil
}

// ───────────────────────── functions / oracle ─────────────────────────────

func (p *parser) optionalArrowType(incMsg string) (NodeID, error) {
	// Allow a GAP between params ')' and '->'. If '->' is found AFTER the gap,
	// parse the type and attach the gap's annotations to that type. Otherwise,
	// restore so the gap can belong to the subsequent join (e.g., 'do').
	saveI := p.i
	lead := p.collectGap() // GAP between ')' and '->' (if any)
	if p.match(ARROW) {
		arrowTok := p.prev()
		post, err := p.takeReqGap(arrowTok, incMsg) // GAP immediately after '->'
		if err != nil {
			return 0, err
		}
		r, err := p.expr(0)
		if err != nil {
			return 0, err
		}
		r = p.attachAnnotFrom(mergeAnn(lead, post), r)
		return r, nil // parsed type (one node)
	}
	// No arrow: put the parser back so the gap isn't consumed.
	p.i = saveI
	return p.mkLeafIR("id", -1, "Any"), nil // single synthetic node
}

// Small shared header for fun/oracle: params + optional arrow type
type fnHeader struct{ params, arrow NodeID }

func (p *parser) parseFnHeader(kind string) (fnHeader, error) {
	ps, err := p.params()
	if err != nil {
		return fnHeader{}, err
	}
	msg := "expected return type after '->'"
	if kind == "oracle" {
		msg = "expected output type after '->'"
	}
	ar, err := p.optionalArrowType(msg)
	return fnHeader{ps, ar}, err
}

func (p *parser) funExpr(openTok int) (NodeID, error) {
	h, err := p.parseFnHeader("fun")
	if err != nil {
		return 0, err
	}
	body, err := p.parseDoBlockWithLeadingGap()
	if err != nil {
		return 0, err
	}
	node := p.mkIR("fun", openTok, p.i-1, h.params, h.arrow, body)
	return node, nil
}

func (p *parser) oracleExpr(openTok int) (NodeID, error) {
	h, err := p.parseFnHeader("oracle")
	if err != nil {
		return 0, err
	}
	var src NodeID
	// Collect annotations/newlines before optional 'from'.
	saveI := p.i
	preFrom := p.collectGap()
	matchedFrom := p.match(FROM)
	if matchedFrom {
		fromTok := p.prev()
		postFrom, err := p.takeReqGap(fromTok, "expected expression after 'from'")
		if err != nil {
			return 0, err
		}
		ex, err := p.expr(0)
		if err != nil {
			return 0, err
		}
		ex = p.attachAnnotFrom(mergeAnn(preFrom, postFrom), ex)
		src = ex
	} else {
		// No 'from': restore so the gap belongs to whatever follows the oracle.
		p.i = saveI
		src = p.mkIR("array", -1, -1) // build only when needed
	}
	body := p.mkIR("oracle", openTok, p.i-1, h.params, h.arrow, src)
	return body, nil
}

// readKeyString allows stacked PRE-annotations (handled recursively).
// Span order: (1) PRE "str" child; (2) "annot" wrapper; (3) final key "str" leaf.
func (p *parser) readKeyString() (NodeID, annSrc, error) {
	// Stackable PRE immediately before key
	var pre annSrc
	for !p.atEnd() && p.peek().Type == ANNOTATION {
		pre = mergeAnn(pre, p.collectAnnotsSrc())
	}
	if p.match(STRING) {
		return p.mkLeafIR("str", p.i-1, p.prev().Literal), pre, nil
	}
	t := p.peek()
	if isWordLike(t.Type) {
		p.i++
		name := t.Lexeme
		if s, ok := t.Literal.(string); ok {
			name = s
		}
		return p.mkLeafIR("str", p.i-1, name), pre, nil
	}
	g := p.peek()
	if p.interactive && p.onlyGapsToEOF() {
		line, col := p.posAfterLastSpan()
		return 0, annNone(), &Error{Kind: DiagIncomplete, Msg: "expected key", Line: line, Col: col}
	}
	line, col := p.posAtByte(g.StartByte)
	return 0, annNone(), &Error{Kind: DiagParse, Msg: "expected key", Line: line, Col: col}
}

func isWordLike(tt TokenType) bool {
	switch tt {
	case ID, TYPE, ENUM, BOOLEAN, NULL,
		AND, OR, NOT, LET, DO, END, RETURN, BREAK, CONTINUE,
		IF, THEN, ELIF, ELSE, FUNCTION, ORACLE, MODULE, FOR, IN, FROM,
		TYPECONS:
		return true
	}
	return false
}

// ───────────────────────────── for-target helpers ─────────────────────────

func (p *parser) forTarget() (NodeID, error) {
	// 'for let P in ...'
	if p.match(LET) {
		letTokIdx := p.i - 1
		letTok := p.toks[letTokIdx]
		pre, err := p.takeReqGap(letTok, "expected pattern after 'let'")
		if err != nil {
			return 0, err
		}
		const assignBP = 10
		pat, err := p.expr(assignBP + 1)
		if err != nil {
			return 0, err
		}
		if pre.ok {
			pat = p.attachAnnotFrom(pre, pat)
		}
		endTok := p.lastSpanEndTok
		if endTok < 0 {
			endTok = letTokIdx
		}
		return p.mkIR("let", letTokIdx, endTok, pat), nil
	}

	// Otherwise parse an expression and require it to be assignable.
	save := p.i
	e, err := p.expr(90)
	if err != nil {
		return 0, err
	}
	if !p.assignableIR(e) {
		p.i = save
		g := p.peek()
		line, col := p.posAtByte(g.StartByte)
		return 0, &Error{Kind: DiagParse, Msg: "invalid for-target", Line: line, Col: col}
	}
	return e, nil
}

func (p *parser) assignableIR(id NodeID) bool {
	// unwrap annotations
	cur := id
	for {
		n := p.ir.Nodes[cur]
		if n.Tag == "annot" && len(n.Kids) >= 2 {
			cur = n.Kids[1]
			continue
		}
		break
	}
	tag := p.ir.Nodes[cur].Tag
	switch tag {
	case "id", "get", "idx", "array", "map", "let":
		return true
	default:
		return false
	}
}
=== END FILE: ./mindscript/parser.go ===

=== BEGIN FILE: ./mindscript/printer.go ===
// printer.go: pretty-printers for MindScript ASTs, types, and runtime values.
//
// What this file does
// -------------------
// This module provides the formatting layer for MindScript. It renders two
// kinds of data to human-readable, stable strings:
//
//  1. Parsed source ASTs (S-expressions) → MindScript source code.
//     - Entry points: Pretty, Standardize, FormatSExpr.
//     - Produces whitespace- and newline-stable output with minimal
//     parentheses, based on operator precedence. It understands all
//     statement and expression tags emitted by the parser (e.g. "fun",
//     "oracle", "for", "if/elif/else", "type", "block", "assign",
//     "return/break/continue", arrays, maps, calls, indexing, properties,
//     unary and binary operators).
//     - Annotation nodes use the simplified 3-ary form:
//     ("annot", ("str", text), wrappedNode)
//     PRE/POST is *not* encoded by the parser anymore. All annotations are
//     attached to values (or noops), and the pretty-printer decides PRE vs POST
//     based on layout at binding sites.
//     - Formatting emits no space before '(' for calls and for 'fun(...)'
//     and 'oracle(...)' parameter lists, matching the lexer’s CLROUND rule.
//     - Control keywords render without parens:
//     return expr
//     break expr
//     continue [expr]
//     A `null` payload prints as the bare keyword (e.g., `continue`).
//
//  2. Type ASTs (S-expressions) → compact type strings.
//     - Entry point: FormatType.
//     - Supported forms:
//     ("id", "Any"|"Null"|"Bool"|"Int"|"Num"|"Str"|"Type")
//     ("unop","?", T)         → prints as `T?`
//     ("array", T)            → prints as `[T]`
//     ("map", ("pair"| "pair!", ("str",k), T) ...)
//     Required fields print with a trailing `!` on the key.
//     Value annotations (if wrapped in "annot") are respected and decided
//     PRE vs POST by the same centralized policy as expressions.
//     ("enum", literalS... )  → prints as `Enum[ ... ]`, where members
//     may be scalars, arrays, or maps.
//     ("binop","->", A, B)    → prints as `(A) -> B`, flattened across
//     right-associated chains.
//     - Output is stable. Multi-line maps are rendered with sorted keys to
//     avoid visual churn.
//     - When the last field ends with a POST, the closing `}` appears on the
//     next line without an extra blank line.
//
// Dependencies (other files)
// --------------------------
// • parser.go
//   - S = []any (AST payload shape)
//   - ParseSExpr(string) (used by Pretty/Standardize)
//   - AST tags: "block", "fun", "oracle", "for", "if",
//     "type", "return", "break", "continue", "assign", "let", "array", "map",
//     "pair"/"pair!", "get", "idx", "call", "id", "str", "int", "num", "bool",
//     "null", "unop", "binop", "annot", "noop".
//
// • interpreter.go (runtime model)
//   - Value, ValueTag (VTNull, VTBool, VTInt, VTNum, VTStr, VTArray, VTMap,
//     VTFun, VTType, VTModule, VTHandle)
//   - Fun, TypeValue, MapObject (Entries/Keys).
//
// • modules.go (module loader)
//   - Module struct and prettySpec(string) (used for VTModule display).
//
// • errors.go (shared errors)
//   - WrapErrorWithSource(err, src) (used by Pretty/Standardize).
//
// PUBLIC vs PRIVATE layout
// ------------------------
// This file is organized in two blocks:
//  1. PUBLIC: the user-facing constants & functions with thorough docstrings.
//  2. PRIVATE: helper types and functions that implement the printers.
//
// Formatting policy highlights
// ----------------------------
//   - Indentation uses **tabs** only (gofmt-style).
//   - Canonical output (`Standardize`) ends with exactly one trailing '\n'.
//
// Requiredness in value maps
// --------------------------
// The printer never emits required fields ("pair!") in **expression/value maps**:
// requiredness is a **type-level** concept only. If the AST carried "pair!" in a
// value map (e.g., via parser sugar), it is dropped in the printed code.
//
// Canonicalizations & Omissions (parser ↔ printer contract)
// ---------------------------------------------------------
// These are deliberate simplifications made by the parser and normalized by
// the printer; users may not see certain syntactic sugar re-emitted:
//   - Param types default to `Any` and are not printed (e.g., `fun(x)` not `x: Any`).
//   - Function return type defaults to `Any` and is not printed (`fun(...) do ... end`
//     without `-> Any`).
//   - `oracle(...)` without `from` carries an empty default source; `from ...` is omitted.
//   - Bare `return` / `break` / `continue` carry an implicit `null` value and print
//     as the bare keyword (no `null`).
//   - Redundant parentheses are removed; only minimal parentheses are emitted.
//   - Calls print with no space before '(' (canonical `f(x)` form).
//   - Property indices written as `obj.(expr)` or `obj.12` are printed canonically
//     as `obj[expr]` / `obj[12]`.
//   - Trailing commas in arrays/maps/parameter lists are dropped in output.
//   - Map keys that are identifier-like print without quotes; others are quoted.
//   - **Expression maps** ignore the required marker `!` at runtime; the printer
//     therefore **drops `!` in value maps** (e.g., `{ id!: 1 }` → `{ id: 1 }`).
//     (Type maps still print required keys as `key!`.)
package mindscript

import (
	"fmt"
	"sort"
	"strconv"
	"strings"
)

// ==============================
// ========== PUBLIC ============
// ==============================

// MaxInlineWidth controls when arrays/maps are rendered on a single line by
// FormatValue / FormatType / FormatSExpr. The single-line decision accounts for
// the current indentation; i.e., it uses the remaining space on the line after
// tabs (tab width = 4) and any preceding text.
var MaxInlineWidth = 80

// Pretty parses a MindScript source string and returns a formatted version.
//
// Behavior:
//   - Parses src via ParseSExpr. If parsing fails, the error is wrapped with
//     source context via WrapErrorWithSource.
//   - On success, pretty-prints the AST using FormatSExpr, producing stable,
//     whitespace-normalized code with minimal parentheses.
//   - Supports annotations using the 3-ary form:
//     ("annot", ("str", text), X)
//     PRE/POST is chosen by the pretty-printer at binding sites.
//
// Errors:
//   - Returns a non-nil error if parsing fails; otherwise returns the formatted text.
func Pretty(src string) (string, error) {
	ast, err := ParseSExpr(src)
	if err != nil {
		if e, ok := err.(*Error); ok {
			if e.Src == nil {
				e.Src = &SourceRef{Name: "<main>", Src: src}
			}
			return "", fmt.Errorf("%s", FormatError(e))
		}
		return "", err
	}
	return FormatSExpr(ast), nil
}

// Standardize returns the canonical source form:
//   - deterministic layout
//   - indentation using tabs
//   - exactly one trailing newline
//
// It is equivalent to Pretty(src), but ensures precisely one '\n' at the end.
func Standardize(src string) (string, error) {
	ast, err := ParseSExpr(src)
	if err != nil {
		if e, ok := err.(*Error); ok {
			if e.Src == nil {
				e.Src = &SourceRef{Name: "<standardize>", Src: src}
			}
			return "", fmt.Errorf("%s", FormatError(e))
		}
		return "", err
	}
	out := FormatSExpr(ast)
	if !strings.HasSuffix(out, "\n") {
		out += "\n"
	} else {
		out = strings.TrimRight(out, "\n") + "\n"
	}
	return out, nil
}

// FormatSExpr renders a parsed MindScript AST (S-expr) to a stable source string.
//
// Inputs:
//   - n: an AST produced by parser.go (e.g., the result of ParseSExpr).
//
// Output policy:
//   - Statements (fun/oracle/for/if/type/block/return/break/continue/assign)
//     are rendered with keywords and indentation.
//   - Expressions use minimal parentheses according to a fixed precedence table;
//     property access vs calls/indexing binds tightly.
//   - Arrays and maps are printed inline (AST form).
//   - Annotation nodes wrap the printed construct; PRE vs POST is chosen centrally.
//   - **POST-after-separator rule** is enforced for inline cases.
//
// This function does not parse; it strictly formats the provided AST.
func FormatSExpr(n S) string {
	doc := docProgram(n)
	var b strings.Builder
	r := renderer{
		out:      &b,
		maxWidth: MaxInlineWidth,
		tabWidth: 4,
	}
	r.render(doc)
	return strings.TrimRight(b.String(), "\n")
}

// FormatType renders a type S-expression into a compact, human-readable string.
// It uses the same centralized PRE/POST policy for value annotations inside
// type maps and enum literals.
func FormatType(t S) string {
	doc := docType(t)
	var b strings.Builder
	r := renderer{
		out:      &b,
		maxWidth: MaxInlineWidth,
		tabWidth: 4,
	}
	r.render(doc)
	return b.String()
}

// FormatValue renders a runtime Value by first adapting it to the printer’s AST
// (with cycle guards and opaque fallbacks) and then delegating to FormatSExpr.
func FormatValue(v Value) string {
	ast := ValueToAST(v)
	return FormatSExpr(ast)
}

//// END_OF_PUBLIC

// ===============================
// ========= PRIVATE =============
// ===============================

/* ---------- small globals & utilities ---------- */

func isIdent(s string) bool {
	if s == "" {
		return false
	}
	b := []byte(s)
	c := b[0]
	if !((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') || c == '_') {
		return false
	}
	for i := 1; i < len(b); i++ {
		c = b[i]
		if !((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') || (c >= '0' && c <= '9') || c == '_') {
			return false
		}
	}
	return true
}

func quoteString(s string) string {
	var b strings.Builder
	b.WriteByte('"')
	for _, r := range s {
		switch r {
		case '\\':
			b.WriteString(`\\`)
		case '"':
			b.WriteString(`\"`)
		case '\n':
			b.WriteString(`\n`)
		case '\r':
			b.WriteString(`\r`)
		case '\t':
			b.WriteString(`\t`)
		case '\b':
			b.WriteString(`\b`)
		case '\f':
			b.WriteString(`\f`)
		default:
			b.WriteRune(r)
		}
	}
	b.WriteByte('"')
	return b.String()
}

func oneLine(s string) string {
	s = strings.ReplaceAll(s, "\n", " ")
	return strings.TrimSpace(s)
}

// unwrap VTType payload to its AST (supports legacy S too).
func typeAst(data any) S {
	switch tv := data.(type) {
	case *TypeValue:
		return tv.Ast
	case S:
		return tv
	default:
		return S{}
	}
}

// NOTE: The parser no longer encodes PRE vs POST; all annotations live on values.
// The pretty-printer chooses PRE vs POST purely by layout at binding sites.

/* ---------- Doc engine (tiny) ---------- */

type docKind int

const (
	dText     docKind = iota
	dLine             // space if flat, newline if broken
	dSoftLine         // empty if flat, newline if broken
	dHardLine         // always newline
	dGroup
	dNest
	dConcat
)

type Doc struct {
	k      docKind
	s      string
	a      *Doc
	kids   []*Doc
	indent int // for Nest
}

func Text(s string) *Doc      { return &Doc{k: dText, s: s} }
func LineDoc() *Doc           { return &Doc{k: dLine} }
func SoftLineDoc() *Doc       { return &Doc{k: dSoftLine} }
func HardLineDoc() *Doc       { return &Doc{k: dHardLine} }
func Group(d *Doc) *Doc       { return &Doc{k: dGroup, a: d} }
func Nest(n int, d *Doc) *Doc { return &Doc{k: dNest, a: d, indent: n} }
func Concat(ds ...*Doc) *Doc  { return &Doc{k: dConcat, kids: ds} }

func Join(sep *Doc, items []*Doc) *Doc {
	if len(items) == 0 {
		return Concat()
	}
	out := make([]*Doc, 0, len(items)*2-1)
	for i, it := range items {
		if i > 0 {
			out = append(out, sep)
		}
		out = append(out, it)
	}
	return Concat(out...)
}

type renderer struct {
	out      *strings.Builder
	maxWidth int
	tabWidth int

	col         int  // current column in characters (tabs count as tabWidth)
	depth       int  // indentation depth (tabs)
	atLineStart bool // just after newline
}

func (r *renderer) writeIndentIfNeeded() {
	if r.atLineStart {
		for i := 0; i < r.depth; i++ {
			r.out.WriteByte('\t')
		}
		r.col = r.depth * r.tabWidth
		r.atLineStart = false
	}
}
func (r *renderer) writeString(s string) {
	if s == "" {
		return
	}
	r.writeIndentIfNeeded()
	r.out.WriteString(s)
	r.col += len(s)
}
func (r *renderer) newline() {
	r.out.WriteByte('\n')
	r.atLineStart = true
	// col will be set when indent is written
}

func (r *renderer) render(d *Doc) {
	r.atLineStart = false // caller controls leading indentation
	r.renderGroup(d)
}

func (r *renderer) renderGroup(d *Doc) {
	// Render a group with "flat if fits" policy.
	if r.fitsFlat(d, r.maxWidth-r.col) {
		r.renderFlat(d)
	} else {
		r.renderBroken(d)
	}
}

func (r *renderer) renderFlat(d *Doc) {
	switch d.k {
	case dText:
		r.writeString(d.s)
	case dLine:
		r.writeString(" ")
	case dSoftLine:
		// nothing
	case dHardLine:
		// hard line cannot appear in flat mode if fitsFlat was true,
		// but guard just in case: break the line.
		r.newline()
	case dGroup:
		r.renderFlat(d.a)
	case dNest:
		old := r.depth
		r.depth += d.indent
		r.renderFlat(d.a)
		r.depth = old
	case dConcat:
		for _, k := range d.kids {
			r.renderFlat(k)
		}
	}
}

func (r *renderer) renderBroken(d *Doc) {
	switch d.k {
	case dText:
		r.writeString(d.s)
	case dLine:
		r.newline()
	case dSoftLine:
		r.newline()
	case dHardLine:
		r.newline()
	case dGroup:
		// In broken mode, nested groups still try flat if they fit at this point.
		r.renderGroup(d.a)
	case dNest:
		old := r.depth
		r.depth += d.indent
		r.renderBroken(d.a)
		r.depth = old
	case dConcat:
		for _, k := range d.kids {
			r.renderBroken(k)
		}
	}
}

// fitsFlat reports whether the doc can be rendered flat within the given budget.
// Any HardLine inside makes it not flat-fit.
func (r *renderer) fitsFlat(d *Doc, budget int) bool {
	if budget < 0 {
		return false
	}
	switch d.k {
	case dText:
		return len(d.s) <= budget
	case dLine:
		return 1 <= budget
	case dSoftLine:
		return 0 <= budget
	case dHardLine:
		return false
	case dGroup:
		return r.fitsFlat(d.a, budget)
	case dNest:
		return r.fitsFlat(d.a, budget)
	case dConcat:
		for _, k := range d.kids {
			if !r.fitsFlat(k, budget) {
				return false
			}
			// reduce budget by flat width of k
			budget -= flatWidth(k)
		}
		return true
	default:
		return false
	}
}

func flatWidth(d *Doc) int {
	switch d.k {
	case dText:
		return len(d.s)
	case dLine:
		return 1
	case dSoftLine:
		return 0
	case dHardLine:
		return 1 // arbitrary; but any hardline makes fitsFlat false before using this
	case dGroup:
		return flatWidth(d.a)
	case dNest:
		return flatWidth(d.a)
	case dConcat:
		sum := 0
		for _, k := range d.kids {
			sum += flatWidth(k)
		}
		return sum
	default:
		return 0
	}
}

/* ---------- shared Doc helpers ---------- */

func idOrQuoted(name string) *Doc {
	if isIdent(name) {
		return Text(name)
	}
	return Text(quoteString(name))
}

// PRE annotations (block/head) — prints as lines above current position.
func annotPre(text string) *Doc {
	if strings.TrimSpace(text) == "" {
		return Concat()
	}
	lines := strings.Split(text, "\n")
	ds := make([]*Doc, 0, len(lines)*2)
	for _, ln := range lines {
		ln = strings.TrimSpace(ln)
		ds = append(ds, Text("# "+ln), HardLineDoc())
	}
	return Concat(ds...)
}

// POST annotations (inline/trailing) — prints on the same line.
// IMPORTANT: POST captures the rest of the line, so we force a newline here.
func annotInline(text string) *Doc {
	trim := oneLine(text)
	if trim == "" {
		return Concat()
	}
	return Concat(Text(" # "+trim), HardLineDoc())
}

func braced(open string, inside *Doc, close string) *Doc {
	return Concat(Text(open), inside, Text(close))
}

// inlineOrMultiAdvanced builds a `[ a, b ]` or multi-line with indentation.
// If endsLastLine is true, the trailing SoftLine is omitted to avoid an extra
// blank line before the closing bracket/brace.
func inlineOrMultiAdvanced(open string, elems []*Doc, close string, endsLastLine bool) *Doc {
	if len(elems) == 0 {
		// exact-empty without spaces: [] or {}
		return Text(open + close)
	}
	sep := Concat(Text(","), LineDoc())
	inside := Join(sep, elems)
	body := Concat(SoftLineDoc(), inside)
	if !endsLastLine {
		body = Concat(body, SoftLineDoc())
	}
	return Group(braced(open, Nest(1, body), close))
}

// inlineOrMulti is the default variant when the last element does not force
// a newline (or when callers don't track it).
func inlineOrMulti(open string, elems []*Doc, close string) *Doc {
	return inlineOrMultiAdvanced(open, elems, close, false)
}

// Minimal entry builder; annotation handling is centralized elsewhere.
func kvEntry(keyDoc *Doc, valDoc *Doc) *Doc {
	return Concat(keyDoc, Text(": "), valDoc)
}

/* ---------- Comma-aware joining (centralized POST-after-comma logic) ---------- */

type sepItem struct {
	main *Doc // rendered item (element or entry) without its trailing POST
	post string
}

// joinCommaWithPost joins items with commas, printing any item's POST
// *after the comma that follows that item*. The last item's POST (if any)
// prints after the item (no comma). POST forces newline via annotInline.
func joinCommaWithPost(items []sepItem) *Doc {
	if len(items) == 0 {
		return Concat()
	}
	out := make([]*Doc, 0, len(items)*3)
	for i, it := range items {
		out = append(out, it.main)
		if i < len(items)-1 {
			out = append(out, Text(","))
			if it.post != "" {
				out = append(out, annotInline(it.post))
			} else {
				out = append(out, LineDoc())
			}
		} else if it.post != "" {
			out = append(out, annotInline(it.post))
		}
	}
	return Concat(out...)
}

/* ---------- AST helpers: tags, shapes, precedence ---------- */

func tag(n S) string   { return n[0].(string) }
func getId(n S) string { return n[1].(string) }
func getStr(n S) string {
	// Used for ("str", s), but safe for ("id", name) too.
	return n[1].(string)
}
func listS(n S, from int) []S {
	if len(n) <= from {
		return nil
	}
	out := make([]S, 0, len(n)-from)
	for i := from; i < len(n); i++ {
		out = append(out, n[i].(S))
	}
	return out
}

// Keys/names are not annotated; unwrap name only.
func unwrapKeyName(n S) string { return n[1].(string) }

var binPrec = map[string]struct {
	p     int
	right bool
}{
	"->": {15, true},
	"*":  {70, false}, "/": {70, false}, "%": {70, false},
	"+": {60, false}, "-": {60, false},
	"<": {50, false}, "<=": {50, false}, ">": {50, false}, ">=": {50, false},
	"==": {40, false}, "!=": {40, false},
	"and": {30, false},
	"or":  {20, false},
}

func exprPrec(n S) int {
	switch tag(n) {
	case "assign":
		return 10
	case "binop":
		if pr, ok := binPrec[n[1].(string)]; ok {
			return pr.p
		}
		return 60
	case "unop":
		if n[1].(string) == "?" {
			return 90
		}
		return 80
	case "call", "idx", "get":
		return 90
	default:
		return 100
	}
}

func parenIf(need int, d *Doc, n S) *Doc {
	if exprPrec(n) < need {
		return Concat(Text("("), d, Text(")"))
	}
	return d
}

func parenIfLE(need int, d *Doc, n S) *Doc {
	if exprPrec(n) <= need {
		return Concat(Text("("), d, Text(")"))
	}
	return d
}

/* ---------- AST → Doc ---------- */

// docSeq renders a sequence of statements, preserving explicit blank-line
// separators represented as "noop" nodes:
//
//	stmt1, noop, stmt2  →  stmt1\n\nstmt2
//
// while keeping the old behavior when no "noop" is present:
//
//	stmt1, stmt2        →  stmt1\nstmt2
//
// Leading and trailing noops in a block are ignored.
func docSeq(kids []S) *Doc {
	var ds []*Doc
	firstStmt := true
	pendingNoop := false

	for _, k := range kids {
		if tag(k) == "noop" {
			// A noop only has effect when it appears between real statements.
			if !firstStmt {
				pendingNoop = true
			}
			continue
		}

		if firstStmt {
			ds = append(ds, docStmt(k))
			firstStmt = false
			pendingNoop = false
			continue
		}

		// Always separate statements with at least one newline, and add an
		// extra blank line when an explicit noop was present between them.
		ds = append(ds, HardLineDoc())
		if pendingNoop {
			ds = append(ds, HardLineDoc())
		}
		ds = append(ds, docStmt(k))
		pendingNoop = false
	}
	return Concat(ds...)
}

func docProgram(n S) *Doc {
	if tag(n) != "block" {
		return docStmt(n)
	}
	return docSeq(listS(n, 1))
}

func docStmt(n S) *Doc {
	switch tag(n) {
	case "noop":
		return Concat()

	case "annot":
		text, wrapped, _ := asAnnotASTRaw(n)

		// Special case: ("annot", text, "noop") represents a floating header
		// annotation with a blank-line run afterwards, e.g.:
		//
		//   # a
		//
		//   x
		//
		// Render it as a PRE-style comment block here; docSeq/docBlock will
		// supply the blank line between this and the next real statement.
		if tag(wrapped) == "noop" {
			return annotPre(text)
		}

		body := docStmt(wrapped)
		main, post := attachInlineOrPre(body, text)
		if post != "" {
			// Inline statement annotation: keep it on the same line as the
			// wrapped statement; line-breaking between statements is handled
			// centrally by docSeq/docBlock.
			return Concat(main, Text(" # "+oneLine(post)))
		}
		// PRE-style or multi-line annotations are already baked into main
		// by attachInlineOrPre (via annotPre).
		return main

	case "fun":
		params, ret, body := n[1].(S), n[2].(S), n[3].(S)
		header := Concat(Text("fun("), docParams(params), Text(")"))
		if !(tag(ret) == "id" && getId(ret) == "Any") {
			header = Concat(header, Text(" -> "), docType(ret))
		}
		return Concat(
			header, Text(" do"), HardLineDoc(),
			Nest(1, docBlock(body)), HardLineDoc(),
			Text("end"),
		)

	case "oracle":
		params, outT, src := n[1].(S), n[2].(S), n[3].(S)
		header := Concat(Text("oracle("), docParams(params), Text(")"))
		if !(tag(outT) == "id" && getId(outT) == "Any") {
			header = Concat(header, Text(" -> "), docType(outT))
		}
		if !(tag(src) == "array" && len(src) == 1) {
			header = Concat(header, Text(" from "), docExpr(src))
		}
		return header

	case "for":
		tgt, iter, body := n[1].(S), n[2].(S), n[3].(S)
		// Target never prints "let" — it's implied in the surface syntax.
		head := Concat(Text("for "), docPattern(tgt), Text(" in "), docExpr(iter), Text(" do"))
		return Concat(head, HardLineDoc(), Nest(1, docBlock(body)), HardLineDoc(), Text("end"))

	case "while":
		cond, body := n[1].(S), n[2].(S)
		head := Concat(Text("while "), docExpr(cond), Text(" do"))
		return Concat(head, HardLineDoc(), Nest(1, docBlock(body)), HardLineDoc(), Text("end"))

	case "if":
		arms := listS(n, 1)
		first := arms[0]
		d := Concat(
			Text("if "), docExpr(first[1].(S)), Text(" then"), HardLineDoc(),
			Nest(1, docBlock(first[2].(S))),
		)
		for i := 1; i < len(arms) && tag(arms[i]) == "pair"; i++ {
			arm := arms[i]
			d = Concat(d, HardLineDoc(),
				Text("elif "), docExpr(arm[1].(S)), Text(" then"), HardLineDoc(),
				Nest(1, docBlock(arm[2].(S))),
			)
		}
		// possible else block
		if last := arms[len(arms)-1]; tag(last) != "pair" {
			d = Concat(d, HardLineDoc(), Text("else"), HardLineDoc(), Nest(1, docBlock(last)))
		}
		return Concat(d, HardLineDoc(), Text("end"))

	case "module":
		nameExpr, body := n[1].(S), n[2].(S)
		return Concat(Text("module "), docExpr(nameExpr), Text(" do"), HardLineDoc(),
			Nest(1, docBlock(body)), HardLineDoc(), Text("end"))

	case "type":
		return Concat(Text("type "), docType(n[1].(S)))

	case "return":
		arg := n[1].(S)
		if tag(arg) == "null" {
			return Text("return")
		}
		return Concat(Text("return "), docExpr(arg))
	case "break":
		arg := n[1].(S)
		if tag(arg) == "null" {
			return Text("break")
		}
		return Concat(Text("break "), docExpr(arg))
	case "continue":
		arg := n[1].(S)
		if tag(arg) == "null" {
			return Text("continue")
		}
		return Concat(Text("continue "), docExpr(arg))

	case "let":
		// Declaration-only: let P
		if len(n) >= 2 {
			return Concat(Text("let "), docPattern(n[1].(S)))
		}
		return Text("let")

	case "assign":
		// Decide PRE vs POST for the whole binding (let-or-assign).
		lhs, rhs := n[1].(S), n[2].(S)
		var head *Doc
		if isDeclPattern(lhs) {
			// Declarative assignment: let P = E
			// Strip the syntactic 'let' wrapper from the pattern.
			var pat S
			if tag(lhs) == "let" && len(lhs) >= 2 {
				pat = lhs[1].(S)
			} else {
				pat = lhs
			}
			head = Concat(Text("let "), docPattern(pat), Text(" = "))
		} else {
			head = Concat(docExprMin(lhs, 10), Text(" = "))
		}
		if txt, inner, ok := asAnnotASTRaw(rhs); ok && strings.TrimSpace(txt) != "" {
			val := docExprMin(inner, 10)
			probe := Concat(head, val)
			main, post := attachInlineOrPre(probe, txt)
			if post != "" {
				// Inline binding-level annotation: same behavior as docStmt("annot")
				return Concat(main, Text(" # "+oneLine(post)))
			}
			// PRE-style: annotPre already baked into main.
			return main
		}
		return Concat(head, docExprMin(rhs, 10))

	case "block":
		return Concat(Text("do"), HardLineDoc(), Nest(1, docBlock(n)), HardLineDoc(), Text("end"))

	default:
		return docExpr(n)
	}
}

func docBlock(n S) *Doc {
	if tag(n) != "block" {
		return docStmt(n)
	}
	return docSeq(listS(n, 1))
}

func docParams(arr S) *Doc {
	if tag(arr) != "array" || len(arr) == 1 {
		return Concat()
	}
	items := listS(arr, 1)
	var parts []*Doc
	for i, pi := range items {
		name := getId(pi[1].(S))
		ty := pi[2].(S)
		if !(tag(ty) == "id" && getId(ty) == "Any") {
			parts = append(parts, Concat(Text(name), Text(": "), docType(ty)))
		} else {
			parts = append(parts, Text(name))
		}
		if i < len(items)-1 {
			parts = append(parts, Text(", "))
		}
	}
	return Concat(parts...)
}

func docExprMin(n S, need int) *Doc {
	return parenIf(need, docExpr(n), n)
}

// (old trailing-post helpers removed; annotation policy is centralized)

func docExpr(n S) *Doc {
	switch tag(n) {
	case "id":
		return Text(getId(n))
	case "int":
		return Text(fmt.Sprint(n[1]))
	case "num":
		s := strconv.FormatFloat(n[1].(float64), 'g', -1, 64)
		if !strings.ContainsAny(s, ".eE") {
			s += ".0"
		}
		return Text(s)
	case "str":
		return Text(quoteString(getStr(n)))
	case "bool":
		if n[1].(bool) {
			return Text("true")
		}
		return Text("false")
	case "null":
		return Text("null")

	case "unop":
		op, rhs := n[1].(string), n[2].(S)
		if op == "?" {
			return Concat(docExprMin(rhs, 90), Text("?"))
		}
		if op == "not" {
			return Concat(Text("not "), docExprMin(rhs, 80))
		}
		return Concat(Text(op), docExprMin(rhs, 80))

	case "binop":
		op, l, r := n[1].(string), n[2].(S), n[3].(S)
		my, right := 60, false
		if pr, ok := binPrec[op]; ok {
			my, right = pr.p, pr.right
		}
		// Associativity-aware parentheses:
		//  - right-assoc:  paren LEFT if prec(left) <= my; RIGHT if prec(right) < my
		//  - left-assoc:   paren LEFT if prec(left) <  my; RIGHT if prec(right) <= my
		lDoc := docExpr(l)
		rDoc := docExpr(r)
		if right {
			lDoc = parenIfLE(my, lDoc, l) // inclusive on left
			rDoc = parenIf(my, rDoc, r)   // exclusive on right
		} else {
			lDoc = parenIf(my, lDoc, l)   // exclusive on left
			rDoc = parenIfLE(my, rDoc, r) // inclusive on right
		}
		return Concat(lDoc, Text(" "+op+" "), rDoc)

	case "assign":
		l, r := n[1].(S), n[2].(S)
		if isDeclPattern(l) {
			// let P = E in expression position
			var pat S
			if tag(l) == "let" && len(l) >= 2 {
				pat = l[1].(S)
			} else {
				pat = l
			}
			return Concat(Text("let "), docPattern(pat), Text(" = "), docExprMin(r, 10))
		}
		return Concat(docExprMin(l, 10), Text(" = "), docExprMin(r, 10))

	case "call":
		recv := n[1].(S)
		args := listS(n, 2)
		var argDocs []*Doc
		for _, a := range args {
			argDocs = append(argDocs, docExpr(a))
		}
		return Concat(docExprMin(recv, 90), Text("("), Join(Text(", "), argDocs), Text(")"))

	case "idx":
		recv, ix := n[1].(S), n[2].(S)
		// Be careful with array indices: this is indexing, not array literal.
		return Concat(docExprMin(recv, 90), Text("["), docExpr(ix), Text("]"))

	case "get":
		recv, name := n[1].(S), n[2].(S)[1].(string)
		if isIdent(name) {
			return Concat(docExprMin(recv, 90), Text("."+name))
		}
		return Concat(docExprMin(recv, 90), Text("."+quoteString(name)))

	case "array":
		elems := listS(n, 1)
		if len(elems) == 0 {
			return Text("[]")
		}
		items := make([]sepItem, 0, len(elems))
		for _, e := range elems {
			if txt, inner, ok := asAnnotASTRaw(e); ok {
				main, post := attachInlineOrPre(docExpr(inner), txt)
				items = append(items, sepItem{main: main, post: post})
			} else {
				items = append(items, sepItem{main: docExpr(e), post: ""})
			}
		}
		inside := joinCommaWithPost(items)
		lastEnds := items[len(items)-1].post != ""
		return Group(braced("[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))

	case "map":
		items := listS(n, 1)
		if len(items) == 0 {
			return Text("{}")
		}
		joined := make([]sepItem, 0, len(items))
		for _, pr := range items {
			key := unwrapKeyName(pr[1].(S))
			val := pr[2].(S)
			if txt, inner, ok := asAnnotASTRaw(val); ok {
				joined = append(joined, entryWithAnn(idOrQuoted(key), docExpr(inner), txt))
			} else {
				joined = append(joined, sepItem{main: kvEntry(idOrQuoted(key), docExpr(val)), post: ""})
			}
		}
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("{", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "}"))

	case "enum":
		elems := listS(n, 1)
		if len(elems) == 0 {
			return Text("Enum[]")
		}
		items := make([]sepItem, 0, len(elems))
		for _, e := range elems {
			if txt, inner, ok := asAnnotASTRaw(e); ok {
				main, post := attachInlineOrPre(docExpr(inner), txt)
				items = append(items, sepItem{main: main, post: post})
			} else {
				items = append(items, sepItem{main: docExpr(e), post: ""})
			}
		}
		inside := joinCommaWithPost(items)
		lastEnds := items[len(items)-1].post != ""
		return Group(braced("Enum[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))

	case "let":
		// let P used as an expression
		if len(n) >= 2 {
			return Concat(Text("let "), docPattern(n[1].(S)))
		}
		return Text("let")

	case "return", "break", "continue", "fun", "oracle", "for", "while", "if", "type", "block", "annot", "module":
		return docStmt(n)

	case "opaque":
		return Text(getStr(n))

	default:
		return Text("<" + tag(n) + ">")
	}
}

/* ---------- patterns ---------- */

func isDeclPattern(n S) bool {
	switch tag(n) {
	case "let":
		return true
	case "annot":
		return isDeclPattern(n[2].(S))
	default:
		return false
	}
}

func docPattern(n S) *Doc {
	switch tag(n) {
	case "let":
		// Pattern wrapper: strip 'let' and print the inner shape.
		if len(n) >= 2 {
			return docPattern(n[1].(S))
		}
		return Text("<pattern>")

	case "id":
		return Text(getId(n))

	case "array":
		items := listS(n, 1)
		if len(items) == 0 {
			return Text("[]")
		}
		joined := make([]sepItem, 0, len(items))
		for _, it := range items {
			if txt, inner, ok := asAnnotASTRaw(it); ok {
				main, post := attachInlineOrPre(docPattern(inner), txt)
				joined = append(joined, sepItem{main: main, post: post})
			} else {
				joined = append(joined, sepItem{main: docPattern(it), post: ""})
			}
		}
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("[", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "]"))

	case "map":
		items := listS(n, 1)
		if len(items) == 0 {
			return Text("{}")
		}
		joined := make([]sepItem, 0, len(items))
		for _, it := range items {
			key := unwrapKeyName(it[1].(S))
			val := it[2].(S)
			if txt, inner, ok := asAnnotASTRaw(val); ok {
				joined = append(joined, entryWithAnn(idOrQuoted(key), docPattern(inner), txt))
			} else {
				joined = append(joined, sepItem{main: kvEntry(idOrQuoted(key), docPattern(val)), post: ""})
			}
		}
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("{", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "}"))

	case "annot":
		text, wrapped, _ := asAnnotASTRaw(n)
		// Pattern wrapper: render as PRE. Entry/element sites decide inline.
		return Concat(annotPre(text), docPattern(wrapped))

	default:
		return docExpr(n)
	}
}

/* ---------- AST "annot" helpers ---------- */

// Neutral unwrap: ("annot", ("str", txt), wrapped) → (txt, wrapped, true)
func asAnnotASTRaw(n S) (text string, wrapped S, ok bool) {
	if tag(n) == "annot" {
		return n[1].(S)[1].(string), n[2].(S), true
	}
	return "", n, false
}

/* ---------- Type pretty-printer (as Doc) ---------- */

func docType(t S) *Doc {
	if len(t) == 0 {
		return Text("<type>")
	}
	switch tag(t) {
	case "id":
		return Text(getStr(t))
	case "get":
		recv := t[1].(S)
		prop := t[2].(S)[1].(string)
		// Reuse docType for the receiver so nested gets print as a.b.c
		// If the receiver were ever non-type-ish, docType will fall back gracefully.
		return Concat(docType(recv), Text("."), idOrQuoted(prop))
	case "unop":
		if t[1].(string) == "?" {
			return Concat(docType(t[2].(S)), Text("?"))
		}
		return Text("<unop>")
	case "array":
		elem := S{"id", "Any"}
		if len(t) == 2 {
			elem = t[1].(S)
		}
		return Concat(Text("["), docType(elem), Text("]"))
	case "enum":
		elems := listS(t, 1)
		if len(elems) == 0 {
			return Text("Enum[]")
		}
		var ds []*Doc
		for _, e := range elems {
			ds = append(ds, docTypeLiteral(e))
		}
		return inlineOrMulti("Enum[", ds, "]")
	case "map":
		type fld struct {
			name string
			req  bool
			typ  S
			vAnn string
		}
		var fs []fld
		for _, raw := range listS(t, 1) {
			req := raw[0].(string) == "pair!"
			k := unwrapKeyName(raw[1].(S))
			ft := raw[2].(S)
			txt, inner, ok := asAnnotASTRaw(ft)
			if ok {
				ft = inner
			}
			fs = append(fs, fld{name: k, req: req, typ: ft, vAnn: strings.TrimSpace(txt)})
		}
		sort.Slice(fs, func(i, j int) bool { return fs[i].name < fs[j].name })
		if len(fs) == 0 {
			return Text("{}")
		}
		joined := make([]sepItem, 0, len(fs))
		for _, f := range fs {
			key := idOrQuoted(f.name)
			if f.req {
				key = Concat(key, Text("!"))
			}
			if f.vAnn != "" {
				joined = append(joined, entryWithAnn(key, docType(f.typ), f.vAnn))
			} else {
				joined = append(joined, sepItem{main: kvEntry(key, docType(f.typ)), post: ""})
			}
		}
		inside := joinCommaWithPost(joined)
		lastEnds := joined[len(joined)-1].post != ""
		return Group(braced("{", Nest(1, Concat(SoftLineDoc(), inside, func() *Doc {
			if lastEnds {
				return Concat()
			}
			return SoftLineDoc()
		}())), "}"))
	case "binop":
		if t[1].(string) == "->" && len(t) >= 4 {
			left := t[2].(S)
			right := t[3].(S)
			// Right-associative printing:
			// - If LEFT is itself an arrow, parenthesize it.
			// - Always render RIGHT via docType (which will continue the chain).
			leftDoc := docType(left)
			if tag(left) == "binop" && left[1].(string) == "->" {
				leftDoc = Concat(Text("("), leftDoc, Text(")"))
			}
			return Concat(leftDoc, Text(" -> "), docType(right))
		}
		return Text("<binop>")
	case "annot":
		txt, wrapped, _ := asAnnotASTRaw(t)
		// Outside binding sites, render as PRE.
		return Concat(annotPre(txt), docType(wrapped))
	default:
		return Text("<type>")
	}
}

func docTypeLiteral(lit S) *Doc {
	switch tag(lit) {
	case "null":
		return Text("null")
	case "bool":
		if lit[1].(bool) {
			return Text("true")
		}
		return Text("false")
	case "int":
		return Text(fmt.Sprint(lit[1]))
	case "num":
		return Text(strconv.FormatFloat(lit[1].(float64), 'g', -1, 64))
	case "str":
		return Text(quoteString(getStr(lit)))
	case "array":
		items := listS(lit, 1)
		if len(items) == 0 {
			return Text("[]")
		}
		var ds []*Doc
		for _, s := range items {
			ds = append(ds, docTypeLiteral(s))
		}
		return inlineOrMulti("[", ds, "]")
	case "map":
		items := listS(lit, 1)
		if len(items) == 0 {
			return Text("{}")
		}
		joined := make([]sepItem, 0, len(items))
		for _, pr := range items {
			k := pr[1].(S)[1].(string)
			val := docTypeLiteral(pr[2].(S))
			entry := kvEntry(idOrQuoted(k), val)
			joined = append(joined, sepItem{main: entry, post: ""})
		}
		inside := joinCommaWithPost(joined)
		return Group(braced("{", Nest(1, Concat(SoftLineDoc(), inside, SoftLineDoc())), "}"))
	default:
		return Text("<lit>")
	}
}

/* ---------- Central annotation policy (single place) ---------- */

// Decide inline vs PRE for a candidate: if `candidate + " # ann"` fits flat
// under a conservative width budget, return (candidate, ann) so callers can
// trail as inline (or after-comma). Otherwise return (PRE+candidate, "").
func attachInlineOrPre(candidate *Doc, ann string) (*Doc, string) {
	ann = strings.TrimSpace(ann)
	if ann == "" {
		return candidate, ""
	}
	// Multi-line annotations always become PRE headers.
	if strings.Contains(ann, "\n") {
		return Concat(annotPre(ann), candidate), ""
	}
	// Conservative budget: avoid over-inlining.
	r := renderer{maxWidth: MaxInlineWidth - 8, tabWidth: 4}
	if r.fitsFlat(Concat(candidate, Text(" # "+oneLine(ann))), r.maxWidth) {
		return candidate, ann
	}
	return Concat(annotPre(ann), candidate), ""
}

// Entry helper applying the policy to `key: value`.
// If inline, the POST returns via .post (rendered after the following comma).
// If PRE, the header is part of .main (no trailing POST).
func entryWithAnn(keyDoc, valDoc *Doc, ann string) sepItem {
	probe := kvEntry(keyDoc, valDoc)
	main, post := attachInlineOrPre(probe, ann)
	if post != "" {
		return sepItem{main: probe, post: post}
	}
	return sepItem{main: main, post: ""}
}

/* ---------- Value → AST adapter (single source of truth) ---------- */

// ValueToAST converts a runtime Value into the printer/parser AST (S), preserving:
//   - annotations (as ("annot", ("str", ...), node))
//   - insertion order of maps (MapObject.Keys)
//   - cycle guards (arrays, maps) with python-style markers via ("opaque", "[...]") / ("opaque", "{...}")
//
// Non-source forms (functions, types, modules, handles, unknown) render as ("opaque", "<...>").
func ValueToAST(v Value) S {
	seenA := make(map[*ArrayObject]bool)
	seenM := make(map[*MapObject]bool)
	n := valueToASTRec(v, seenA, seenM)
	if s := strings.TrimSpace(v.Annot); s != "" {
		return S{"annot", S{"str", s}, n}
	}
	return n
}

func valueToASTRec(v Value, seenA map[*ArrayObject]bool, seenM map[*MapObject]bool) S {
	switch v.Tag {
	case VTNull:
		return S{"null"}
	case VTBool:
		return S{"bool", v.Data.(bool)}
	case VTInt:
		return S{"int", v.Data.(int64)}
	case VTNum:
		return S{"num", v.Data.(float64)}
	case VTStr:
		return S{"str", v.Data.(string)}

	case VTArray:
		ao := v.Data.(*ArrayObject)
		if ao == nil {
			return S{"array"} // treat nil as empty
		}
		if seenA[ao] {
			return S{"opaque", "[...]"}
		}
		seenA[ao] = true
		out := S{"array"}
		for _, ev := range ao.Elems {
			node := valueToASTRec(ev, seenA, seenM)
			if ann := strings.TrimSpace(ev.Annot); ann != "" {
				node = S{"annot", S{"str", ann}, node}
			}
			out = append(out, node)
		}
		return out

	case VTMap:
		mo := v.Data.(*MapObject)
		if mo == nil {
			return S{"map"}
		}
		if seenM[mo] {
			return S{"opaque", "{...}"}
		}
		seenM[mo] = true
		out := S{"map"}
		for _, k := range mo.Keys {
			val := mo.Entries[k]
			node := valueToASTRec(val, seenA, seenM)
			// Annotations live on the VALUE.
			if ann := strings.TrimSpace(val.Annot); ann != "" {
				node = S{"annot", S{"str", ann}, node}
			}
			out = append(out, S{"pair", S{"str", k}, node})
		}
		return out

	case VTFun, VTType, VTModule, VTHandle:
		return S{"opaque", valueOpaqueString(v)}
	default:
		return S{"opaque", valueOpaqueString(v)}
	}
}

func valueOpaqueString(v Value) string {
	switch v.Tag {
	case VTFun:
		if f, ok := v.Data.(*Fun); ok && f != nil {
			label := "fun"
			if f.IsOracle {
				label = "oracle"
			}
			var parts []string
			if len(f.ParamTypes) == 0 {
				parts = append(parts, "_:Null")
			} else {
				for i := range f.ParamTypes {
					name := "_"
					if i < len(f.Params) && f.Params[i] != "" {
						name = f.Params[i]
					}
					pt := FormatType(f.ParamTypes[i])
					// Parenthesize arrow types in param position for readability.
					if len(f.ParamTypes[i]) >= 4 && f.ParamTypes[i][0] == "binop" && f.ParamTypes[i][1] == "->" {
						pt = "(" + pt + ")"
					}
					if i > 0 {
						parts = append(parts, "-> "+name+":"+pt)
					} else {
						parts = append(parts, name+":"+pt)
					}
				}
			}
			ret := FormatType(f.ReturnType)
			if len(parts) > 0 {
				return "<" + label + ": " + strings.Join(parts, " ") + " -> " + ret + ">"
			}
			return "<" + label + ": " + ret + ">"
		}
		return "<fun>"
	case VTType:
		return "<type: " + FormatType(typeAst(v.Data)) + ">"
	case VTModule:
		name := "<module>"
		if m, ok := v.Data.(*Module); ok && m != nil && m.Name != "" {
			disp := prettySpec(m.Name)
			if disp == "" {
				disp = m.Name
			}
			name = "<module: " + disp + ">"
		}
		return name
	case VTHandle:
		if h, ok := v.Data.(*Handle); ok && h != nil && h.Kind != "" {
			return "<handle: " + h.Kind + ">"
		}
		return "<handle>"
	case VTNull:
		return "null"
	case VTBool:
		if b, _ := v.Data.(bool); b {
			return "true"
		}
		return "false"
	case VTInt:
		return strconv.FormatInt(v.Data.(int64), 10)
	case VTNum:
		s := strconv.FormatFloat(v.Data.(float64), 'g', -1, 64)
		if !strings.ContainsAny(s, ".eE") {
			s += ".0"
		}
		return s
	case VTStr:
		// Reuse the same quoting policy for visibility; the opaque string is expected raw.
		return quoteString(v.Data.(string))
	default:
		return "<unknown>"
	}
}
=== END FILE: ./mindscript/printer.go ===

