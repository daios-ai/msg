=== BEGIN PUBLIC SECTION OF FILE: lexer.go ===
// lexer.go: provides a whitespace-sensitive, UTF-8–aware lexer for the
// MindScript language. It converts a source string into a linear stream of
// tokens with accurate source positions and rich literal decoding.
//
// ──────────────────────────────────────────────────────────────────────────────
// HIGH-LEVEL OVERVIEW
//
// The lexer scans left→right and emits Token values, always ending with EOF.
// Each token carries:
//   - Type     — a TokenType enum
//   - Lexeme   — the exact source slice (verbatim characters from the input)
//   - Literal  — the decoded value for literal tokens (e.g., bool/int/float/string)
//   - Line/Col — 1-based line and 0-based column of the token’s *start*
//   - StartByte/EndByte — byte offsets [start, end) for the token’s source span
//
// Whitespace-sensitive delimiters:
//
//	The lexer decides between LROUND/CLROUND (and LSQUARE/CLSQUARE) solely by
//	whether there is immediate whitespace before the delimiter:
//
//	  '('  → LROUND  if there IS preceding whitespace
//	         CLROUND if there is NO preceding whitespace
//	  '['  → LSQUARE if there IS preceding whitespace
//	         CLSQUARE if there is NO preceding whitespace
//
// Consequences (user-facing syntax):
//   - Calls and parameter lists require NO space before '(':
//     f(x)           // call: uses CLROUND
//     fun(x: T)      // function params: uses CLROUND
//     oracle(x: T)   // oracle params: uses CLROUND
//     With a space ("fun (x: T)"), '(' becomes LROUND and is NOT treated as a
//     parameter list; the parser will error.
//   - Indexing requires NO space before '[':
//     arr[i]         // indexing: uses CLSQUARE
//     With a space ("arr [i]"), '[' is LSQUARE and is NOT treated as indexing.
//   - Grouping "(expr)" is produced regardless of LROUND/CLROUND, but only
//     CLROUND participates in call/juxtaposition chains.
//
// The '.' character is context-sensitive:
//   - If it begins a number (e.g., “.5” or “1.” or “1.2e3”), a NUMBER/INTEGER is
//     produced.
//   - Otherwise it is PERIOD (typically for property access).
//
// IDENTIFIERS & KEYWORDS
//
//	Identifiers match [A-Za-z_][A-Za-z0-9_]* and normally produce ID.
//	Reserved words produce dedicated TokenTypes (e.g., IF, LET, FUNCTION, etc.).
//	After a PERIOD, both identifiers *and* quoted strings are treated as
//	property names and forced to ID (even if the text is a keyword). This allows:
//	    obj."then"   // ID with Literal="then"
//	    obj.then     // ID with Literal="then"
//
// LITERALS
//   - STRING — single or double quotes, JSON-style escapes, including \uXXXX with
//     optional UTF-16 surrogate pair handling. Source must be valid UTF-8; non-ASCII
//     bytes in the lexeme are validated and decoded.
//   - INTEGER — 64-bit signed (ParseInt base 10), when no dot/exp part.
//   - NUMBER  — 64-bit float (ParseFloat), for forms with '.' and/or exponent.
//   - BOOLEAN — “true” or “false” (Literal: bool).
//   - NULL    — “null” (Literal: nil).
//
// ANNOTATIONS
//   - Hash-line annotations: one or more consecutive lines where, after optional
//     indentation, the first non-space is '#'. The leading '#' (and at most one
//     optional following space) are stripped; lines are joined with '\n', and a
//     single ANNOTATION token is emitted. A blank/non-# line ends the block.
//
// ERRORS (start-of-token anchoring)
//   - Lexical errors (e.g., bad escape, invalid UTF-8, unexpected character) are
//     reported as *Error* with precise location anchored to the **start** of the
//     offending token (the token that is being scanned).
//   - Interactive/REPL mode: if enabled via NewLexerInteractive, unterminated
//     strings produce *IncompleteError* (still anchored to the token start).
//
// OUTPUT
//   - Scan returns the full token slice *including* the terminal EOF token.
//   - Each token’s Lexeme is the exact source text (e.g., a STRING’s lexeme
//     includes the quotes and escapes), while Literal carries the decoded value.
//
// ──────────────────────────────────────────────────────────────────────────────
//
// FILE ORGANIZATION
//  1. PUBLIC API  — exported enums/types/constructors/methods & their docs.
//  2. PRIVATE     — all non-exported helpers, internal tables, and scanning.
//
// The PUBLIC API docs below are intentionally exhaustive so the behavior is
// understandable without reading the implementation.
//
// ──────────────────────────────────────────────────────────────────────────────
package mindscript

import (
	"errors"
	"fmt"
	"strconv"
	"strings"
	"unicode/utf16"
	"unicode/utf8"
)

////////////////////////////////////////////////////////////////////////////////
//                               PUBLIC API
////////////////////////////////////////////////////////////////////////////////

// TokenType is the enumeration of all token kinds the lexer can emit.
// Most names are self-explanatory; groups are listed for clarity.
//
// Special:
//
//	EOF     — end-of-file sentinel (always the final token)
//	ILLEGAL — produced only for unrecoverable internal conditions (not used by Scan)
//
// Punctuation (some are whitespace-sensitive, see '(' and '[' notes below):
//
//	LROUND, CLROUND   — '(' with/without preceding whitespace respectively
//	RROUND            — ')'
//	LSQUARE, CLSQUARE — '[' with/without preceding whitespace respectively
//	RSQUARE           — ']'
//	LCURLY, RCURLY    — '{', '}'
//	COLON, COMMA, PERIOD, QUESTION — ':', ',', '.', '?'
//
// Operators:
//
//	PLUS, MINUS, MULT, DIV, MOD — '+', '-', '*', '/', '%'
//	ASSIGN                      — '='
//	EQ, NEQ                     — '==', '!='
//	LESS, LESS_EQ               — '<',  '<='
//	GREATER, GREATER_EQ         — '>',  '>='
//	BANG                        — '!' (used by the language in object/type literals)
//	ARROW                       — '->'
//
// Literals & identifiers:
//
//	ID, STRING, INTEGER, NUMBER, BOOLEAN, NULL
//
// Keywords (produced when the identifier text equals these words, except when
// forced to an ID after PERIOD/property access):
//
//	AND, OR, NOT,
//	LET, DO, END, RETURN, BREAK, CONTINUE,
//	IF, THEN, ELIF, ELSE,
//	FUNCTION, ORACLE, MODULE,
//	FOR, IN, FROM, WHILE,
//	TYPECONS, TYPE, ENUM
//
// Annotation:
//
//	ANNOTATION — emitted for multi-line blocks starting with '#'.
type TokenType int

const (
	// Special
	EOF TokenType = iota
	ILLEGAL

	// Punctuation
	LROUND   // "(" when preceded by whitespace
	CLROUND  // "(" when not preceded by whitespace (juxtaposition/call form)
	RROUND   // ")"
	LSQUARE  // "["
	CLSQUARE // "[" when not preceded by whitespace (index-close)
	RSQUARE  // "]"
	LCURLY   // "{"
	RCURLY   // "}"
	COLON    // ":"
	COMMA    // ","
	PERIOD   // "."
	QUESTION // "?"

	// Operators
	PLUS
	MINUS
	MULT
	DIV
	MOD
	ASSIGN // "="
	EQ     // "=="
	NEQ    // "!="
	LESS
	LESS_EQ
	GREATER
	GREATER_EQ
	BANG  // "!" (required-field marker in object/type literals)
	ARROW // "->"

	// Literals & identifiers
	ID
	STRING
	INTEGER
	NUMBER
	BOOLEAN
	NULL

	// Keywords
	AND
	OR
	NOT
	LET
	DO
	END
	RETURN
	BREAK
	CONTINUE
	IF
	THEN
	ELIF
	ELSE
	FUNCTION
	ORACLE
	MODULE
	FOR
	IN
	FROM
	WHILE
	TYPECONS
	TYPE
	ENUM

	// Annotation token (from lines starting with '#')
	ANNOTATION
	NOOP
)

// Token is a single lexical unit produced by the lexer.
//
// Fields:
//
//	Type    — the TokenType kind.
//	Lexeme  — the exact source slice comprising the token (verbatim, including
//	          quotes for strings, escape sequences, etc.).
//	Literal — a decoded value for literal tokens:
//	          • STRING  → Go string with escapes and surrogate pairs resolved
//	          • INTEGER → int64
//	          • NUMBER  → float64
//	          • BOOLEAN → bool
//	          • NULL    → nil
//	          Non-literal tokens usually carry nil or an unmodified string
//	          (keywords may store their text; property IDs store the property name).
//	Line    — 1-based line number at which this token starts.
//	Col     — 0-based column index at which this token starts.
//	StartByte / EndByte — byte offsets [start, end) for the token’s slice.
type Token struct {
	Type      TokenType
	Lexeme    string
	Literal   interface{}
	Line      int
	Col       int
	StartByte int
	EndByte   int
}

// HARD errors produced by the lexer use the unified *Error type defined in
// errors.go. See DiagLex and DiagIncomplete.
//
// In interactive mode, the lexer returns *Error with Kind=DiagIncomplete
// when input ends inside an unterminated construct (e.g., string literal).
//
// **Error locations:** All lexing errors (both hard and incomplete) are anchored
// to the **start of the offending token** (the token whose scanning raised
// the error). This makes diagnostics stable and easy to map to spans.

// Lexer is a streaming tokenizer for MindScript.
//
// Construction:
//   - NewLexer(src)            — normal mode. Unterminated constructs produce LexError.
//   - NewLexerInteractive(src) — REPL-friendly mode. Unterminated constructs
//     produce IncompleteError.
//
// Semantics:
//   - Scan() returns the full token slice including EOF. It never panics for
//     malformed input; instead it returns (nil, error).
//   - Whitespace is skipped, but influences '(' and '[' classification (see TokenType docs).
//   - PERIOD vs number: a '.' followed by digits begins a number IFF there is
//     either preceding whitespace *or* the previous token cannot be a left operand.
//     Otherwise '.' is PERIOD used for property access.
//   - After PERIOD, the *next* identifier or quoted string is forced to ID,
//     even if it matches a keyword.
//
// Positioning:
//   - Line numbers are 1-based; column indices are 0-based.
//   - A token’s position is captured at the start of scanning that token.
type Lexer struct {
	// public type with no exported fields; use constructors + Scan()
	src    string
	start  int // start index of current token
	cur    int // current index
	line   int // 1-based
	col    int // 0-based column within line
	tokens []Token

	// precise token start position
	tokStartLine int
	tokStartCol  int

	// interactive mode: produce IncompleteError for unterminated constructs at EOF
	interactive bool
}

// NewLexer creates a new lexer for the given source in normal mode.
func NewLexer(src string) *Lexer {
	return &Lexer{
		src: src,
		// very rough guess to reduce reslices on big files
		tokens: make([]Token, 0, len(src)/4),
		line:   1,
		col:    0,
	}
}

// NewLexerInteractive creates a lexer in interactive mode.
// Unterminated strings return IncompleteError at EOF, allowing REPLs to request more input.
func NewLexerInteractive(src string) *Lexer {
	return &Lexer{
		src:         src,
		tokens:      make([]Token, 0, len(src)/4),
		line:        1,
		col:         0,
		interactive: true,
	}
}

// Scan tokenizes the entire source string and returns the resulting slice of
// tokens. The returned slice always ends with EOF. On error, it returns (nil, err).
//
// Error behavior summary:
//   - Normal mode: returns *LexError on malformed input or unterminated constructs.
//   - Interactive mode: returns *IncompleteError at EOF if a construct is
//     unterminated; other issues still return *LexError.
//
// Note: Token.Lexeme is the exact source span; Token.Literal contains decoded
// values for STRING/INTEGER/NUMBER/BOOLEAN/NULL as described in Token docs.
func (l *Lexer) Scan() ([]Token, error) {
	for {
		tok, err := l.scanToken()
		if err != nil {
			return nil, err
		}
		if tok.Type == EOF {
			return l.tokens, nil
		}
	}
}

=== END PUBLIC SECTION FOR FILE: lexer.go ===

=== BEGIN PUBLIC SECTION OF FILE: parser.go ===
// parser.go — Pratt parser for MindScript that produces compact S-expressions.
//
// OVERVIEW
// --------
// This module implements the newline-aware Pratt parser for the MindScript
// language. It consumes the token stream produced by the *whitespace-sensitive*
// lexer (see lexer.go) and builds a compact, Lisp-style S-expression (AST).
//
// Design goals:
//   - Keep the grammar readable via precedence rules (Pratt parser).
//   - Encode the AST in a tiny, serialisable structure (S-expressions).
//   - Respect whitespace-sensitive signals emitted by the lexer:
//   - '(' can be LROUND or CLROUND; only CLROUND participates in calls.
//   - '[' can be LSQUARE or CLSQUARE; only CLSQUARE participates in indexing.
//   - '.' is PERIOD unless it started a number in the lexer.
//   - multi-line '#' annotations become ANNOTATION tokens.
//   - blank-line runs may be emitted as NOOP tokens.
//   - Support an "interactive" mode that surfaces *Error{Kind:DiagIncomplete}
//     at EOF instead of hard parse errors, suitable for REPLs.
//
// NEW IN THIS VERSION (liberal spacing inside delimiters + precise error locations)
// -------------------------------------------------------------------------------
//
//   - The parser **skips NOOP tokens inside delimited constructs** so that blank
//     lines are treated like whitespace within:
//     array literals:            [ … ]
//     map/object literals:       { … }
//     call argument lists:       f( … )
//     parameter lists:           fun(x: T, …)
//     bracket indices:           a[ … ]
//     computed properties:       obj.( … )
//     grouping parentheses:      ( … )
//
//     The semantics of NOOP at the top level and inside blocks are unchanged: a
//     NOOP still parses as ("noop") outside the above delimited contexts.
//
//   - **Precise error locations:** the parser now uses lexer byte offsets to place
//     carets exactly where users expect.
//
//   - If a token is **missing** (e.g., expected ')' at EOF), the error is
//     anchored **immediately after the last completed node** (using EndByte).
//
//   - If an **unexpected token** is present (wrong lookahead), the error is
//     anchored at the **start** of that token (using StartByte).
//
//   - Interactive incomplete errors follow the same policy.
//
// What the parser returns
// -----------------------
// The AST is a tree of S-expressions: []any whose first element is a string tag.
// Examples (non-exhaustive):
//
//		("block", n1, n2, ...)
//		("id",   name)               // string
//		("int",  int64)              // from INTEGER
//		("num",  float64)            // from NUMBER
//		("str",  string)             // decoded literal
//		("bool", bool)               // from BOOLEAN
//		("null")                     // from NULL
//
//		("unop", op, rhs)            // prefix "-" or "not"; postfix "?"  (op is string)
//		("binop", op, lhs, rhs)      // "+", "-", "*", "/", "%", comparisons, "==", "!=", "and", "or"
//		("assign", target, value)    // "=" (right-assoc)
//
//		("call", callee, arg1, arg2, ...)
//		("get",  obj, ("str", name))                  // property: obj.name or obj."name"
//		("idx",  obj, indexExpr)                      // obj[expr] or obj.(expr) or obj.12
//
//		("array", e1, e2, ...)
//		("map",   ("pair",  keyStrExpr, value)*)
//		("map",   ("pair!", keyStrExpr, value)*)      // required-field (key! : value)
//		("enum",  item1, item2, ...)                  // from Enum[ ... ]
//
//		("fun",     paramsArray, retTypeExprOrAny, bodyBlock)
//		("oracle",  paramsArray, outTypeExprOrAny, sourceExpr)  // optional 'from' expression
//		("module",  nameExpr, bodyBlock)             // module NAME do ... end
//
//		("if", ("pair", cond1, thenBlk1), ..., elseBlk?)       // if/elif/else
//		("while", cond, bodyBlock)
//		("for",   targetPatternOrLvalue, iterExpr, bodyBlock)
//
//		// Declaration patterns (used by 'let' and 'for' targets):
//		("decl", name)
//		("darr", p1, p2, ...)                               // array destructure
//		("dobj", ("pair", keyStrExpr, subPattern), ...)     // object destructure
//
//		// Annotations (from '#'-blocks). POST is encoded by a leading "<" in text.
//		("annot", ("str", textOr<text>), wrappedNode)
//
//	 Annotation encoding:
//	   - PRE annotations are stored as-is:        ("annot", ("str", "note"), expr/pattern)
//	   - POST annotations are prefixed with "<":  ("annot", ("str", "<note"), expr)
//
// Node spans
// ----------
// For tooling, the parser records byte spans (StartByte/EndByte) for each node
// in post-order. Use ParseSExprWithSpans to receive a sidecar *SpanIndex that
// maps each node to its original source span (see spans.go).
//
// **Implementation note (this file):**
// Spans are emitted *at construction sites* for every AST node, including
// intermediate postfix nodes like ("get") and ("idx"), computed-index grouping
// via obj.( … ), structural containers like ("pair")/("pair!"), parameter
// pairs, "decl" patterns, and PRE-annotation wrappers in pattern/key contexts.
// Children are emitted first; when a parent node is formed, its span is
// appended using the earliest token of the left-hand side and the last token
// consumed for the construct, preserving post-order.
//
// Dependencies
// ------------
//   - lexer.go
//   - NewLexer / NewLexerInteractive
//   - Token / TokenType definitions and the tokenization rules
//   - *Error (unified hard diagnostic) with DiagKind; the parser may return
//     *Error{Kind:DiagIncomplete} in interactive mode.
//   - spans.go
//   - type Span, type SpanIndex
//   - func BuildSpanIndexPostOrder(ast S, spans []Span) *SpanIndex
//
// Whitespace-sensitive behavior consumed from the lexer
// -----------------------------------------------------
//
//   - CLROUND vs LROUND:
//
//   - Prefix  (both) parse a parenthesised *grouping*.
//
//   - Postfix only CLROUND starts a *call* chain: f(x) → ("call", ...).
//     `f (x)` uses LROUND and is *not* a call.
//
//   - CLSQUARE vs LSQUARE:
//
//   - Prefix  (both) parse an array literal.
//
//   - Postfix only CLSQUARE is indexing: a[i]. `a [i]` is not indexing.
//
//   - PERIOD chains:
//
//   - After '.', the next token is coerced by the lexer to an ID if it is an
//     identifier or a quoted string, even if it lexically matches a keyword.
//     The parser accepts after '.' one of:
//     (a) LROUND/CLROUND expr RROUND   → computed property: ("idx", obj, expr)
//     (b) INTEGER                      → numeric index:    ("idx", obj, ("int", ...))
//     (c) ID or STRING                 → property:         ("get", obj, ("str", name))
//     Anything else is a parse error.
//
//   - Annotations (ANNOTATION tokens):
//
//   - Classification into PRE vs POST is *line-sensitive*:
//     If the last token on the same line before '#' can end an expression,
//     the annotation is POST (attaches to the left); otherwise it is PRE.
//
//   - General rule: multiple *consecutive PRE* annotations are disallowed
//     (parser error; suggest combining). Inside key parsing, recursion allows
//     stacked PRE annotations in front of a key.
//
//   - POST annotations are attached in the postfix chain; PRE wrap the node
//     that follows. In the AST, POST is represented by a leading "<" in the
//     stored text (see encoding above).
//
//   - NOOP (blank lines):
//     Inside delimiters (() [] {} args/params/indices/grouping): skipped like
//     whitespace. Elsewhere (top level, block bodies): parsed as ("noop").
//     Lone PRE annotation followed by a blank line:
//     ("annot", ("str", text), ("noop"), true),
//     the interpreter treats these as true no-ops; see interpreter.go.
//
// Grammar sketch (informal)
// -------------------------
//
//	program      := expr* EOF
//	expr         := prefix (postfix | infix)*
//	prefix       := literals | ids | grouping | arrays | maps | enums
//	                | unary ("-" | "not") expr
//	                | "fun"    params ["->" type] block
//	                | "oracle" params ["->" type] ["from" expr] block
//	                | "module" expr "do" block "end"
//	                | "if" cond "then" block {"elif" cond "then" block} ["else" block] "end"
//	                | "do" block "end"
//	                | "for" forTarget "in" expr block
//	                | "while" expr block
//	                | "let" declPattern
//	                | annotation (PRE) expr
//	postfix      := "?" | call | index | dot | annotation (POST)
//	call         := CLROUND [args] RROUND
//	index        := CLSQUARE expr RSQUARE
//	dot          := PERIOD ( LROUND expr RROUND | INTEGER | ID | STRING )
//	infix        := right-assoc "=" | right-assoc "->" | precedence-based binary op
//
//	declPattern  := ID | "[" [declPattern {"," declPattern}] "]"
//	                     | "{" [key ":" declPattern {"," key ":" declPattern}] "}"
//	forTarget    := ["let"] (declPattern | assignable)
//	params       := CLROUND [param {"," param}] RROUND
//	param        := ID [":" typeExpr]
//	block        := "do" blockBody "end"    // in forms that require it
//	blockBody    := expr*                    // until a listed keyword (e.g. "end")
//
// Precedence & associativity
// --------------------------
//
//	Highest …  unary ("-", "not"), postfix '?'
//	70         "*" "/" "%"
//	60         "+" "-"
//	50         "<" "<=" ">" ">="
//	40         "==" "!="
//	30         "and"
//	20         "or"
//	15         "->"     (right-assoc as general operator; also used in fun/oracle headers)
//	10         "="      (right-assoc; target must be id/get/idx/decl/darr/dobj)
//
// Errors
// ------
//   - HARD errors are returned as *Error with appropriate Kind:
//   - DiagParse for grammatical mistakes.
//   - DiagIncomplete at EOF in interactive mode (REPL continue).
//   - **Error placement (this version):**
//   - Missing token (e.g. expected ')'): caret after the last completed node.
//   - Wrong lookahead token: caret at the start of that token.
//   - The parser never pretty-prints; public entry points format via FormatError.
//   - The parser never panics for malformed input.
//
// PUBLIC API
// ----------
=== END PUBLIC SECTION FOR FILE: parser.go ===

