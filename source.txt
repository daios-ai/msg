=== BEGIN FILE: lexer.go ===
// lexer.go: provides a whitespace-sensitive, UTF-8–aware lexer for the
// MindScript language. It converts a source string into a linear stream of
// tokens with accurate source positions and rich literal decoding.
//
// ──────────────────────────────────────────────────────────────────────────────
// HIGH-LEVEL OVERVIEW
//
// The lexer scans left→right and emits Token values, always ending with EOF.
// Each token carries:
//   - Type    — a TokenType enum
//   - Lexeme  — the exact source slice (verbatim characters from the input)
//   - Literal — the decoded value for literal tokens (e.g., bool/int/float/string)
//   - Line/Col— 1-based line and 0-based column of the token’s start
//
// The lexer decides between LROUND/CLROUND (and LSQUARE/CLSQUARE) solely by
// whether there is immediate whitespace before the delimiter:
//
//	'('  → LROUND  if there IS preceding whitespace
//	      CLROUND if there is NO preceding whitespace
//	'['  → LSQUARE if there IS preceding whitespace
//	      CLSQUARE if there is NO preceding whitespace
//
// Consequences (user-facing syntax):
//
//   - Calls and parameter lists require NO space before '('.
//     f(x)           // call: uses CLROUND
//     fun(x: T)      // function params: uses CLROUND
//     oracle(x: T)   // oracle params: uses CLROUND
//     With a space ("fun (x: T)"), '(' becomes LROUND and is NOT treated as a
//     parameter list; the parser will error.
//
//   - Indexing requires NO space before '['.
//     arr[i]         // indexing: uses CLSQUARE
//     With a space ("arr [i]"), '[' is LSQUARE and is NOT treated as indexing.
//
//   - Grouping "(expr)" is produced regardless of LROUND/CLROUND, but only
//     CLROUND participates in call/juxtaposition chains.
//
// This lets the parser distinguish grouping/indexing from juxtaposition/call-like
// forms without lookbehind in the parser.
//
// The '.' character is context-sensitive:
//   - If it begins a number (e.g., “.5” or “1.” or “1.2e3”), a NUMBER/INTEGER is
//     produced.
//   - Otherwise it is PERIOD (typically for property access).
//
// IDENTIFIERS & KEYWORDS
//
//	Identifiers match [A-Za-z_][A-Za-z0-9_]* and normally produce ID.
//	Reserved words produce dedicated TokenTypes (e.g., IF, LET, FUNCTION, etc.).
//	After a PERIOD, both identifiers *and* quoted strings are treated as
//	property names and forced to ID (even if the text is a keyword). This allows:
//	    obj."then"   // ID with Literal="then"
//	    obj.then     // ID with Literal="then"
//
// LITERALS
//   - STRING — single or double quotes, JSON-style escapes, including \uXXXX with
//     optional UTF-16 surrogate pair handling. Source must be valid UTF-8; non-ASCII
//     bytes in the lexeme are validated and decoded.
//   - INTEGER — 64-bit signed (ParseInt base 10), when no dot/exp part.
//   - NUMBER  — 64-bit float (ParseFloat), for forms with '.' and/or exponent.
//   - BOOLEAN — “true” or “false” (Literal: bool).
//   - NULL    — “null” (Literal: nil).
//
// COMMENTS vs ANNOTATIONS (hash syntax)
//   - Line comment:       "## ... <newline>"           → ignored
//   - Inline comment:     "##( ... )" (no nesting)     → ignored
//   - Inline annotation:  "#( ... )"                   → emits ANNOTATION with text
//   - Block annotations:  one or more consecutive lines where, after optional
//     indentation, the first non-space is '#'. The leading '#' (and one optional
//     following space/tab) are stripped; lines are joined with '\n', and a single
//     ANNOTATION token is emitted. A blank/non-# line ends the block.
//
// ERRORS
//   - Lexical errors (e.g., bad escape, invalid UTF-8, unexpected character) are
//     reported as *LexError* with precise location.
//   - Interactive/REPL mode: if enabled via NewLexerInteractive, unterminated
//     strings or unterminated "#(...)" / "##(...)" inline blocks produce
//     *IncompleteError* instead of LexError. Use IsIncomplete(err) to detect this
//     and prompt for more input.
//
// OUTPUT
//   - Scan returns the full token slice *including* the terminal EOF token.
//   - Each token’s Lexeme is the exact source text (e.g., a STRING’s lexeme
//     includes the quotes and escapes), while Literal carries the decoded value.
//
// ──────────────────────────────────────────────────────────────────────────────
//
// FILE ORGANIZATION
//  1. PUBLIC API  — exported enums/types/constructors/methods & their docs.
//  2. PRIVATE     — all non-exported helpers, internal tables, and scanning.
//
// The PUBLIC API docs below are intentionally exhaustive so the behavior is
// understandable without reading the implementation.
//
// ──────────────────────────────────────────────────────────────────────────────
package mindscript

import (
	"errors"
	"fmt"
	"strconv"
	"strings"
	"unicode/utf16"
	"unicode/utf8"
)

////////////////////////////////////////////////////////////////////////////////
//                               PUBLIC API
////////////////////////////////////////////////////////////////////////////////

// TokenType is the enumeration of all token kinds the lexer can emit.
// Most names are self-explanatory; groups are listed for clarity.
//
// Special:
//
//	EOF     — end-of-file sentinel (always the final token)
//	ILLEGAL — produced only for unrecoverable internal conditions (not used by Scan)
//
// Punctuation (some are whitespace-sensitive, see '(' and '[' notes below):
//
//	LROUND, CLROUND   — '(' with/without preceding whitespace respectively
//	RROUND            — ')'
//	LSQUARE, CLSQUARE — '[' with/without preceding whitespace respectively
//	RSQUARE           — ']'
//	LCURLY, RCURLY    — '{', '}'
//	COLON, COMMA, PERIOD, QUESTION — ':', ',', '.', '?'
//
// Operators:
//
//	PLUS, MINUS, MULT, DIV, MOD — '+', '-', '*', '/', '%'
//	ASSIGN                      — '='
//	EQ, NEQ                     — '==', '!='
//	LESS, LESS_EQ               — '<',  '<='
//	GREATER, GREATER_EQ         — '>',  '>='
//	BANG                        — '!' (used by the language in object/type literals)
//	ARROW                       — '->'
//
// Literals & identifiers:
//
//	ID, STRING, INTEGER, NUMBER, BOOLEAN, NULL
//
// Keywords (produced when the identifier text equals these words, except when
// forced to an ID after PERIOD/property access):
//
//	AND, OR, NOT,
//	LET, DO, END, RETURN, BREAK, CONTINUE,
//	IF, THEN, ELIF, ELSE,
//	FUNCTION, ORACLE,
//	FOR, IN, FROM, WHILE,
//	TYPECONS, TYPE, ENUM
//
// Annotation:
//
//	ANNOTATION — emitted for "#( … )" lines or multi-line blocks starting with '#'.
type TokenType int

const (
	// Special
	EOF TokenType = iota
	ILLEGAL

	// Punctuation
	LROUND   // "(" when preceded by whitespace
	CLROUND  // "(" when not preceded by whitespace (juxtaposition/call form)
	RROUND   // ")"
	LSQUARE  // "["
	CLSQUARE // "[" when not preceded by whitespace (index-close)
	RSQUARE  // "]"
	LCURLY   // "{"
	RCURLY   // "}"
	COLON    // ":"
	COMMA    // ","
	PERIOD   // "."
	QUESTION // "?"

	// Operators
	PLUS
	MINUS
	MULT
	DIV
	MOD
	ASSIGN // "="
	EQ     // "=="
	NEQ    // "!="
	LESS
	LESS_EQ
	GREATER
	GREATER_EQ
	BANG  // "!" (required-field marker in object/type literals)
	ARROW // "->"

	// Literals & identifiers
	ID
	STRING
	INTEGER
	NUMBER
	BOOLEAN
	NULL

	// Keywords
	AND
	OR
	NOT
	LET
	DO
	END
	RETURN
	BREAK
	CONTINUE
	IF
	THEN
	ELIF
	ELSE
	FUNCTION
	ORACLE
	FOR
	IN
	FROM
	WHILE
	TYPECONS
	TYPE
	ENUM

	// Annotation token (from lines starting with '#')
	ANNOTATION
)

// Token is a single lexical unit produced by the lexer.
//
// Fields:
//
//	Type    — the TokenType kind.
//	Lexeme  — the exact source slice comprising the token (verbatim, including
//	          quotes for strings, escape sequences, etc.).
//	Literal — a decoded value for literal tokens:
//	          • STRING  → Go string with escapes and surrogate pairs resolved
//	          • INTEGER → int64
//	          • NUMBER  → float64
//	          • BOOLEAN → bool
//	          • NULL    → nil
//	          Non-literal tokens usually carry nil or an unmodified string
//	          (keywords may store their text; property IDs store the property name).
//	Line    — 1-based line number at which this token starts.
//	Col     — 0-based column index at which this token starts.
type Token struct {
	Type      TokenType
	Lexeme    string
	Literal   interface{}
	Line      int
	Col       int
	StartByte int
	EndByte   int
}

// LexError reports a lexical error detected during scanning (e.g., invalid
// escape sequence, malformed number, unexpected character, invalid UTF-8).
// In non-interactive mode, unterminated strings or inline "#(...)" / "##(...)"
// also produce LexError.
//
// The error position (Line, Col) refers to the location where the lexer
// detected the problem (generally close to the token’s start).
type LexError struct {
	Line int
	Col  int
	Msg  string
}

func (e *LexError) Error() string {
	return fmt.Sprintf("LEXICAL ERROR at %d:%d: %s", e.Line, e.Col, e.Msg)
}

// IncompleteError signals that more input is required to complete a construct.
// It is returned *only* by a lexer created with NewLexerInteractive when the
// end of input is reached inside:
//   - a string literal,
//   - an inline annotation "#( ... )", or
//   - an inline comment   "##( ... )".
//
// Use IsIncomplete(err) to detect this case in REPLs and prompt the user for
// more lines instead of failing the parse.
type IncompleteError struct {
	Line int
	Col  int
	Msg  string
}

func (e *IncompleteError) Error() string {
	return fmt.Sprintf("INCOMPLETE at %d:%d: %s", e.Line, e.Col, e.Msg)
}

// IsIncomplete reports whether err is an *IncompleteError. Helpful in REPLs
// to distinguish “need more input” from real lexical errors.
func IsIncomplete(err error) bool {
	_, ok := err.(*IncompleteError)
	return ok
}

// Lexer is a streaming tokenizer for MindScript.
//
// Construction:
//   - NewLexer(src)            — normal mode. Unterminated constructs produce LexError.
//   - NewLexerInteractive(src) — REPL-friendly mode. Unterminated constructs
//     produce IncompleteError.
//
// Semantics:
//   - Scan() returns the full token slice including EOF. It never panics for
//     malformed input; instead it returns (nil, error).
//   - Whitespace is skipped, but influences '(' and '[' classification (see TokenType docs).
//   - PERIOD vs number: a '.' followed by digits begins a number IFF there is
//     either preceding whitespace *or* the previous token cannot be a left operand.
//     Otherwise '.' is PERIOD used for property access.
//   - After PERIOD, the *next* identifier or quoted string is forced to ID,
//     even if it matches a keyword.
//
// Positioning:
//   - Line numbers are 1-based; column indices are 0-based.
//   - A token’s position is captured at the start of scanning that token.
type Lexer struct {
	// public type with no exported fields; use constructors + Scan()
	src              string
	start            int // start index of current token
	cur              int // current index
	line             int // 1-based
	col              int // 0-based column within line
	tokens           []Token
	whitespaceBefore bool

	// precise token start position
	tokStartLine int
	tokStartCol  int

	// interactive mode: produce IncompleteError for unterminated constructs at EOF
	interactive bool
}

// NewLexer creates a new lexer for the given source in normal mode.
// Unterminated strings / "#(...)" / "##(...)" yield LexError.
func NewLexer(src string) *Lexer {
	return &Lexer{
		src:  src,
		line: 1,
		col:  0,
	}
}

// NewLexerInteractive creates a lexer in interactive mode.
// Unterminated strings or inline paren blocks return IncompleteError at EOF,
// allowing REPLs to request more input.
func NewLexerInteractive(src string) *Lexer {
	return &Lexer{
		src:         src,
		line:        1,
		col:         0,
		interactive: true,
	}
}

// Scan tokenizes the entire source string and returns the resulting slice of
// tokens. The returned slice always ends with EOF. On error, it returns (nil, err).
//
// Error behavior summary:
//   - Normal mode: returns *LexError on malformed input or unterminated constructs.
//   - Interactive mode: returns *IncompleteError at EOF if a construct is
//     unterminated; other issues still return *LexError.
//
// Note: Token.Lexeme is the exact source span; Token.Literal contains decoded
// values for STRING/INTEGER/NUMBER/BOOLEAN/NULL as described in Token docs.
func (l *Lexer) Scan() ([]Token, error) {
	for {
		tok, err := l.scanToken()
		if err != nil {
			return nil, err
		}
		if tok.Type == EOF {
			return l.tokens, nil
		}
	}
}

//// END_OF_PUBLIC

////////////////////////////////////////////////////////////////////////////////
//                            PRIVATE IMPLEMENTATION
////////////////////////////////////////////////////////////////////////////////

// ---------------- keywords map (private) ----------------

var keywords = map[string]TokenType{
	"null":     NULL,
	"false":    BOOLEAN,
	"true":     BOOLEAN,
	"and":      AND,
	"or":       OR,
	"not":      NOT,
	"let":      LET,
	"do":       DO,
	"end":      END,
	"return":   RETURN,
	"break":    BREAK,
	"continue": CONTINUE,
	"if":       IF,
	"then":     THEN,
	"elif":     ELIF,
	"else":     ELSE,
	"fun":      FUNCTION,
	"oracle":   ORACLE,
	"for":      FOR,
	"in":       IN,
	"from":     FROM,
	"while":    WHILE,
	"type":     TYPECONS,
	"Type":     TYPE,
	"Null":     TYPE,
	"Str":      TYPE,
	"Int":      TYPE,
	"Num":      TYPE,
	"Bool":     TYPE,
	"Any":      TYPE,
	"Enum":     ENUM,
}

// ---------------- core scanning helpers ----------------

func (l *Lexer) isAtEnd() bool { return l.cur >= len(l.src) }

func (l *Lexer) peek() (byte, bool) {
	if l.isAtEnd() {
		return 0, false
	}
	return l.src[l.cur], true
}

func (l *Lexer) peekN(n int) (byte, bool) {
	idx := l.cur + n
	if idx >= len(l.src) {
		return 0, false
	}
	return l.src[idx], true
}

func (l *Lexer) advance() (byte, bool) {
	if l.isAtEnd() {
		return 0, false
	}
	ch := l.src[l.cur]
	l.cur++
	if ch == '\n' {
		l.line++
		l.col = 0
	} else {
		l.col++
	}
	return ch, true
}

func (l *Lexer) rewindToStart() {
	// We rewind only within the bounds of the current token start; line/col are kept
	// for error arrows (OK since we set tokStartLine/Col before scanning).
	l.cur = l.start
}

func (l *Lexer) addToken(tt TokenType, lit interface{}) Token {
	lex := l.src[l.start:l.cur]
	tok := Token{
		Type:      tt,
		Lexeme:    lex,
		Literal:   lit,
		Line:      l.tokStartLine,
		Col:       l.tokStartCol,
		StartByte: l.start,
		EndByte:   l.cur,
	}
	l.tokens = append(l.tokens, tok)
	l.start = l.cur
	l.whitespaceBefore = false
	return tok
}

func (l *Lexer) previousToken() *Token {
	if len(l.tokens) == 0 {
		return nil
	}
	return &l.tokens[len(l.tokens)-1]
}

func (l *Lexer) skipWhitespace() {
	l.whitespaceBefore = false
	for !l.isAtEnd() {
		ch, _ := l.peek()
		switch ch {
		case ' ', '\r', '\n', '\t':
			l.whitespaceBefore = true
			l.advance()
			l.start = l.cur
		default:
			return
		}
	}
}

// ---------------- small predicates ----------------

func canBeLeftOperand(t TokenType) bool {
	switch t {
	case ID, STRING, INTEGER, NUMBER, BOOLEAN, NULL,
		TYPE, ENUM,
		RROUND, RSQUARE, RCURLY,
		QUESTION:
		return true
	default:
		return false
	}
}

func isDigit(b byte) bool { return b >= '0' && b <= '9' }
func isHex(b byte) bool {
	return (b >= '0' && b <= '9') || (b >= 'a' && b <= 'f') || (b >= 'A' && b <= 'F')
}
func isAlpha(b byte) bool { return (b >= 'a' && b <= 'z') || (b >= 'A' && b <= 'Z') || b == '_' }
func isAlphaNum(b byte) bool {
	return (b >= 'a' && b <= 'z') ||
		(b >= 'A' && b <= 'Z') ||
		(b >= '0' && b <= '9') ||
		b == '_'
}

func (l *Lexer) afterDotIsProperty() bool {
	p := l.previousToken()
	return p != nil && p.Type == PERIOD
}

// ---------------- error builders ----------------

func (l *Lexer) err(msg string) error {
	return &LexError{Line: l.line, Col: l.col, Msg: msg}
}

func (l *Lexer) errIncomplete(msg string) error {
	return &IncompleteError{Line: l.line, Col: l.col, Msg: msg}
}

// ---------------- scanners ----------------

// scanString parses a JSON-style string literal (single or double quotes).
func (l *Lexer) scanString() (string, error) {
	del := l.src[l.start]
	if del != '"' && del != '\'' {
		return "", l.err("internal: scanString without quote")
	}
	// consume the delimiter
	l.advance()

	var out []rune
	for !l.isAtEnd() {
		ch, _ := l.advance()
		if ch == byte(del) {
			return string(out), nil
		}
		if ch == '\\' {
			if l.isAtEnd() {
				if l.interactive {
					return "", l.errIncomplete("unfinished escape sequence")
				}
				return "", l.err("unfinished escape sequence")
			}
			esc, _ := l.advance()
			switch esc {
			case '"':
				out = append(out, '"')
			case '\'':
				out = append(out, '\'')
			case '\\':
				out = append(out, '\\')
			case '/':
				out = append(out, '/')
			case 'b':
				out = append(out, '\b')
			case 'f':
				out = append(out, '\f')
			case 'n':
				out = append(out, '\n')
			case 'r':
				out = append(out, '\r')
			case 't':
				out = append(out, '\t')
			case 'u':
				// expect 4 hex digits
				var hex string
				for i := 0; i < 4; i++ {
					b, ok := l.peek()
					if !ok || !isHex(b) {
						if l.interactive {
							return "", l.errIncomplete("unicode escape was not terminated (expect 4 hex digits)")
						}
						return "", l.err("unicode escape was not terminated (expect 4 hex digits)")
					}
					hex += string(b)
					l.advance()
				}
				v, err := strconv.ParseInt(hex, 16, 32)
				if err != nil {
					return "", l.err("invalid unicode escape")
				}
				r := rune(v)

				// handle surrogate pair \uD800-\uDBFF followed by \uDC00-\uDFFF
				if 0xD800 <= r && r <= 0xDBFF {
					saveCur := l.cur
					saveLine, saveCol := l.line, l.col
					if b1, ok := l.peek(); ok && b1 == '\\' {
						l.advance()
						if b2, ok := l.peek(); ok && b2 == 'u' {
							l.advance()
							var hex2 string
							for i := 0; i < 4; i++ {
								b, ok := l.peek()
								if !ok || !isHex(b) {
									if l.interactive {
										return "", l.errIncomplete("unicode surrogate pair low was not terminated")
									}
									return "", l.err("unicode surrogate pair low was not terminated")
								}
								hex2 += string(b)
								l.advance()
							}
							v2, err := strconv.ParseInt(hex2, 16, 32)
							if err != nil {
								return "", l.err("invalid unicode surrogate pair low")
							}
							r2 := rune(v2)
							if 0xDC00 <= r2 && r2 <= 0xDFFF {
								cp := utf16.DecodeRune(r, r2)
								out = append(out, cp)
								continue
							}
						}
					}
					// not a valid pair; rewind and just emit r
					l.cur = saveCur
					l.line, l.col = saveLine, saveCol
				}
				out = append(out, r)
			default:
				return "", l.err(fmt.Sprintf("invalid escape sequence: \\%c", esc))
			}
			continue
		}
		// normal char; ensure it’s valid UTF-8 boundary (source may be UTF-8)
		if ch < utf8.RuneSelf {
			out = append(out, rune(ch))
			continue
		}
		// If we see a non-ASCII byte here, the source itself is UTF-8; back up one byte and decode rune.
		l.cur-- // step back 1 byte to let utf8.DecodeRuneInString read from correct start
		r, size := utf8.DecodeRuneInString(l.src[l.cur:])
		if r == utf8.RuneError && size == 1 {
			return "", l.err("invalid UTF-8 in source")
		}
		out = append(out, r)
		l.cur += size
		l.col += size - 1
	}
	if l.interactive {
		return "", l.errIncomplete("string was not terminated")
	}
	return "", l.err("string was not terminated")
}

// scanIdentifier parses [A-Za-z_][A-Za-z0-9_]*
func (l *Lexer) scanIdentifier() string {
	for {
		b, ok := l.peek()
		if !ok || !isAlphaNum(b) {
			break
		}
		l.advance()
	}
	return l.src[l.start:l.cur]
}

// scanNumber parses integer or float; supports .5, 1., 1.23e-4, etc.
func (l *Lexer) scanNumber() (tok TokenType, lit interface{}, err error) {
	sawDigits := false
	// optional leading digits
	for {
		b, ok := l.peek()
		if !ok || !isDigit(b) {
			break
		}
		l.advance()
		sawDigits = true
	}

	// decimal point with optional digits
	sawDot := false
	if b, ok := l.peek(); ok && b == '.' {
		if sawDigits {
			l.advance() // consume '.'
			sawDot = true
			for {
				b, ok := l.peek()
				if !ok || !isDigit(b) {
					break
				}
				l.advance()
			}
		} else if b2, ok2 := l.peekN(1); ok2 && isDigit(b2) {
			l.advance() // consume '.'
			sawDot = true
			for {
				b, ok := l.peek()
				if !ok || !isDigit(b) {
					break
				}
				l.advance()
			}
			sawDigits = true
		}
	}

	// exponent
	sawExp := false
	if b, ok := l.peek(); ok && (b == 'e' || b == 'E') {
		save := l.cur
		l.advance()
		if b2, ok := l.peek(); ok && (b2 == '+' || b2 == '-') {
			l.advance()
		}
		if b3, ok := l.peek(); ok && isDigit(b3) {
			sawExp = true
			for {
				b4, ok := l.peek()
				if !ok || !isDigit(b4) {
					break
				}
				l.advance()
			}
		} else {
			l.cur = save
		}
	}

	if !sawDigits {
		return ILLEGAL, nil, l.err("malformed number")
	}

	lex := l.src[l.start:l.cur]
	if !sawDot && !sawExp {
		v, convErr := strconv.ParseInt(lex, 10, 64)
		if convErr != nil {
			return ILLEGAL, nil, l.err("invalid integer literal")
		}
		return INTEGER, v, nil
	}
	vf, convErr := strconv.ParseFloat(lex, 64)
	if convErr != nil {
		return ILLEGAL, nil, l.err("invalid float literal")
	}
	return NUMBER, vf, nil
}

// ignoreUntilNewline eats until '\n' or EOF.
func (l *Lexer) ignoreUntilNewline() {
	for {
		b, ok := l.peek()
		if !ok || b == '\n' {
			return
		}
		l.advance()
	}
}

// scanAnnotation captures consecutive lines that start with '#' (ignoring leading spaces).
// Terminates on blank line or a line that does not begin (after spaces) with '#'.
func (l *Lexer) scanAnnotation() (string, error) {
	var bldr strings.Builder

	// Consume (indent* '#') with one optional post-# space/tab. Returns true if a line was consumed.
	consumeHashOnLine := func() (bool, error) {
		// skip leading spaces/tabs (not newline)
		for {
			b, ok := l.peek()
			if !ok || b == '\n' {
				break
			}
			if b == ' ' || b == '\t' || b == '\r' {
				l.advance()
				continue
			}
			break
		}
		b, ok := l.peek()
		if !ok || b != '#' {
			return false, nil
		}
		l.advance() // consume '#'
		// Optional single space/tab immediately after '#'
		if b2, ok2 := l.peek(); ok2 && (b2 == ' ' || b2 == '\t') {
			l.advance()
		}
		return true, nil
	}

	// We were called with current char being '#' already consumed by caller in scanToken.
	// Trim one optional space/tab after this first '#'.
	if b, ok := l.peek(); ok && (b == ' ' || b == '\t') {
		l.advance()
	}

	// Capture the rest of the first line
	for {
		b, ok := l.peek()
		if !ok || b == '\n' {
			if ok {
				l.advance() // consume newline
			}
			bldr.WriteByte('\n')
			break
		}
		bldr.WriteByte(b)
		l.advance()
	}

	// Capture subsequent lines that also start with '#'
	for {
		save := l.cur
		cont, err := consumeHashOnLine()
		if err != nil {
			return "", err
		}
		if !cont {
			l.cur = save
			break
		}
		// read until newline
		for {
			b, ok := l.peek()
			if !ok || b == '\n' {
				if ok {
					l.advance()
				}
				bldr.WriteByte('\n')
				break
			}
			bldr.WriteByte(b)
			l.advance()
		}
	}

	s := bldr.String()
	if len(s) == 0 {
		return "", errors.New("incomplete annotation")
	}
	// Trim trailing newlines (at most what we added)
	for len(s) > 0 && s[len(s)-1] == '\n' {
		s = s[:len(s)-1]
	}
	return s, nil
}

// scanInlineParens reads everything between a '(' and the matching ')'.
// Caller must ensure the next byte is '('.
// No nesting supported; runs across newlines; errors if EOF before ')'.
func (l *Lexer) scanInlineParens() (string, error) {
	// consume '('
	if b, ok := l.peek(); !ok || b != '(' {
		return "", l.err("internal: expected '(' after inline opener")
	}
	l.advance()

	var bldr strings.Builder
	for {
		b, ok := l.peek()
		if !ok {
			if l.interactive {
				return "", l.errIncomplete("inline block was not terminated with ')'")
			}
			return "", l.err("inline block was not terminated with ')'")
		}
		if b == ')' {
			l.advance() // consume ')'
			break
		}
		bldr.WriteByte(b)
		l.advance()
	}
	return bldr.String(), nil
}

// --- hash/comment helpers ---

// handleDoubleHash processes '##' comments.
// Returns (handled, err). When handled, the content is ignored and start is advanced.
func (l *Lexer) handleDoubleHash() (bool, error) {
	b1, ok := l.peek()
	if !ok || b1 != '#' {
		return false, nil
	}
	l.advance() // second '#'

	// Inline comment: ##( ... ) → ignore entirely
	if b2, ok2 := l.peek(); ok2 && b2 == '(' {
		if _, err := l.scanInlineParens(); err != nil {
			return true, err
		}
		l.start = l.cur
		return true, nil
	}

	// Line comment: ## ... \n → ignore until newline
	l.ignoreUntilNewline()
	l.start = l.cur
	return true, nil
}

// handleSingleHash processes '#' annotations (inline or multiline).
// Returns (producedAnnotation, text, err).
func (l *Lexer) handleSingleHash() (bool, string, error) {
	if b1, ok := l.peek(); ok && b1 == '(' {
		text, err := l.scanInlineParens()
		if err != nil {
			return false, "", err
		}
		return true, text, nil
	}
	annot, err := l.scanAnnotation()
	if err != nil {
		return false, "", l.err("incomplete annotation")
	}
	return true, annot, nil
}

// --- misc helpers ---

func (l *Lexer) dotStartsNumber() bool {
	b, ok := l.peek()
	if !ok || !isDigit(b) {
		return false
	}
	prev := l.previousToken()
	if l.whitespaceBefore || prev == nil || !canBeLeftOperand(prev.Type) {
		return true
	}
	return false
}

// ---------------- main tokenization ----------------

func (l *Lexer) scanToken() (Token, error) {
	for {
		l.skipWhitespace()
		l.tokStartLine = l.line
		l.tokStartCol = l.col
		l.start = l.cur

		if l.isAtEnd() {
			return l.addToken(EOF, nil), nil
		}

		ch, _ := l.advance()

		// Single-char tokens & punctuation with whitespace-sensitive "(" and "["
		switch ch {
		case '(':
			if l.whitespaceBefore {
				return l.addToken(LROUND, "("), nil
			}
			return l.addToken(CLROUND, "("), nil
		case ')':
			return l.addToken(RROUND, ")"), nil
		case '[':
			if l.whitespaceBefore {
				return l.addToken(LSQUARE, "["), nil
			}
			return l.addToken(CLSQUARE, "["), nil
		case ']':
			return l.addToken(RSQUARE, "]"), nil
		case '{':
			return l.addToken(LCURLY, "{"), nil
		case '}':
			return l.addToken(RCURLY, "}"), nil
		case '+':
			return l.addToken(PLUS, "+"), nil
		case '*':
			return l.addToken(MULT, "*"), nil
		case '/':
			return l.addToken(DIV, "/"), nil
		case '%':
			return l.addToken(MOD, "%"), nil
		case ':':
			return l.addToken(COLON, ":"), nil
		case ',':
			return l.addToken(COMMA, ","), nil
		case '?':
			return l.addToken(QUESTION, "?"), nil
		}

		// '.' : either decimal-starting float or PERIOD
		if ch == '.' {
			if l.dotStartsNumber() {
				l.rewindToStart()
				tt, lit, err := l.scanNumber()
				if err != nil {
					return Token{}, err
				}
				return l.addToken(tt, lit), nil
			}
			return l.addToken(PERIOD, "."), nil
		}

		// Two-char operators and fallbacks
		switch ch {
		case '-':
			if b, ok := l.peek(); ok && b == '>' {
				l.advance()
				return l.addToken(ARROW, "->"), nil
			}
			return l.addToken(MINUS, "-"), nil
		case '=':
			if b, ok := l.peek(); ok && b == '=' {
				l.advance()
				return l.addToken(EQ, "=="), nil
			}
			return l.addToken(ASSIGN, "="), nil
		case '!':
			if b, ok := l.peek(); ok && b == '=' {
				l.advance()
				return l.addToken(NEQ, "!="), nil
			}
			return l.addToken(BANG, "!"), nil
		case '<':
			if b, ok := l.peek(); ok && b == '=' {
				l.advance()
				return l.addToken(LESS_EQ, "<="), nil
			}
			return l.addToken(LESS, "<"), nil
		case '>':
			if b, ok := l.peek(); ok && b == '=' {
				l.advance()
				return l.addToken(GREATER_EQ, ">="), nil
			}
			return l.addToken(GREATER, ">"), nil
		}

		// Comments / Annotations
		if ch == '#' {
			// Try '##' comment forms first
			if handled, err := l.handleDoubleHash(); handled || err != nil {
				if err != nil {
					return Token{}, err
				}
				continue
			}
			// Single '#': inline '#(...)' or multiline block
			ok, text, err := l.handleSingleHash()
			if err != nil {
				return Token{}, err
			}
			if ok {
				return l.addToken(ANNOTATION, text), nil
			}
		}

		// Strings
		if ch == '"' || ch == '\'' {
			l.rewindToStart()
			text, err := l.scanString()
			if err != nil {
				return Token{}, err
			}
			// After '.' a quoted key becomes ID (property name)
			if l.afterDotIsProperty() {
				return l.addToken(ID, text), nil
			}
			return l.addToken(STRING, text), nil
		}

		// Numbers (starting with digit)
		if isDigit(ch) {
			l.rewindToStart()
			tt, lit, err := l.scanNumber()
			if err != nil {
				return Token{}, err
			}
			return l.addToken(tt, lit), nil
		}

		// Identifiers / Keywords
		if isAlpha(ch) {
			l.rewindToStart()
			lex := l.scanIdentifier()
			// After '.', treat as property name (ID)
			if l.afterDotIsProperty() {
				return l.addToken(ID, lex), nil
			}
			if tt, ok := keywords[lex]; ok {
				switch tt {
				case NULL:
					return l.addToken(NULL, nil), nil
				case BOOLEAN:
					if lex == "true" {
						return l.addToken(BOOLEAN, true), nil
					}
					return l.addToken(BOOLEAN, false), nil
				default:
					return l.addToken(tt, lex), nil
				}
			}
			return l.addToken(ID, lex), nil
		}

		return Token{}, l.err(fmt.Sprintf("unexpected character: %q", ch))
	}
}
=== END FILE: lexer.go ===

=== BEGIN FILE: parser.go ===
// parser.go: Pratt parser for MindScript that produces Lisp-style S-expressions.
//
// OVERVIEW
// --------
// This file implements a newline-aware, interactive-ready Pratt parser for the
// MindScript language. The parser consumes the token stream produced by the
// lexer (see lexer.go) and returns a compact, Lisp-y S-expression (AST) where
// each node is encoded as []any with a leading tag string, e.g.:
//
//	("binop", "+", ("int", 1), ("id", "x"))
//
// DEPENDENCIES
// ------------
// • lexer.go — provides:
//   - Token / TokenType definitions and all token kinds used here
//   - Lexer, NewLexer, NewLexerInteractive
//   - IncompleteError and IsIncomplete(err)
//
// • standard library: fmt (for error messages)
//
// PARSING MODE & INTERACTIVITY
// ----------------------------
// Two entry points are provided:
//
//	ParseSExpr(src string)            — normal parsing (unterminated input is a ParseError)
//	ParseSExprInteractive(src string) — REPL-friendly (unterminated input becomes IncompleteError)
//
// In interactive mode, reaching EOF inside a construct (e.g., missing ')', ']', '}', 'end',
// or RHS after an operator/unary) yields IncompleteError with a precise position.
//
// OUTPUT SHAPE (AST)
// ------------------
// The result is always a top-level block:
//
//	("block", expr1, expr2, ...)
//
// Node forms (tags) the parser emits:
//
// Literals & identifiers:
//
//	("id",   name)           // identifiers and builtin TYPE tokens (e.g., "Int") used as names
//	("int",  int64)
//	("num",  float64)
//	("str",  string)
//	("bool", bool)
//	("null")
//
// Unary/postfix:
//
//	("unop", "-",   expr)    // prefix minus
//	("unop", "not", expr)    // logical not
//	("unop", "?",   expr)    // postfix optional operator
//
// Calls / property / indexing:
//
//	("call", callee, arg1, arg2, ...)
//	("idx",  object, indexExpr)                // obj[expr] or obj.<integer> or obj.(expr)
//	("get",  object, ("str", propertyName))    // obj.<id> or obj."string"
//
// Arrays & maps:
//
//	("array", elements...)
//	("map", ("pair",  keyStrExpr, valueExpr)*)
//	("map", ("pair!", keyStrExpr, valueExpr)* ) // key marked required via '!' in literal (e.g., {name!: "x"})
//
// Enum sugar:
//
//	Enum[ e1, e2, ... ] → ("enum", e1, e2, ...)
//
// Functions & oracles:
//
//	fun (p1[: T], p2[: T], ...) [-> Ret] do ... end
//	   → ("fun",   paramsArray, RetOr("id","Any"), bodyBlock)
//	oracle (p1[: T], ...) [-> Out] [from SourceExpr]
//	   → ("oracle", paramsArray, OutOr("id","Any"), SourceOr("array"))
//
// where paramsArray is ("array", ("pair", ("id", name), typeExpr), ...).
//
// Blocks and control:
//
//	do ... end                → ("block", ...)
//	if cond then … elif … else … end
//	   → ("if", ("pair", cond1, thenBlock1), ("pair", cond2, thenBlock2), [, elseBlock])
//	for <target> in <iter> do ... end → ("for", target, iter, body)
//	while cond do ... end             → ("while", cond, body)
//
// Return/break/continue (newline-sensitive):
//
//	If the next token is on the SAME line → take a following expression.
//	Else (or at EOF)                      → implicit null.
//	Examples:
//	  return 1      → ("return", ("int", 1))
//	  return\nx     → ("return", ("null"))
//
// Declaration patterns (used by `let` and `for` targets):
//
//	("decl", name)                        // let x
//	("darr", p1, p2, ...)                 // let [a, b, ...]
//	("dobj", ("pair", keyStrExpr, p), ...) // let {k: p, ...}
//
// Annotations may wrap the next pattern or expression:
//
//	("annot", ("str", text), node)
//
// Operators & precedence (higher binds tighter; '=' and '->' are right-assoc):
//
//	70: * / %
//	60: + -
//	50: < <= > >=
//	40: == !=
//	30: and
//	20: or
//	15: ->
//	10: =
//
// '=' yields ("assign", target, value) and requires an assignable target: one of
// "id", "get", "idx", "decl", "darr", "dobj".
//
// ERROR MODEL
// -----------
// • ParseError  — structural/grammar problems with location.
// • IncompleteError (interactive mode) — source ended while a construct was incomplete.
//
// NOTE ON WHITESPACE RULE FOR CALLS/PARAM LISTS
// -------------------------------------
// The lexer classifies '(' and '[' based on *immediate* preceding whitespace:
//
//   - '('  → LROUND  if there IS preceding whitespace
//     → CLROUND if there is NO preceding whitespace
//   - '['  → LSQUARE if there IS preceding whitespace
//     → CLSQUARE if there is NO preceding whitespace
//
// This distinction lets the parser treat juxtaposition forms as calls/indexing
// without lookbehind, and treat spaced forms as grouping/array literals.
//
// Consequences (important user-facing syntax):
//
//   - Calls and parameter lists require NO space before '(':
//     f(x)         // call: CLROUND
//     fun(x: T)    // function params: CLROUND
//     oracle(x: T) // oracle params: CLROUND
//     Using a space changes the token to LROUND and will NOT be seen as a call.
//     Example: "f (x)" parses as identifier 'f' followed by a grouped expression,
//     not a function call.
//
//   - Indexing requires NO space before '[':
//     arr[i]    // CLSQUARE (indexing)
//     With a space ("arr [i]"), '[' becomes LSQUARE and is NOT treated as indexing.
//
//   - Plain grouping still works with both LROUND/CLROUND:
//     (expr)   // grouping
//     but only CLROUND participates in call/postfix chains.
//
// NOTE ON TOKENS & NEWLINES
// -------------------------
// The parser relies on token positions provided by the lexer. For newline-sensitive
// behavior of return/break/continue, it checks whether the next token shares the same line.
//
// The complete behavior above is sufficient to consume this API without reading the
// implementation details below.
package mindscript

import (
	"fmt"
)

////////////////////////////////////////////////////////////////////////////////
//                                  PUBLIC API
////////////////////////////////////////////////////////////////////////////////

// S is the S-expression node representation used by the parser.
// It is a []any whose first element is a string tag (e.g., "binop", "id", "array").
// Subsequent elements are the node’s children or data.
//
// Common node forms (non-exhaustive):
//
//	("block", n1, n2, ...)
//	("id",   name)            // string name
//	("int",  int64)
//	("num",  float64)
//	("str",  string)
//	("bool", bool)
//	("null")
//	("unop", op, rhs)         // op is string: "-", "not", "?"
//	("binop", op, lhs, rhs)   // op is string: "+", "and", "==", ...
//	("assign", target, value) // "="
//	("call", callee, arg...)
//	("get",  obj, ("str", name))
//	("idx",  obj, indexExpr)
//	("array", ...)
//	("map", ("pair",  keyStrExpr, value)*)
//	("map", ("pair!", keyStrExpr, value)*)
//	("enum", ...)
//	("fun", paramsArray, retTypeExprOrAny, bodyBlock)
//	("oracle", paramsArray, outTypeExprOrAny, sourceExprOrEmptyArray)
//	("if", ("pair", cond, thenBlock)* [, elseBlock])
//	("while", cond, bodyBlock)
//	("for",   targetPatternOrLvalue, iterExpr, bodyBlock)
//	("decl", name) | ("darr", ...) | ("dobj", ("pair", keyStrExpr, pat)*)
//	("annot", ("str", text), wrappedNode)
type S = []any

// L is a small helper to build S-expression nodes. The first argument is the
// string tag; the remaining arguments are appended as children.
func L(tag string, parts ...any) S { return append([]any{tag}, parts...) }

// ParseError reports a non-interactive parse failure with a location.
// Typical causes: unexpected tokens, missing delimiters, invalid assignment target.
type ParseError struct {
	Line, Col int
	Msg       string
}

func (e *ParseError) Error() string {
	return fmt.Sprintf("PARSE ERROR at %d:%d: %s", e.Line, e.Col, e.Msg)
}

// ParseSExpr parses a MindScript source string into an S-expression AST.
// It lexes the source using NewLexer (normal mode) and parses according to the
// grammar summarized in the file header comments.
//
// Returns:
//   - S  — a top-level ("block", ...) node.
//   - error — *ParseError on grammar problems, or lexer errors propagated.
//
// Newline-sensitive control:
//
//	return/break/continue take an expression only if the next token is on the same line;
//	otherwise they default to ("null").
//
// Property/index/call chaining:
//   - Calls:           f(args...)             → ("call", f, args...)
//   - Index:           a[b]                   → ("idx", a, b)
//   - Computed dot:    a.(expr)               → ("idx", a, expr)
//   - Numeric dot:     a.42                   → ("idx", a, ("int", 42))
//   - Named property:  a.name / a."name"      → ("get", a, ("str", "name"))
//
// Operators:
//
//	Precedence and associativity are as listed in the header; "=" and "->" are right-assoc.
//	"=" yields ("assign", target, value) and requires assignable targets.
func ParseSExpr(src string) (S, error) {
	lex := NewLexer(src)
	toks, err := lex.Scan()
	if err != nil {
		return nil, err
	}
	p := &parser{toks: toks}
	return p.program()
}

// ParseSExprWithSpans parses `src` into an S-expression AST and also returns a
// SpanIndex mapping each node (addressed by NodePath) to its source byte span.
//
// Behavior:
//   - The AST is identical to ParseSExpr (stable contract).
//   - Spans are recorded in post-order (children before parent).
//   - Spans are half-open byte intervals [StartByte, EndByte) in `src`.
//   - The top-level ("block", ...) is included.
//
// Errors mirror ParseSExpr; lexer errors and parse errors are propagated unchanged.
func ParseSExprWithSpans(src string) (S, *SpanIndex, error) {
	lex := NewLexer(src)
	toks, err := lex.Scan()
	if err != nil {
		return nil, nil, err
	}
	p := &parser{toks: toks}
	ast, err := p.program()
	if err != nil {
		return nil, nil, err
	}
	idx := BuildSpanIndexPostOrder(ast, p.post) // sidecar index (spans.go)
	return ast, idx, nil
}

// ParseSExprInteractive is identical to ParseSExpr but uses NewLexerInteractive
// and returns *IncompleteError when the input ends mid-construct (e.g., missing
// ')', ']', '}', 'end', or the RHS of an operator/unary).
//
// Use IsIncomplete(err) to detect this case in a REPL and request more input.
func ParseSExprInteractive(src string) (S, error) {
	lex := NewLexerInteractive(src)
	toks, err := lex.Scan()
	if err != nil {
		return nil, err
	}
	p := &parser{toks: toks, interactive: true}
	return p.program()
}

//// END_OF_PUBLIC

////////////////////////////////////////////////////////////////////////////////
//                           PRIVATE IMPLEMENTATION
////////////////////////////////////////////////////////////////////////////////

// Pratt parser state
type parser struct {
	toks        []Token
	i           int
	interactive bool
	post        []Span
}

func (p *parser) atEnd() bool { return p.peek().Type == EOF }
func (p *parser) peek() Token {
	if p.i >= len(p.toks) {
		return p.toks[len(p.toks)-1]
	}
	return p.toks[p.i]
}
func (p *parser) prev() Token { return p.toks[p.i-1] }
func (p *parser) match(tt ...TokenType) bool {
	if p.atEnd() {
		return false
	}
	for _, t := range tt {
		if p.peek().Type == t {
			p.i++
			return true
		}
	}
	return false
}
func (p *parser) need(t TokenType, msg string) (Token, error) {
	if p.match(t) {
		return p.prev(), nil
	}
	g := p.peek()
	if p.interactive && g.Type == EOF {
		return Token{}, &IncompleteError{Line: g.Line, Col: g.Col, Msg: msg}
	}
	return Token{}, &ParseError{Line: g.Line, Col: g.Col, Msg: msg}
}

func (p *parser) nextTokenIsOnSameLine(as Token) bool {
	if p.atEnd() {
		return false
	}
	return p.peek().Line == as.Line
}

// precedence (higher binds tighter)
func lbp(t TokenType) (int, bool) {
	switch t {
	case ARROW:
		return 15, true // low precedence, right-assoc
	case MULT, DIV, MOD:
		return 70, true
	case PLUS, MINUS:
		return 60, true
	case LESS, LESS_EQ, GREATER, GREATER_EQ:
		return 50, true
	case EQ, NEQ:
		return 40, true
	case AND:
		return 30, true
	case OR:
		return 20, true
	case ASSIGN: // '=' right-assoc
		return 10, true
	}
	return 0, false
}

func (p *parser) program() (S, error) {
	var items []any
	for !p.atEnd() {
		e, err := p.expr(0)
		if err != nil {
			return nil, err
		}
		items = append(items, e)
	}
	root := L("block", items...)
	// Append span for the top-level block: cover all non-EOF tokens.
	// If there are no tokens besides EOF, leave span empty (Start=End=0).
	startTok := 0
	endTok := len(p.toks) - 2 // last non-EOF
	if endTok >= startTok && len(p.toks) > 0 {
		p.post = append(p.post, Span{
			StartByte: p.toks[startTok].StartByte,
			EndByte:   p.toks[endTok].EndByte,
		})
	} else {
		p.post = append(p.post, Span{})
	}
	return root, nil
}

func (p *parser) expr(minBP int) (S, error) {
	// Record the token index at which this expression starts.
	nodeStartTok := p.i
	// ---- prefix ----
	t := p.peek()
	p.i++
	var left S
	switch t.Type {
	case ID, TYPE:
		// Identifiers and capitalized built-in types behave the same in expressions.
		left = L("id", t.Literal)

	case ENUM:
		// Enum[ ... ] sugar → ("enum", ...)
		if p.peek().Type == LSQUARE || p.peek().Type == CLSQUARE {
			p.i++
			arr, err := p.arrayLiteralAfterOpen()
			if err != nil {
				return nil, err
			}
			it := make([]any, 0, len(arr)-1)
			for i := 1; i < len(arr); i++ {
				it = append(it, arr[i])
			}
			left = L("enum", it...)
		} else {
			left = L("id", t.Literal)
		}

	case INTEGER:
		left = L("int", t.Literal)
	case NUMBER:
		left = L("num", t.Literal)
	case STRING:
		left = L("str", t.Literal)
	case BOOLEAN:
		left = L("bool", t.Literal)
	case NULL:
		left = L("null")

	case MINUS, NOT:
		// unary operators require a RHS expression
		if p.atEnd() && p.interactive {
			return nil, &IncompleteError{Line: t.Line, Col: t.Col, Msg: "expected expression after unary operator"}
		}
		r, err := p.expr(80)
		if err != nil {
			return nil, err
		}
		left = L("unop", t.Lexeme, r)

	case LROUND, CLROUND:
		inner, err := p.expr(0)
		if err != nil {
			return nil, err
		}
		if _, err := p.need(RROUND, "expected ')'"); err != nil {
			return nil, err
		}
		left = inner

	case LSQUARE, CLSQUARE:
		a, err := p.arrayLiteralAfterOpen()
		if err != nil {
			return nil, err
		}
		left = a

	case LCURLY:
		// map literal {k: v, ...} where k is id or string; id may have trailing '!' for required
		if p.match(RCURLY) {
			left = L("map")
		} else {
			var pairs []any
			for {
				k, req, err := p.keyRequired()
				if err != nil {
					return nil, err
				}
				if _, err := p.need(COLON, "expected ':' after key"); err != nil {
					return nil, err
				}
				v, err := p.expr(0)
				if err != nil {
					return nil, err
				}
				tag := "pair"
				if req {
					tag = "pair!"
				}
				pairs = append(pairs, L(tag, k, v))
				if !p.match(COMMA) {
					break
				}
			}
			if _, err := p.need(RCURLY, "expected '}'"); err != nil {
				return nil, err
			}
			left = L("map", pairs...)
		}

	case FUNCTION:
		params, err := p.params()
		if err != nil {
			return nil, err
		}
		var ret any = L("id", "Any")
		if p.match(ARROW) {
			if p.atEnd() && p.interactive {
				return nil, &IncompleteError{Line: t.Line, Col: t.Col, Msg: "expected return type after '->'"}
			}
			r, err := p.expr(0)
			if err != nil {
				return nil, err
			}
			ret = r
		}
		body, err := p.parseBlock(true) // require 'do'
		if err != nil {
			return nil, err
		}
		left = L("fun", params, ret, body)

	case ORACLE:
		params, err := p.params()
		if err != nil {
			return nil, err
		}
		var out any = L("id", "Any")
		if p.match(ARROW) {
			if p.atEnd() && p.interactive {
				return nil, &IncompleteError{Line: t.Line, Col: t.Col, Msg: "expected output type after '->'"}
			}
			o, err := p.expr(0)
			if err != nil {
				return nil, err
			}
			out = o
		}
		// accept any expression after `from`
		var src any = L("array")
		if p.match(FROM) {
			if p.atEnd() && p.interactive {
				return nil, &IncompleteError{Line: t.Line, Col: t.Col, Msg: "expected expression after 'from'"}
			}
			ex, err := p.expr(0)
			if err != nil {
				return nil, err
			}
			src = ex
		}
		left = L("oracle", params, out, src)

	case RETURN, BREAK, CONTINUE:
		// Newline-sensitive behavior; see file header.
		if !p.nextTokenIsOnSameLine(t) {
			switch t.Type {
			case RETURN:
				left = L("return", L("null"))
			case BREAK:
				left = L("break", L("null"))
			default:
				left = L("continue", L("null"))
			}
		} else {
			x, err := p.expr(0)
			if err != nil {
				return nil, err
			}
			switch t.Type {
			case RETURN:
				left = L("return", x)
			case BREAK:
				left = L("break", x)
			default:
				left = L("continue", x)
			}
		}

	case IF:
		thenIf, err := p.ifExpr()
		if err != nil {
			return nil, err
		}
		left = thenIf

	case DO:
		// do ... end
		body, err := p.parseBlock(false) // 'do' already consumed
		if err != nil {
			return nil, err
		}
		left = body

	case FOR:
		f, err := p.forExpr()
		if err != nil {
			return nil, err
		}
		left = f

	case WHILE:
		w, err := p.whileExpr()
		if err != nil {
			return nil, err
		}
		left = w

	case LET:
		pat, err := p.declPattern()
		if err != nil {
			return nil, err
		}
		left = pat
		if tag, _ := pat[0].(string); tag == "darr" || tag == "dobj" {
			if p.peek().Type != ASSIGN {
				g := p.peek()
				// If input ended, mark as incomplete
				if p.interactive && g.Type == EOF {
					return nil, &IncompleteError{Line: g.Line, Col: g.Col, Msg: "expected '=' after destructuring let pattern"}
				}
				return nil, &ParseError{Line: g.Line, Col: g.Col, Msg: "expected '=' after destructuring let pattern"}
			}
		}

	case TYPECONS: // "type" <expr>
		if p.atEnd() && p.interactive {
			return nil, &IncompleteError{Line: t.Line, Col: t.Col, Msg: "expected type expression after 'type'"}
		}
		x, err := p.expr(0)
		if err != nil {
			return nil, err
		}
		left = L("type", x)

	case ANNOTATION:
		// wrap next expression
		if p.atEnd() && p.interactive {
			return nil, &IncompleteError{Line: t.Line, Col: t.Col, Msg: "expected expression after annotation"}
		}
		x, err := p.expr(0)
		if err != nil {
			return nil, err
		}
		txt := ""
		if s, ok := t.Literal.(string); ok {
			txt = s
		}
		left = L("annot", L("str", txt), x)

	default:
		if t.Type == EOF && p.interactive {
			return nil, &IncompleteError{Line: t.Line, Col: t.Col, Msg: "unexpected end of input"}
		}
		return nil, &ParseError{Line: t.Line, Col: t.Col, Msg: fmt.Sprintf("unexpected token '%s'", t.Lexeme)}
	}

	// ---- postfix chain: call (), index [], get . ----
	for {
		switch p.peek().Type {
		case QUESTION:
			p.i++
			left = L("unop", "?", left)
			continue
		case CLROUND:
			p.i++
			var args []any
			if !p.match(RROUND) {
				for {
					a, err := p.expr(0)
					if err != nil {
						return nil, err
					}
					args = append(args, a)
					if !p.match(COMMA) {
						break
					}
				}
				if _, err := p.need(RROUND, "expected ')'"); err != nil {
					return nil, err
				}
			}
			left = L("call", append([]any{left}, args...)...)
			continue
		case CLSQUARE:
			p.i++
			idx, err := p.expr(0)
			if err != nil {
				return nil, err
			}
			if _, err := p.need(RSQUARE, "expected ']'"); err != nil {
				return nil, err
			}
			left = L("idx", left, idx)
			continue
		case PERIOD:
			p.i++
			// obj.(EXPR) → idx(obj, EXPR)
			if p.match(LROUND) || p.match(CLROUND) {
				ex, err := p.expr(0)
				if err != nil {
					return nil, err
				}
				if _, err := p.need(RROUND, "expected ')' after computed property"); err != nil {
					return nil, err
				}
				left = L("idx", left, ex)
				continue
			}
			if p.match(INTEGER) { // obj.90 → idx(obj, 90)
				left = L("idx", left, L("int", p.prev().Literal))
				continue
			}
			if p.match(ID) || p.match(STRING) {
				left = L("get", left, L("str", p.prev().Literal))
				continue
			}
			g := p.peek()
			return nil, &ParseError{Line: g.Line, Col: g.Col, Msg: "expected property name, integer, or '(expr)' after '.'"}
		}
		break
	}

	// ---- infix ops ----
	for {
		op := p.peek()
		bp, ok := lbp(op.Type)
		if !ok || bp < minBP {
			break
		}
		p.i++

		nextBP := bp + 1
		if op.Type == ASSIGN || op.Type == ARROW { // right-assoc
			nextBP = bp
		}

		if op.Type == ASSIGN && !assignable(left) {
			return nil, &ParseError{Line: op.Line, Col: op.Col, Msg: "invalid assignment target"}
		}

		if p.atEnd() && p.interactive {
			return nil, &IncompleteError{Line: op.Line, Col: op.Col, Msg: "expected expression after operator"}
		}
		right, err := p.expr(nextBP)
		if err != nil {
			return nil, err
		}
		if op.Type == ASSIGN {
			left = L("assign", left, right)
		} else {
			left = L("binop", op.Lexeme, left, right)
		}
	}
	// On completion of this node, append its span (post-order).
	// Use tokens[nodeStartTok ... p.i-1].
	if p.i-1 >= nodeStartTok && nodeStartTok < len(p.toks) {
		endTok := p.i - 1
		// Guard against EOF being the only thing consumed (shouldn't happen).
		if endTok >= 0 && endTok < len(p.toks) {
			p.post = append(p.post, Span{
				StartByte: p.toks[nodeStartTok].StartByte,
				EndByte:   p.toks[endTok].EndByte,
			})
		} else {
			p.post = append(p.post, Span{})
		}
	} else {
		p.post = append(p.post, Span{})
	}
	return left, nil
}

// ---------- helpers ----------

func (p *parser) arrayLiteralAfterOpen() (S, error) {
	// We’ve already consumed the opening '['
	if p.match(RSQUARE) {
		return L("array"), nil
	}
	var elems []any
	for {
		it, err := p.expr(0)
		if err != nil {
			return nil, err
		}
		elems = append(elems, it)
		if !p.match(COMMA) {
			break
		}
	}
	if _, err := p.need(RSQUARE, "expected ']'"); err != nil {
		return nil, err
	}
	return L("array", elems...), nil
}

func (p *parser) params() (S, error) {
	if _, err := p.need(CLROUND, "expected '(' to start parameters"); err != nil {
		return nil, err
	}
	if p.match(RROUND) {
		return L("array"), nil
	}
	var ps []any
	for {
		idTok, err := p.need(ID, "expected parameter name")
		if err != nil {
			return nil, err
		}
		var t any = L("id", "Any")
		if p.match(COLON) {
			if p.atEnd() && p.interactive {
				return nil, &IncompleteError{Line: idTok.Line, Col: idTok.Col, Msg: "expected type after ':'"}
			}
			e, err := p.expr(0) // parse type as expression
			if err != nil {
				return nil, err
			}
			t = e
		}
		ps = append(ps, L("pair", L("id", idTok.Literal), t))
		if !p.match(COMMA) {
			break
		}
	}
	if _, err := p.need(RROUND, "expected ')' after parameters"); err != nil {
		return nil, err
	}
	return L("array", ps...), nil
}

// ifExpr builds: ("if", ("pair", cond1, thenBlk1), ("pair", cond2, thenBlk2), ..., [elseBlk?])
// NOTE: The final element, if present, is a bare ("block", ...) for 'else', not wrapped in a tag.
func (p *parser) ifExpr() (S, error) {
	cond, err := p.expr(0)
	if err != nil {
		return nil, err
	}
	if _, err := p.need(THEN, "expected 'then'"); err != nil {
		return nil, err
	}
	thenBlk, err := p.blockUntil(END, ELIF, ELSE)
	if err != nil {
		return nil, err
	}
	arms := []any{L("pair", cond, thenBlk)}
	for p.match(ELIF) {
		c, err := p.expr(0)
		if err != nil {
			return nil, err
		}
		if _, err := p.need(THEN, "expected 'then'"); err != nil {
			return nil, err
		}
		b, err := p.blockUntil(END, ELIF, ELSE)
		if err != nil {
			return nil, err
		}
		arms = append(arms, L("pair", c, b))
	}
	var elseTail []any
	if p.match(ELSE) {
		b, err := p.blockUntil(END)
		if err != nil {
			return nil, err
		}
		elseTail = []any{b}
	}
	if _, err := p.need(END, "expected 'end'"); err != nil {
		return nil, err
	}
	return L("if", append(arms, elseTail...)...), nil
}

// Parse a block; if requireDo is true, assert a leading 'do'.
// Always requires a trailing 'end'.
func (p *parser) parseBlock(requireDo bool) (S, error) {
	if requireDo {
		if _, err := p.need(DO, "expected 'do'"); err != nil {
			return nil, err
		}
	}
	b, err := p.blockUntil(END)
	if err != nil {
		return nil, err
	}
	if _, err := p.need(END, "expected 'end'"); err != nil {
		return nil, err
	}
	return b, nil
}

func (p *parser) blockUntil(stops ...TokenType) (S, error) {
	stop := map[TokenType]bool{}
	for _, s := range stops {
		stop[s] = true
	}
	var items []any
	for !p.atEnd() && !stop[p.peek().Type] {
		e, err := p.expr(0)
		if err != nil {
			return nil, err
		}
		items = append(items, e)
	}
	return L("block", items...), nil
}

func (p *parser) forExpr() (S, error) {
	// for <target> in <expr> do ... end
	tgt, err := p.forTarget()
	if err != nil {
		return nil, err
	}
	if _, err := p.need(IN, "expected 'in'"); err != nil {
		return nil, err
	}
	iter, err := p.expr(0)
	if err != nil {
		return nil, err
	}
	body, err := p.parseBlock(true)
	if err != nil {
		return nil, err
	}
	return L("for", tgt, iter, body), nil
}

func (p *parser) whileExpr() (S, error) {
	// while COND do ... end
	cond, err := p.expr(0)
	if err != nil {
		return nil, err
	}
	body, err := p.parseBlock(true)
	if err != nil {
		return nil, err
	}
	return L("while", cond, body), nil
}

func (p *parser) forTarget() (S, error) {
	// Allow explicit `let <pattern>`
	if p.match(LET) {
		return p.declPattern()
	}

	// If the next token clearly starts a pattern, parse a *declaration* pattern
	// (implicit binding) just like Python: `for [k, v] in ...` or `for {k:x} in ...`.
	switch p.peek().Type {
	case LSQUARE, CLSQUARE, LCURLY, ANNOTATION:
		save := p.i
		pt, err := p.declPattern()
		if err == nil {
			return pt, nil // e.g., ("darr", ...), ("dobj", ...)
		}
		// In interactive mode, bubble up incompleteness instead of falling back
		if p.interactive && IsIncomplete(err) {
			return nil, err
		}
		p.i = save // fall back below if pattern parse fails
	}

	// Otherwise, parse a simple prim+postfix expression and require it's assignable
	save := p.i
	e, err := p.expr(90)
	if err != nil {
		return nil, err
	}
	if !assignable(e) {
		p.i = save
		g := p.peek()
		return nil, &ParseError{Line: g.Line, Col: g.Col, Msg: "invalid for-target (must be id/get/idx/decl/pattern)"}
	}

	// Bare id → implicit declaration, same as before
	if e[0].(string) == "id" {
		return L("decl", e[1].(string)), nil
	}
	return e, nil
}

func assignable(n S) bool {
	if len(n) == 0 {
		return false
	}
	switch n[0] {
	case "id", "get", "idx", "decl", "darr", "dobj":
		return true
	default:
		return false
	}
}

// ----- Declaration patterns (with annotation wrapper) -----

func (p *parser) declPattern() (S, error) {
	// Allow stacked line-leading annotations to wrap the next pattern
	if p.match(ANNOTATION) {
		txt := ""
		if s, ok := p.prev().Literal.(string); ok {
			txt = s
		}
		sub, err := p.declPattern()
		if err != nil {
			return nil, err
		}
		return L("annot", L("str", txt), sub), nil
	}

	if p.match(ID) {
		return L("decl", p.prev().Literal), nil
	}
	if p.match(LSQUARE, CLSQUARE) {
		return p.arrayDeclPattern()
	}
	if p.match(LCURLY) {
		return p.objectDeclPattern()
	}
	g := p.peek()
	if p.interactive && g.Type == EOF {
		return nil, &IncompleteError{Line: g.Line, Col: g.Col, Msg: "expected let pattern (id, [], or {})"}
	}
	return nil, &ParseError{Line: g.Line, Col: g.Col, Msg: "expected let pattern (id, [], or {})"}
}

func (p *parser) arrayDeclPattern() (S, error) {
	if p.match(RSQUARE) {
		return L("darr"), nil
	}
	var parts []any
	for {
		pt, err := p.declPattern()
		if err != nil {
			return nil, err
		}
		parts = append(parts, pt)
		if !p.match(COMMA) {
			break
		}
	}
	if _, err := p.need(RSQUARE, "expected ']' in array pattern"); err != nil {
		return nil, err
	}
	return L("darr", parts...), nil
}

// read a key token and turn it into ("str", key)
func (p *parser) readKeyString() (S, error) {
	// Allow annotation(s) directly in front of the key.
	if p.match(ANNOTATION) {
		txt := ""
		if s, ok := p.prev().Literal.(string); ok {
			txt = s
		}
		k, err := p.readKeyString()
		if err != nil {
			return nil, err
		}
		return L("annot", L("str", txt), k), nil
	}

	// Quoted key.
	if p.match(STRING) {
		return L("str", p.prev().Literal), nil
	}

	// Any word-like token (identifier, built-in type, or keyword), plus true/false/null.
	t := p.peek()
	switch t.Type {
	case ID, TYPE,
		AND, OR, NOT, LET, DO, END, RETURN, BREAK, CONTINUE,
		IF, THEN, ELIF, ELSE, FUNCTION, ORACLE, FOR, IN, FROM,
		TYPECONS, ENUM,
		BOOLEAN, NULL:
		p.i++
		name := t.Lexeme
		if s, ok := t.Literal.(string); ok { // ID/TYPE carry string Literal
			name = s
		}
		return L("str", name), nil
	}

	g := p.peek()
	if p.interactive && g.Type == EOF {
		return nil, &IncompleteError{Line: g.Line, Col: g.Col, Msg: "expected key"}
	}
	return nil, &ParseError{Line: g.Line, Col: g.Col, Msg: "expected key"}
}

func (p *parser) keyRequired() (key S, required bool, err error) {
	k, err := p.readKeyString()
	if err != nil {
		return nil, false, err
	}
	req := p.match(BANG)
	return k, req, nil
}

func (p *parser) objectDeclPattern() (S, error) {
	if p.match(RCURLY) {
		return L("dobj"), nil
	}
	var pairs []any
	for {
		key, err := p.readKeyString()
		if err != nil {
			return nil, err
		}
		if _, err := p.need(COLON, "expected ':' after key in object pattern"); err != nil {
			return nil, err
		}
		pt, err := p.declPattern()
		if err != nil {
			return nil, err
		}
		pairs = append(pairs, L("pair", key, pt))
		if !p.match(COMMA) {
			break
		}
	}
	if _, err := p.need(RCURLY, "expected '}' in object pattern"); err != nil {
		return nil, err
	}
	return L("dobj", pairs...), nil
}
=== END FILE: parser.go ===

=== BEGIN FILE: spans.go ===
// spans.go — Sidecar spans for MindScript ASTs (S-expressions)
//
// WHAT THIS MODULE DOES
// =====================
// This module provides a tiny, non-invasive mechanism to associate **source-code
// byte spans** with nodes of a MindScript AST (encoded as the S-expression type
// `S` from parser.go) **without modifying the AST itself**.
//
// The spans are modeled as half-open byte intervals `[StartByte, EndByte)`
// relative to the original UTF-8 source. Line/column coordinates are intentionally
// omitted here to keep the structure minimal; callers can derive them on demand
// from the original source text.
//
// HOW SPANS ARE ASSOCIATED TO NODES
// =================================
// We use a *sidecar* structure (`SpanIndex`) keyed by a stable, structural
// address called a **NodePath**. A `NodePath` is a slice of child indexes
// into the AST tree: e.g. `[]int{0,2,1}` means “root’s 0th child → its 2nd
// child → its 1st child”. Paths are defined against the S-expression shape
// where a node is `[]any{tagString, child0, child1, ...}` — i.e. the first
// element is the string tag, and child index 0 refers to the element at
// S[1], child index 1 refers to S[2], etc.
//
// This file does **not** compute spans itself. Instead, the parser (or any
// external producer) records one `Span` per AST node in **post-order**
// (children before parent) while constructing or inspecting the tree, and
// then calls `BuildSpanIndexPostOrder(ast, spans)` to bind those spans to
// concrete paths via a deterministic walk of the AST in the same order.
//
// The result is a `SpanIndex` you can query with a `NodePath` to retrieve
// the associated byte interval in the original source.
//
// DEPENDENCIES ON OTHER FILES
// ===========================
// • parser.go
//   - Defines the S-expression node type alias `type S = []any`.
//   - Produces the AST that this module indexes.
//   - (Optional instrumentation) While parsing, record a `Span` per finished
//     node in **post-order** (children first, then the node) using the token
//     byte spans collected by the lexer.
//
// • lexer.go
//   - Tokens should carry precise byte offsets (`StartByte`/`EndByte`) so that
//     the parser can compute node spans as:
//     node.StartByte = firstToken.StartByte
//     node.EndByte   = lastToken.EndByte
//
// PERFORMANCE & CONCURRENCY
// =========================
// Building an index is O(n) in the number of AST nodes. `SpanIndex` is
// read-only after construction and safe to share for concurrent reads.
// Memory usage is one map entry per node (string key per `NodePath`).
//
// PUBLIC VS PRIVATE LAYOUT
// ========================
// The file is split into a PUBLIC API (types and functions you call) and a
// PRIVATE section (helpers and internal details). The PUBLIC API is fully
// documented so its behavior is understandable without reading the PRIVATE
// code.
//
// ─────────────────────────────────────────────────────────────────────────────
package mindscript

import (
	"strconv"
	"strings"
)

////////////////////////////////////////////////////////////////////////////////
//                                  PUBLIC API
////////////////////////////////////////////////////////////////////////////////

// Span represents a half-open byte interval [StartByte, EndByte) in the original
// source text. Offsets are counted in bytes from the start of the UTF-8 source.
// EndByte is exclusive.
//
// Line/column coordinates are not stored here to keep Span minimal; if you need
// them, compute (line, col) from the original source using your preferred mapping.
type Span struct {
	StartByte int // inclusive
	EndByte   int // exclusive
}

// NodePath is a stable structural address into an S-expression AST.
// Each integer selects a child in the node's children array:
//
//	path element k  → child at S[k+1] (since S[0] is the string tag).
//
// Example:
//
//	// ("call", callee, arg0, arg1)
//	//  tag   ^      ^ child0 ^ child1
//	//  S[0]        S[1]      S[2]     S[3]
//	path []int{0}   → callee
//	path []int{2}   → arg1
type NodePath []int

// SpanIndex stores a sidecar mapping from NodePath → Span for an AST.
// It is read-only after construction. Use Get to retrieve spans by path.
//
// Typical construction flow (performed by the parser or a post-pass):
//  1. Walk/construct the AST while recording one Span per node in post-order.
//  2. Call BuildSpanIndexPostOrder(ast, postorderSpans) to bind spans to paths.
//  3. Query with si.Get(path) wherever you need source intervals.
type SpanIndex struct {
	byPath map[string]Span
}

// Get returns the span associated with the given path, if present.
// The boolean is false if the path is unknown or the index is nil.
//
// A SpanIndex may be partial (e.g., producer skipped some nodes). In that case
// only the recorded nodes will resolve to spans.
func (si *SpanIndex) Get(p NodePath) (Span, bool) {
	if si == nil {
		return Span{}, false
	}
	sp, ok := si.byPath[pathKey(p)]
	return sp, ok
}

// BuildSpanIndexPostOrder constructs a SpanIndex by walking the AST in
// **post-order** (children before parent) and binding each visited node to
// the next span from the provided `postorder` slice.
//
// Contract:
//   - The `postorder` slice must list exactly one Span for each node in `root`
//     in post-order. If it is longer, extras are ignored; if shorter, remaining
//     nodes are left unindexed (Get will return (Span{}, false) for them).
//   - The resulting index is read-only and safe for concurrent reads.
//
// Complexity: O(n) time and O(n) space where n is the number of AST nodes.
//
// Example usage (parser instrumentation idea):
//
//	// During parse, for each finished node (after parsing children):
//	//   spans = append(spans, Span{StartByte:firstTok.StartByte, EndByte:lastTok.EndByte})
//	idx := BuildSpanIndexPostOrder(ast, spans)
//	sp, ok := idx.Get(NodePath{0,2}) // lookup "child 0's child 2"
func BuildSpanIndexPostOrder(root S, postorder []Span) *SpanIndex {
	si := &SpanIndex{byPath: make(map[string]Span, len(postorder))}
	bindPostOrder(si, root, postorder)
	return si
}

//// END_OF_PUBLIC

////////////////////////////////////////////////////////////////////////////////
//                                 PRIVATE
////////////////////////////////////////////////////////////////////////////////

// pathKey serializes a NodePath to a compact "a.b.c" string used as the map key.
func pathKey(p NodePath) string {
	if len(p) == 0 {
		return ""
	}
	var sb strings.Builder
	for i, x := range p {
		if i > 0 {
			sb.WriteByte('.')
		}
		sb.WriteString(strconv.Itoa(x))
	}
	return sb.String()
}

// bindPostOrder walks `root` in post-order, assigning spans from `postorder`
// to each visited node, in order.
func bindPostOrder(si *SpanIndex, root S, postorder []Span) {
	i := 0
	var walk func(n S, path NodePath)
	walk = func(n S, path NodePath) {
		// Visit children
		for ci := 1; ci < len(n); ci++ {
			if child, ok := n[ci].(S); ok {
				walk(child, append(path, ci-1))
			}
		}
		// Bind this node
		if i < len(postorder) {
			si.byPath[pathKey(path)] = postorder[i]
			i++
		}
	}
	walk(root, nil)
}
=== END FILE: spans.go ===

=== BEGIN FILE: printer.go ===
// printer.go: pretty-printers for MindScript ASTs, types, and runtime values.
//
// What this file does
// -------------------
// This module provides the formatting layer for MindScript. It renders three
// kinds of data to human-readable, stable strings:
//
//  1. Parsed source ASTs (S-expressions) → MindScript source code.
//     - Entry points: Pretty(src), FormatSExpr(ast).
//     - Produces whitespace- and newline-stable output with minimal
//     parentheses, based on operator precedence. It understands all
//     statement and expression tags emitted by the parser (e.g. "fun",
//     "oracle", "for", "if/elif/else", "type", "block", "assign",
//     "return/break/continue", arrays, maps, calls, indexing, properties,
//     unary and binary operators).
//     - Annotation nodes ("annot", ("str", text), <node>) become `# ...`
//     line comments immediately above the printed construct.
//     - Property names are emitted bare if they are identifier-like, otherwise
//     quoted. Destructuring declaration patterns ("decl" | "darr" | "dobj")
//     are rendered in a compact, readable form.
//     - Formatting emits no space before '(' for calls and for 'fun(...)'
//     and 'oracle(...)' parameter lists, matching the lexer’s CLROUND rule.
//
//  2. Type ASTs (S-expressions) → compact type strings.
//     - Entry point: FormatType(t).
//     - Supported forms:
//     • ("id", "Any"|"Null"|"Bool"|"Int"|"Num"|"Str"|"Type")
//     • ("unop","?", T)         → prints as `T?`
//     • ("array", T)            → prints as `[T]`
//     • ("map", ("pair"| "pair!", ("str",k), T) ...)
//     Required fields print with a trailing `!` on the key.
//     Key/value annotations (if wrapped in "annot") become `# ...` lines.
//     • ("enum", literalS... )  → prints as `Enum[ ... ]`, where members
//     may be scalars, arrays, or maps.
//     • ("binop","->", A, B)    → prints as `(A) -> B`, flattened across
//     right-associated chains.
//     - Output is stable. Multi-line maps are rendered with sorted keys to
//     avoid visual churn.
//
//  3. Runtime values (Value) → width-aware strings.
//     - Entry point: FormatValue(v).
//     - Scalars print plainly (`null`, `true/false`, numbers, quoted strings).
//     - Arrays and maps prefer a single-line rendering if it fits the
//     `MaxInlineWidth` budget and no elements/keys force multi-line; else
//     they fall back to pretty, multi-line output with indentation.
//     - Map keys are emitted bare if they’re identifier-like, otherwise quoted.
//     - Per-value annotations (Value.Annot) and per-key annotations in maps
//     (MapObject.KeyAnn[name]) are printed as `# ...` header lines.
//     - Functions print as `<fun: a:T -> b:U -> R>` (or `_:Null` for zero-arg).
//     - Types (VTType) are printed by extracting the embedded type AST and
//     delegating to FormatType.
//     - Modules print as `<module: <pretty name>>` when available.
//
// Dependencies (other files)
// --------------------------
// • parser.go
//   - S = []any (AST payload shape)
//   - ParseSExpr(string) (used by Pretty)
//   - AST tags: "block", "fun", "oracle", "for", "while", "if", "then", "elif",
//     "else", "type", "return", "break", "continue", "assign", "array", "map",
//     "pair"/"pair!", "get", "idx", "call", "id", "str", "int", "num", "bool",
//     "null", "unop", "binop", "decl", "darr", "dobj", "annot".
//
// • interpreter.go (runtime model)
//   - Value, ValueTag (VTNull, VTBool, VTInt, VTNum, VTStr, VTArray, VTMap,
//     VTFun, VTType, VTModule, VTHandle)
//   - Fun, TypeValue, MapObject (Entries/KeyAnn/Keys).
//
// • modules.go (module loader)
//   - Module struct and prettySpec(string) (used for VTModule display).
//
// • errors.go (shared errors)
//   - WrapErrorWithSource(err, src) (used by Pretty).
//
// PUBLIC vs PRIVATE layout
// ------------------------
// This file is organized in two blocks:
//  1. PUBLIC: the user-facing constants & functions with thorough docstrings.
//  2. PRIVATE: helper types and functions that implement the printers.
//
// Formatting policy highlights:
//   - Indentation uses **tabs** only (gofmt-style).
//   - Canonical output (`Standardize`) ends with exactly one trailing '\n'.
package mindscript

import (
	"fmt"
	"sort"
	"strconv"
	"strings"
	"unicode"
)

// ==============================
// ========== PUBLIC ============
// ==============================

// MaxInlineWidth controls when arrays/maps are rendered on a single line
// by FormatValue. If the one-line candidate exceeds this many characters,
// or if any element/key is annotated or multiline by nature, the value is
// rendered across multiple lines with indentation.
//
// This setting is read at call time and is safe to change between calls.
var MaxInlineWidth = 80

// IndentWithTabs controls whether pretty output uses tabs (like gofmt) for
// indentation. When false, two spaces are used.
var IndentWithTabs = true

// Pretty parses a MindScript source string and returns a formatted version.
//
// Behavior:
//   - Parses `src` via ParseSExpr (from parser.go). If parsing fails,
//     the error is wrapped with source context via WrapErrorWithSource.
//   - On success, it pretty-prints the resulting AST using FormatSExpr,
//     producing stable, whitespace-normalized code.
//   - Annotation nodes ("annot", ("str", text), X) print as `# text`
//     lines directly above X.
//   - Operators use minimal parentheses according to precedence/associativity:
//   - "->" (lowest), "+", "-", "*", "/", "%", comparisons, "==", "!=", "and", "or"
//   - postfix call/index/get bind tightest; unary ("not", "-") binds tighter than
//     binary arithmetic; assignment is the loosest among term operators.
//   - Object/map keys are emitted unquoted if they match identifier syntax
//     ([A-Za-z_][A-Za-z0-9_]*), else quoted with JSON-style escapes.
//   - Destructuring declaration patterns ("decl"/"darr"/"dobj") and
//     assignments print in a user-friendly layout.
//
// Errors:
//   - Returns a non-nil error if parsing fails; otherwise returns the formatted text.
func Pretty(src string) (string, error) {
	ast, err := ParseSExpr(src)
	if err != nil {
		return "", WrapErrorWithSource(err, src)
	}
	return FormatSExpr(ast), nil
}

// Standardize returns the canonical source form ("standard_source_code"):
//   - deterministic layout
//   - indentation using tabs (configurable via IndentWithTabs)
//   - exactly one trailing newline
func Standardize(src string) (string, error) {
	ast, err := ParseSExpr(src)
	if err != nil {
		return "", WrapErrorWithSource(err, src)
	}
	out := FormatSExpr(ast)
	if !strings.HasSuffix(out, "\n") {
		out += "\n"
	} else {
		// ensure exactly one
		out = strings.TrimRight(out, "\n") + "\n"
	}
	return out, nil
}

// FormatSExpr renders a parsed MindScript AST (S-expr) to a stable source string.
//
// Inputs:
//   - n: an AST produced by parser.go (e.g., the result of ParseSExpr).
//
// Output policy:
//   - Statements (fun/oracle/for/if/type/block/return/break/continue/assign)
//     are rendered with keywords and indentation similar to Pretty.
//   - Expressions are printed with minimal parentheses based on a fixed
//     precedence table; property access vs calls/indexing binds tightly.
//   - Arrays and maps are printed inline (AST form) without line-width heuristics;
//     however, annotations on map keys are emitted as preceding `# ...` comment lines.
//   - Annotation nodes wrap the printed construct with header comments.
//
// This function does not parse; it strictly formats the provided AST.
func FormatSExpr(n S) string {
	var b strings.Builder
	p := pp{out: out{b: &b}}
	p.printProgram(n)
	return strings.TrimRight(b.String(), "\n")
}

// FormatType renders a type S-expression into a compact, human-readable string.
//
// Supported forms (see types.go for the type system):
//   - ("id", name)              →  name
//   - ("unop","?", T)           →  T?
//   - ("array", T)              →  [T]
//   - ("map", ("pair"| "pair!", ("str",k), T) ...)
//   - Required fields print as `key!:`
//   - Keys are quoted if not identifier-like
//   - Key/value annotations (when wrapped in "annot") emit `# ...`
//     lines above the field
//   - ("enum", lit1, lit2, ...) →  Enum[lit1, lit2, ...] where literals may be
//     scalars, arrays, or maps rendered in a type-literal form
//   - ("binop","->", A, B)      →  (A) -> B ; right-assoc chains flatten
//
// Output is stable; when multi-line, map fields are sorted by key name for
// readability and determinism.
func FormatType(t S) string {
	var b strings.Builder
	o := out{b: &b}
	writeType(&o, t)
	return b.String()
}

// FormatValue renders a runtime Value into a stable, readable string.
//
// Layout policy:
//   - Scalars: `null`, booleans, ints, floats (guaranteed to show a decimal
//     point for non-scientific output), and quoted strings (with JSON-style escapes).
//   - Arrays: single-line `[ a, b, c ]` when all elements are single-line
//     and total length ≤ MaxInlineWidth; otherwise multi-line with one element per
//     line, indented, and with trailing commas omitted.
//   - Maps (MapObject):
//   - Keys are emitted in sorted order for stability.
//   - Single-line `{ k1: v1, k2: v2 }` is used when feasible under the
//     MaxInlineWidth budget and when no key carries a header annotation.
//   - Multi-line maps indent each entry; key/value annotations are emitted
//     as `# ...` lines immediately above the field.
//   - Keys are quoted when not identifier-like.
//   - Functions: printed as `<fun: name1:T1 -> name2:T2 -> R>`
//   - Zero-arg closures display `_:Null` for the single implicit unit.
//   - Types (VTType): the embedded type AST is pretty-printed via FormatType.
//   - Modules: printed as `<module: pretty-name>` if available.
//   - Value.Annot (header annotation): if non-empty, prints as leading `# ...`
//     lines above the value.
//
// Width sensitivity is only applied to runtime arrays/maps (not AST/type forms),
// and is controlled by MaxInlineWidth.
func FormatValue(v Value) string {
	var b strings.Builder
	o := out{b: &b}
	writeValue(&o, v)
	return b.String()
}

//// END_OF_PUBLIC

// ===============================
// ========= PRIVATE =============
// ===============================

/* ---------- globals & tiny helpers ---------- */

func isIdent(s string) bool {
	if s == "" {
		return false
	}
	rs := []rune(s)
	if !(unicode.IsLetter(rs[0]) || rs[0] == '_') {
		return false
	}
	for _, r := range rs[1:] {
		if !(unicode.IsLetter(r) || unicode.IsDigit(r) || r == '_') {
			return false
		}
	}
	return true
}

func quoteString(s string) string {
	var b strings.Builder
	b.WriteByte('"')
	for _, r := range s {
		switch r {
		case '\\':
			b.WriteString(`\\`)
		case '"':
			b.WriteString(`\"`)
		case '\n':
			b.WriteString(`\n`)
		case '\r':
			b.WriteString(`\r`)
		case '\t':
			b.WriteString(`\t`)
		case '\b':
			b.WriteString(`\b`)
		case '\f':
			b.WriteString(`\f`)
		default:
			b.WriteRune(r)
		}
	}
	b.WriteByte('"')
	return b.String()
}

/* ---------- small writer with indentation & annotations ---------- */

type out struct {
	b     *strings.Builder
	depth int
}

func (o *out) write(s string) { o.b.WriteString(s) }
func (o *out) nl()            { o.b.WriteByte('\n') }
func (o *out) pad() {
	for i := 0; i < o.depth; i++ {
		o.b.WriteByte('\t')
	}
}
func (o *out) line(s string)        { o.pad(); o.b.WriteString(s) }
func (o *out) withIndent(fn func()) { o.depth++; fn(); o.depth-- }
func (o *out) annot(text string) {
	if text == "" {
		return
	}
	for _, ln := range strings.Split(text, "\n") {
		o.line("# " + strings.TrimSpace(ln))
		o.nl()
	}
}

// unwraps the current VTType payload to its AST (supports legacy S as well).
func typeAst(data any) S {
	switch tv := data.(type) {
	case *TypeValue:
		return tv.Ast
	case S: // legacy fallback
		return tv
	default:
		return S{}
	}
}

/* ---------- source -> pretty (AST printer) ---------- */

type pp struct {
	out out
}

func (p *pp) write(s string) { p.out.write(s) }
func (p *pp) nl()            { p.out.nl() }
func (p *pp) sp()            { p.out.write(" ") }
func (p *pp) pad()           { p.out.pad() }

func (p *pp) printProgram(n S) {
	if tag(n) != "block" {
		p.printStmt(n)
		return
	}
	kids := children(n)
	for i, k := range kids {
		p.printStmt(k.(S))
		if i < len(kids)-1 {
			p.nl()
		}
	}
}

func (p *pp) printStmt(n S) {
	switch tag(n) {
	case "annot":
		text := getStr(child(n, 0))
		p.out.annot(text)
		p.printStmt(child(n, 1))

	case "fun":
		p.pad()
		p.write("fun(")
		p.printParams(child(n, 0))
		p.write(")")
		ret := child(n, 1)
		if !(tag(ret) == "id" && getId(ret) == "Any") {
			p.sp()
			p.write("->")
			p.sp()
			p.printExpr(ret, 0)
		}
		p.sp()
		p.write("do")
		p.nl()
		p.out.withIndent(func() { p.printBlock(child(n, 2)) })
		if len(child(n, 2)) > 1 {
			p.nl()
		}
		p.pad()
		p.write("end")

	case "oracle":
		p.pad()
		p.write("oracle(")
		p.printParams(child(n, 0))
		p.write(")")
		out := child(n, 1)
		if !(tag(out) == "id" && getId(out) == "Any") {
			p.sp()
			p.write("->")
			p.sp()
			p.printExpr(out, 0)
		}
		srcs := child(n, 2)
		if len(srcs) > 1 {
			p.sp()
			p.write("from ")
			p.printArray(srcs)
		}

	case "for":
		p.pad()
		p.write("for ")
		tgt := child(n, 0)
		// For targets that are declaration patterns, print them as patterns:
		//  - ("decl", x)    → "let x"
		//  - ("darr", ...)  → "[...]"  (no "let")
		//  - ("dobj", ...)  → "{...}"  (no "let")
		// Non-pattern assignables print as normal expressions.
		if isDeclPattern(tgt) {
			if tag(tgt) == "decl" {
				p.write("let ")
				p.printPattern(tgt) // prints just the name
			} else {
				p.printPattern(tgt)
			}
		} else {
			p.printExpr(tgt, 0)
		}
		p.sp()
		p.write("in ")
		p.printExpr(child(n, 1), 0)
		p.sp()
		p.write("do")
		p.nl()
		p.out.withIndent(func() { p.printBlock(child(n, 2)) })
		if len(child(n, 2)) > 1 {
			p.nl()
		}
		p.pad()
		p.write("end")

	case "while":
		p.pad()
		p.write("while ")
		p.printExpr(child(n, 0), 0)
		p.sp()
		p.write("do")
		p.nl()
		p.out.withIndent(func() { p.printBlock(child(n, 1)) })
		if len(child(n, 1)) > 1 {
			p.nl()
		}
		p.pad()
		p.write("end")

	case "if":
		arms := children(n)
		first := arms[0].(S)
		p.pad()
		p.write("if ")
		p.printExpr(child(first, 0), 0)
		p.sp()
		p.write("then")
		p.nl()
		p.out.withIndent(func() { p.printBlock(child(first, 1)) })
		if len(child(first, 1)) > 1 {
			p.nl()
		}
		i := 1
		for i < len(arms) && tag(arms[i].(S)) == "pair" {
			arm := arms[i].(S)
			p.pad()
			p.write("elif ")
			p.printExpr(child(arm, 0), 0)
			p.sp()
			p.write("then")
			p.nl()
			p.out.withIndent(func() { p.printBlock(child(arm, 1)) })
			if len(child(arm, 1)) > 1 {
				p.nl()
			}
			i++
		}
		if i < len(arms) {
			elseBlk := arms[i].(S)
			p.pad()
			p.write("else")
			p.nl()
			p.out.withIndent(func() { p.printBlock(elseBlk) })
			if len(elseBlk) > 1 {
				p.nl()
			}
		}
		p.pad()
		p.write("end")

	case "type":
		p.pad()
		p.write("type ")
		p.printExpr(child(n, 0), 0)

	case "return":
		p.pad()
		p.write("return(")
		p.printExpr(child(n, 0), 0)
		p.write(")")

	case "break":
		p.pad()
		p.write("break(")
		p.printExpr(child(n, 0), 0)
		p.write(")")

	case "continue":
		p.pad()
		p.write("continue(")
		p.printExpr(child(n, 0), 0)
		p.write(")")

	case "assign":
		lhs, rhs := child(n, 0), child(n, 1)
		p.pad()
		if isDeclPattern(lhs) {
			p.write("let ")
			p.printPattern(lhs)
			p.sp()
			p.write("=")
			p.sp()
			p.printExpr(rhs, 0)
		} else {
			p.printExpr(lhs, 0)
			p.sp()
			p.write("=")
			p.sp()
			p.printExpr(rhs, 0)
		}

	case "block":
		p.pad()
		p.write("do")
		p.nl()
		p.out.withIndent(func() { p.printBlock(n) })
		p.pad()
		p.write("end")

	default:
		p.pad()
		p.printExpr(n, 0)
	}
}

func (p *pp) printBlock(n S) {
	if tag(n) != "block" {
		p.printStmt(n)
		return
	}
	kids := children(n)
	for i, k := range kids {
		p.printStmt(k.(S))
		if i < len(kids)-1 {
			p.nl()
		}
	}
}

func (p *pp) printParams(arr S) {
	if tag(arr) != "array" || len(arr) == 1 {
		return
	}
	items := children(arr)
	for i, it := range items {
		pi := it.(S)
		name := getId(child(pi, 0))
		p.write(name)
		ty := child(pi, 1)
		if !(tag(ty) == "id" && getId(ty) == "Any") {
			p.write(": ")
			p.printExpr(ty, 0)
		}
		if i < len(items)-1 {
			p.write(", ")
		}
	}
}

func (p *pp) printExpr(n S, _ctx int) {
	switch tag(n) {
	case "id":
		p.write(getId(n))
	case "int":
		p.write(fmt.Sprint(n[1]))
	case "num":
		f := n[1].(float64)
		s := strconv.FormatFloat(f, 'g', -1, 64)
		if !strings.ContainsAny(s, ".eE") {
			s += ".0"
		}
		p.write(s)
	case "str":
		p.write(quoteString(getStr(n)))
	case "bool":
		if n[1].(bool) {
			p.write("true")
		} else {
			p.write("false")
		}
	case "null":
		p.write("null")
	case "unop":
		op := n[1].(string)
		if op == "?" {
			recv := n[2].(S)
			if prec(recv) < 90 {
				p.write("(")
				p.printExpr(recv, 0)
				p.write(")")
			} else {
				p.printExpr(recv, 90)
			}
			p.write("?")
			return
		}
		if op == "not" {
			p.write("not ")
		} else {
			p.write(op)
		}
		operand := n[2].(S)
		if prec(operand) < 80 {
			p.write("(")
			p.printExpr(operand, 0)
			p.write(")")
		} else {
			p.printExpr(operand, 80)
		}
	case "binop":
		op := n[1].(string)
		my := binopPrec(op)
		l, r := n[2].(S), n[3].(S)
		if prec(l) < my {
			p.write("(")
			p.printExpr(l, 0)
			p.write(")")
		} else {
			p.printExpr(l, my)
		}
		p.write(" " + op + " ")
		if prec(r) < my {
			p.write("(")
			p.printExpr(r, 0)
			p.write(")")
		} else {
			p.printExpr(r, my)
		}
	case "assign":
		l, r := child(n, 0), child(n, 1)
		if prec(l) < 10 {
			p.write("(")
			p.printExpr(l, 0)
			p.write(")")
		} else {
			p.printExpr(l, 10)
		}
		p.write(" = ")
		if prec(r) < 10 {
			p.write("(")
			p.printExpr(r, 0)
			p.write(")")
		} else {
			p.printExpr(r, 10)
		}
	case "call":
		recv := child(n, 0)
		if prec(recv) < 90 {
			p.write("(")
			p.printExpr(recv, 0)
			p.write(")")
		} else {
			p.printExpr(recv, 90)
		}
		p.write("(")
		for i := 2; i < len(n); i++ {
			if i > 2 {
				p.write(", ")
			}
			p.printExpr(n[i].(S), 0)
		}
		p.write(")")
	case "idx":
		recv := child(n, 0)
		if prec(recv) < 90 {
			p.write("(")
			p.printExpr(recv, 0)
			p.write(")")
		} else {
			p.printExpr(recv, 90)
		}
		p.write("[")
		p.printExpr(child(n, 1), 0)
		p.write("]")
	case "get":
		recv := child(n, 0)
		if prec(recv) < 90 {
			p.write("(")
			p.printExpr(recv, 0)
			p.write(")")
		} else {
			p.printExpr(recv, 90)
		}
		name := getStr(child(n, 1))
		if isIdent(name) {
			p.write("." + name)
		} else {
			p.write("." + quoteString(name))
		}
	case "array":
		p.printArray(n)
	case "map":
		p.printMap(n)

	case "enum":
		// Type expression: Enum[ <lits...> ]
		p.write("Enum[")
		items := children(n)
		for i, it := range items {
			if i > 0 {
				p.write(", ")
			}
			p.printExpr(it.(S), 0) // literals can be scalars, arrays, or maps
		}
		p.write("]")

	case "decl":
		p.write("let " + getId(n))
	case "return", "break", "continue", "fun", "oracle", "for", "while", "if", "type", "block", "annot":
		p.printStmt(n)
	default:
		p.write("<" + tag(n) + ">")
	}
}

func (p *pp) printArray(n S) {
	p.write("[")
	for i, it := range children(n) {
		if i > 0 {
			p.write(", ")
		}
		p.printExpr(it.(S), 0)
	}
	p.write("]")
}

func (p *pp) printMap(n S) {
	p.write("{")
	items := children(n)
	for i, pr := range items {
		if i > 0 {
			p.write(", ")
		}
		pair := pr.(S)
		keyNode := child(pair, 0)
		key, keyAnn := unwrapKey(keyNode)

		if keyAnn != "" {
			// put the annotation as a line comment just before the field
			p.nl()
			p.out.withIndent(func() {
				p.pad()
				p.out.annot(keyAnn)
			})
			p.pad()
		}

		if isIdent(key) {
			p.write(key)
		} else {
			p.write(quoteString(key))
		}
		p.write(": ")
		p.printExpr(child(pair, 1), 0)

		// if we emitted a leading annotation, keep items nicely lined up
		if keyAnn != "" && i < len(items)-1 {
			p.write(", ")
		}
	}
	p.write("}")
}

/* ---------- tiny AST helpers ---------- */

func tag(n S) string     { return n[0].(string) }
func children(n S) []any { return n[1:] }
func child(n S, i int) S { return n[i+1].(S) }
func getId(n S) string   { return n[1].(string) }
func getStr(n S) string  { return n[1].(string) }

func prec(n S) int {
	switch tag(n) {
	case "assign":
		return 10
	case "binop":
		return binopPrec(n[1].(string))
	case "unop":
		if n[1].(string) == "?" {
			return 90
		}
		return 80
	case "call", "idx", "get":
		return 90
	default:
		return 100
	}
}

func unwrapKey(n S) (name string, annot string) {
	for tag(n) == "annot" {
		annot += getStr(child(n, 0))
		n = child(n, 1)
	}
	return getStr(n), annot
}

func binopPrec(op string) int {
	switch op {
	case "->":
		return 15
	case "*", "/", "%":
		return 70
	case "+", "-":
		return 60
	case "<", "<=", ">", ">=":
		return 50
	case "==", "!=":
		return 40
	case "and":
		return 30
	case "or":
		return 20
	default:
		return 60
	}
}

func isDeclPattern(n S) bool {
	switch tag(n) {
	case "decl", "darr", "dobj":
		return true
	case "annot":
		return isDeclPattern(child(n, 1))
	default:
		return false
	}
}

func (p *pp) printPattern(n S) {
	switch tag(n) {
	case "decl":
		p.write(getId(n))
	case "darr":
		p.write("[")
		for i := 1; i < len(n); i++ {
			if i > 1 {
				p.write(", ")
			}
			p.printPattern(n[i].(S))
		}
		p.write("]")
	case "dobj":
		items := children(n)
		needsMultiline := false
		for _, it := range items {
			if tag(it.(S)[2].(S)) == "annot" {
				needsMultiline = true
				break
			}
		}
		if !needsMultiline {
			p.write("{")
			for i, it := range items {
				if i > 0 {
					p.write(", ")
				}
				pr := it.(S)
				key := getStr(child(pr, 0))
				if isIdent(key) {
					p.write(key)
				} else {
					p.write(quoteString(key))
				}
				p.write(": ")
				p.printPattern(child(pr, 1))
			}
			p.write("}")
			return
		}
		p.write("{")
		p.nl()
		p.out.withIndent(func() {
			for i, it := range items {
				pr := it.(S)
				p.pad()
				key := getStr(child(pr, 0))
				if isIdent(key) {
					p.write(key)
				} else {
					p.write(quoteString(key))
				}
				p.write(": ")
				val := child(pr, 1)
				if tag(val) == "annot" {
					p.nl()
					p.printPattern(val)
				} else {
					p.printPattern(val)
				}
				if i < len(items)-1 {
					p.write(",")
				}
				p.nl()
			}
		})
		p.pad()
		p.write("}")
	case "annot":
		p.out.annot(getStr(child(n, 0)))
		p.pad()
		p.printPattern(child(n, 1))
	default:
		p.printExpr(n, 0)
	}
}

/* ---------- Type pretty-printer (private helpers) ---------- */

func writeType(o *out, t S) {
	tagOf := func(x S) string {
		if len(x) == 0 {
			return ""
		}
		return x[0].(string)
	}

	switch tagOf(t) {
	case "id":
		o.write(t[1].(string))

	case "unop":
		if len(t) >= 3 && t[1].(string) == "?" {
			writeType(o, t[2].(S))
			o.write("?")
			return
		}
		o.write("<unop>")

	case "array":
		o.write("[")
		elem := S{"id", "Any"}
		if len(t) == 2 {
			elem = t[1].(S)
		}
		writeType(o, elem)
		o.write("]")

	case "enum":
		o.write("Enum[")
		for i := 1; i < len(t); i++ {
			if i > 1 {
				o.write(", ")
			}
			writeTypeLiteral(o, t[i].(S))
		}
		o.write("]")

	case "map":
		type field struct {
			name     string
			required bool
			typ      S
			keyAnnot string
			valAnnot string
		}
		fs := make([]field, 0, len(t)-1)
		for i := 1; i < len(t); i++ {
			p := t[i].(S) // ("pair"|"pair!", keyNode, T or ("annot", ("str",doc), T))
			req := p[0].(string) == "pair!"
			keyNode := p[1].(S)
			key, keyAnn := unwrapKey(keyNode)
			ft := p[2].(S)
			valAnn := ""
			if len(ft) > 0 && ft[0].(string) == "annot" {
				valAnn = ft[1].(S)[1].(string)
				ft = ft[2].(S)
			}
			fs = append(fs, field{name: key, required: req, typ: ft, keyAnnot: keyAnn, valAnnot: valAnn})
		}
		sort.Slice(fs, func(i, j int) bool { return fs[i].name < fs[j].name })

		o.write("{")
		o.nl()
		o.withIndent(func() {
			for i, f := range fs {
				if f.keyAnnot != "" {
					o.annot(f.keyAnnot)
				}
				if f.valAnnot != "" {
					o.annot(f.valAnnot)
				}
				o.pad()
				if isIdent(f.name) {
					o.write(f.name)
				} else {
					o.write(quoteString(f.name))
				}
				if f.required {
					o.write("!")
				}
				o.write(": ")
				writeType(o, f.typ)
				if i < len(fs)-1 {
					o.write(",")
				}
				o.nl()
			}
		})
		o.pad()
		o.write("}")

	case "binop":
		if len(t) >= 4 && t[1].(string) == "->" {
			params, ret := flattenArrow(t)
			o.write("(")
			for i := range params {
				if i > 0 {
					o.write(", ")
				}
				writeType(o, params[i])
			}
			o.write(") -> ")
			writeType(o, ret)
			return
		}
		o.write("<binop>")

	case "annot":
		o.annot(t[1].(S)[1].(string))
		writeType(o, t[2].(S))

	default:
		o.write("<type>")
	}
}

func writeTypeLiteral(o *out, lit S) {
	switch lit[0].(string) {
	case "null":
		o.write("null")
	case "bool":
		if lit[1].(bool) {
			o.write("true")
		} else {
			o.write("false")
		}
	case "int":
		o.write(fmt.Sprint(lit[1]))
	case "num":
		o.write(strconv.FormatFloat(lit[1].(float64), 'g', -1, 64))
	case "str":
		o.write(quoteString(lit[1].(string)))

	// Allow complex enum members (arrays/maps) for schema-like enums
	case "array":
		o.write("[")
		for i := 1; i < len(lit); i++ {
			if i > 1 {
				o.write(", ")
			}
			writeTypeLiteral(o, lit[i].(S))
		}
		o.write("]")
	case "map":
		o.write("{")
		items := children(lit)
		for i, pr := range items {
			if i > 0 {
				o.write(", ")
			}
			pair := pr.(S)
			k := getStr(child(pair, 0))
			if isIdent(k) {
				o.write(k)
			} else {
				o.write(quoteString(k))
			}
			o.write(": ")
			writeTypeLiteral(o, child(pair, 1))
		}
		o.write("}")

	default:
		o.write("<lit>")
	}
}

func flattenArrow(t S) (params []S, ret S) {
	for {
		if tag(t) == "binop" && t[1].(string) == "->" {
			params = append(params, t[2].(S))
			t = t[3].(S)
			continue
		}
		ret = t
		return
	}
}

/* ---------- Runtime value pretty-printer (private helpers) ---------- */

func mapOneLineMO(keys []string, mo *MapObject) string {
	if len(keys) == 0 {
		return "{}"
	}
	parts := make([]string, 0, len(keys))
	for _, k := range keys {
		if ann, ok := mo.KeyAnn[k]; ok && ann != "" {
			// any annotated key forces multiline
			return ""
		}
		v := mo.Entries[k]
		if isValueMultiline(v) {
			return ""
		}
		key := k
		if !isIdent(key) {
			key = quoteString(key)
		}
		var b strings.Builder
		o := out{b: &b}
		writeValue(&o, v)
		parts = append(parts, key+": "+b.String())
	}
	return "{ " + strings.Join(parts, ", ") + " }"
}

func writeValue(o *out, v Value) {
	// Header annotation (once)
	if v.Annot != "" {
		o.annot(v.Annot)
		o.pad()
	}

	switch v.Tag {

	case VTNull:
		o.write("null")

	case VTBool:
		if v.Data.(bool) {
			o.write("true")
		} else {
			o.write("false")
		}

	case VTInt:
		o.write(strconv.FormatInt(v.Data.(int64), 10))

	case VTNum:
		s := strconv.FormatFloat(v.Data.(float64), 'g', -1, 64)
		if !strings.ContainsAny(s, ".eE") {
			s += ".0"
		}
		o.write(s)

	case VTStr:
		o.write(quoteString(v.Data.(string)))

	case VTArray:
		xs := v.Data.([]Value)
		if oneline := arrayOneLine(xs); oneline != "" && len(oneline) <= MaxInlineWidth {
			o.write(oneline)
			return
		}
		o.write("[")
		o.nl()
		o.withIndent(func() {
			for i, it := range xs {
				if it.Annot == "" {
					o.pad() // pad here only when no header annot
				}
				writeValue(o, it) // annotated values handle their own padding
				if i < len(xs)-1 {
					o.write(",")
				}
				o.nl()
			}
		})
		o.pad()
		o.write("]")

	case VTMap:
		mo := v.Data.(*MapObject)
		// Build a sorted view of keys for stable output
		keys := append([]string(nil), mo.Keys...)
		sort.Strings(keys)

		// One-line candidate (only if no key is annotated)
		if oneline := mapOneLineMO(keys, mo); oneline != "" && len(oneline) <= MaxInlineWidth {
			o.write(oneline)
			return
		}

		// Multiline
		o.write("{")
		o.nl()
		o.withIndent(func() {
			for i, k := range keys {
				if ann, ok := mo.KeyAnn[k]; ok && ann != "" {
					o.annot(ann) // o.annot already pads correctly
				}
				o.pad() // align with annotation line
				if isIdent(k) {
					o.write(k)
				} else {
					o.write(quoteString(k))
				}
				o.write(": ")
				writeValue(o, mo.Entries[k])
				if i < len(keys)-1 {
					o.write(",")
				}
				o.nl()
			}
		})
		o.pad()
		o.write("}")
		return

	case VTFun:
		// Pretty-print as <fun: n1:T1 -> n2:T2 -> R> ; zero-arg as _:Null -> R
		if f, ok := v.Data.(*Fun); ok && f != nil {
			var sb strings.Builder
			sb.WriteString("<fun: ")
			if len(f.ParamTypes) == 0 {
				sb.WriteString("_:Null")
			} else {
				for i := range f.ParamTypes {
					if i > 0 {
						sb.WriteString(" -> ")
					}
					name := "_"
					if i < len(f.Params) && f.Params[i] != "" {
						name = f.Params[i]
					}
					sb.WriteString(name)
					sb.WriteString(":")
					sb.WriteString(FormatType(f.ParamTypes[i]))
				}
			}
			sb.WriteString(" -> ")
			sb.WriteString(FormatType(f.ReturnType))
			sb.WriteString(">")
			o.write(sb.String())
		} else {
			o.write("<fun>")
		}

	case VTType:
		ast := typeAst(v.Data)
		typ := FormatType(ast)
		if strings.Contains(typ, "\n") {
			lines := strings.Split(typ, "\n")
			for i, ln := range lines {
				if i == 0 {
					o.write(ln)
					continue
				}
				o.nl()
				o.pad()
				o.write(ln)
			}
			return
		}
		o.write(typ)

	case VTModule:
		name := "<module>"
		if m, ok := v.Data.(*Module); ok && m != nil && m.Name != "" {
			disp := prettySpec(m.Name)
			if disp == "" {
				disp = m.Name
			}
			name = "<module: " + disp + ">"
		}
		o.write(name)

	default:
		s := "<unknown>"
		if v.Tag == VTHandle {
			if h, ok := v.Data.(*Handle); ok {
				s = "<handle:" + h.Kind + ">"
			} else {
				s = "<handle>"
			}
		}
		o.write(s)
	}
}

/* ---------- single-line candidates ---------- */

func arrayOneLine(xs []Value) string {
	if len(xs) == 0 {
		return "[]"
	}
	parts := make([]string, 0, len(xs))
	for _, it := range xs {
		if isValueMultiline(it) {
			return ""
		}
		var b strings.Builder
		o := out{b: &b}
		writeValue(&o, it)
		parts = append(parts, b.String())
	}
	return "[ " + strings.Join(parts, ", ") + " ]"
}

func isValueMultiline(v Value) bool {
	if v.Annot != "" { // header comments force multi-line
		return true
	}
	switch v.Tag {
	case VTArray:
		xs := v.Data.([]Value)
		if len(xs) == 0 {
			return false
		}
		for _, it := range xs {
			if isValueMultiline(it) {
				return true
			}
		}
		line := arrayOneLine(xs)
		return line == "" || len(line) > MaxInlineWidth
	case VTMap:
		mo := v.Data.(*MapObject)
		if len(mo.Keys) == 0 {
			return false
		}
		keys := append([]string(nil), mo.Keys...)
		sort.Strings(keys)
		line := mapOneLineMO(keys, mo)
		return line == "" || len(line) > MaxInlineWidth

	case VTType:
		t := typeAst(v.Data)
		return len(t) > 0 && t[0].(string) == "map" && len(t) > 1

	default:
		return false
	}
}
=== END FILE: printer.go ===

